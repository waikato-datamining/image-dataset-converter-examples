{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"The image-dataset-converter library (and its dependent libraries) can be used for converting image datasets from one format into another. It has I/O support for the following domains: Image classification Object detection Image segmentation Please refer to the dataset formats section for more details on supported formats. But the library does not just convert datasets, you can also slot in complex filter pipelines to process/clean the data. On this website you can find examples and documentation for: Image classification Object detection Image segmentation Filter usage Placeholders Execution control External functions Multiple I/O Email File handling Temporary storage Docker usage Examples for the additional libraries: Image augmentation Image statistics Image visualizations labelme Paddle PDF Redis Video Practical examples: Detecting animals in trail cameras (SpeciesNet) Depth estimation in videos (MiDaS) Object detection in videos (Yolov5)","title":"Home"},{"location":"depth_estimation_in_videos/","text":"This example shows how to apply a MiDaS monocular depth estimation model to images from a video and displays the depth information as grayscale images. Requirements # NB: No GPU required. Additional image-dataset-converter libraries: image-dataset-converter-imgvis image-dataset-converter-video Data # Input # Output # Preparation # NB: Place all the downloads in the current directory Download the dashcam01.mp4 video from the BoofCV project Download the dpt_levit_224 model (for more, see here ) The host machine must have a Redis server instance running. Two options: Install it natively via sudo apt-get install redis (and then restart it with sudo systemctl restart redis ) Spin up a docker container with: docker run --net=host --name redis-server -d redis MiDaS model # The following command launches the MiDaS model via the container's midas_predict_redis command, running on the CPU: docker run \\ --net=host -u $(id -u):$(id -g) \\ -v `pwd`:/workspace \\ -v `pwd`/cache:/.cache \\ -it waikatodatamining/midas:3.1_cpu \\ midas_predict_redis \\ --redis_in images \\ --redis_out predictions \\ --model_weights /workspace/dpt_levit_224.pt \\ --model_type dpt_levit_224 \\ --prediction_format numpy image-dataset-converter # The following pipeline loads every 2nd frame from the dashcam01.mp4 video, tees off the input image into its own viewer, then obtains the depth information from the MiDaS model (using the Redis backend), turns the predictions into grayscale images and then displays them in another viewer window: idc-convert \\ -l INFO \\ from-video-file \\ -i ./dashcam01.mp4 \\ -n 2 \\ -t dp \\ tee -f \"image-viewer -t input -p 0,50 -s 800,224 -d 1\" \\ redis-predict-dp \\ --channel_out images \\ --channel_in predictions \\ --data_format numpy \\ --timeout 1.0 \\ depth-to-grayscale \\ -t dp \\ any-to-rgb \\ image-viewer \\ -t output \\ -p 0,400 \\ -s 800,224 \\ -d 1","title":"Depth estimation"},{"location":"depth_estimation_in_videos/#requirements","text":"NB: No GPU required. Additional image-dataset-converter libraries: image-dataset-converter-imgvis image-dataset-converter-video","title":"Requirements"},{"location":"depth_estimation_in_videos/#data","text":"","title":"Data"},{"location":"depth_estimation_in_videos/#input","text":"","title":"Input"},{"location":"depth_estimation_in_videos/#output","text":"","title":"Output"},{"location":"depth_estimation_in_videos/#preparation","text":"NB: Place all the downloads in the current directory Download the dashcam01.mp4 video from the BoofCV project Download the dpt_levit_224 model (for more, see here ) The host machine must have a Redis server instance running. Two options: Install it natively via sudo apt-get install redis (and then restart it with sudo systemctl restart redis ) Spin up a docker container with: docker run --net=host --name redis-server -d redis","title":"Preparation"},{"location":"depth_estimation_in_videos/#midas-model","text":"The following command launches the MiDaS model via the container's midas_predict_redis command, running on the CPU: docker run \\ --net=host -u $(id -u):$(id -g) \\ -v `pwd`:/workspace \\ -v `pwd`/cache:/.cache \\ -it waikatodatamining/midas:3.1_cpu \\ midas_predict_redis \\ --redis_in images \\ --redis_out predictions \\ --model_weights /workspace/dpt_levit_224.pt \\ --model_type dpt_levit_224 \\ --prediction_format numpy","title":"MiDaS model"},{"location":"depth_estimation_in_videos/#image-dataset-converter","text":"The following pipeline loads every 2nd frame from the dashcam01.mp4 video, tees off the input image into its own viewer, then obtains the depth information from the MiDaS model (using the Redis backend), turns the predictions into grayscale images and then displays them in another viewer window: idc-convert \\ -l INFO \\ from-video-file \\ -i ./dashcam01.mp4 \\ -n 2 \\ -t dp \\ tee -f \"image-viewer -t input -p 0,50 -s 800,224 -d 1\" \\ redis-predict-dp \\ --channel_out images \\ --channel_in predictions \\ --data_format numpy \\ --timeout 1.0 \\ depth-to-grayscale \\ -t dp \\ any-to-rgb \\ image-viewer \\ -t output \\ -p 0,400 \\ -s 800,224 \\ -d 1","title":"image-dataset-converter"},{"location":"docker/","text":"Below are examples for using the image-dataset-converter library via its Docker images . Interactive session # The following command starts an interactive session, mapping the current working directory to /workspace : docker run --rm -u $(id -u):$(id -g) \\ -v `pwd`:/workspace \\ -it waikatodatamining/image-dataset-converter:latest Conversion pipeline # The following converts an image classification dataset from the sub-dir format (sub-directory names represent the image classification labels) into the ADAMS format , which stores the label in an associated .report file (Java properties file): docker run --rm -u $(id -u):$(id -g) \\ -v `pwd`:/workspace \\ -it waikatodatamining/image-dataset-converter:latest \\ idc-convert -l INFO \\ from-subdir-ic \\ -l INFO \\ -i /workspace/input/ \\ to-adams-ic \\ -l INFO \\ -o /workspace/output \\ -c classification NB: The input and output directories are located below the current working directory ( pwd ).","title":"Docker usage"},{"location":"docker/#interactive-session","text":"The following command starts an interactive session, mapping the current working directory to /workspace : docker run --rm -u $(id -u):$(id -g) \\ -v `pwd`:/workspace \\ -it waikatodatamining/image-dataset-converter:latest","title":"Interactive session"},{"location":"docker/#conversion-pipeline","text":"The following converts an image classification dataset from the sub-dir format (sub-directory names represent the image classification labels) into the ADAMS format , which stores the label in an associated .report file (Java properties file): docker run --rm -u $(id -u):$(id -g) \\ -v `pwd`:/workspace \\ -it waikatodatamining/image-dataset-converter:latest \\ idc-convert -l INFO \\ from-subdir-ic \\ -l INFO \\ -i /workspace/input/ \\ to-adams-ic \\ -l INFO \\ -o /workspace/output \\ -c classification NB: The input and output directories are located below the current working directory ( pwd ).","title":"Conversion pipeline"},{"location":"email/","text":"Whilst most readers and writers are file-based, it is also possible to retrieve and send emails using the following: get-email - retrieve emails from an IMAP folder, FROM and SUBJECT can be stored in placeholders send-email - send emails via SMTP, the from/to/subject/body options automatically expand placeholders The connection parameters are obtained through environment variables that are stored in .env files. The following pipeline uses the get-email reader to poll the images folder for new messages with JPG attachments every 5 seconds. These images get downloaded and then forwarded to the send-email writer that attaches the images and then sends an email to the specified email address: idc-convert -l INFO \\ get-email \\ -l INFO \\ -f images \\ -o {TMP} \\ -r \".*\\.(jpg|JPG)\" \\ -w 5 \\ --only_unseen \\ --mark_as_read \\ --from_placeholder FROM \\ --subject_placeholder SUBJECT \\ send-email \\ -l INFO \\ -f from@example.com \\ -t someone@anotherexample.com \\ -s {SUBJECT} \\ -b \"Message from: {FROM}\"","title":"Email"},{"location":"execution_control/","text":"The following filters can be used for controlling the execution of the pipeline: block - blocks data passing through based on a condition applied to the meta-data list-to-sequence - forwards the elements of lists individually stop - stops the pipeline if the meta-data-based condition holds true sub-process - meta-data condition determines execution of sub-filter(s) tee - meta-data condition determines forking off of data to the sub-pipeline (filter(s), [writer]) trigger - meta-data condition determines execution of the sub-pipeline (reader, [filter(s)], [writer]) Sub-pipelines # With the tee meta-filter, it is possible to filter the images coming through with a separate sub-pipeline. That allows converting the incoming data into multiple output formats with their own preprocessing. The following command loads the VOC XML annotations and saves them in ADAMS and YOLO format in one command, but each scaled differently: idc-convert \\ -l INFO \\ from-voc-od \\ -l INFO \\ -i \"./voc/*.xml\" \\ tee \\ -f \"scale -m replace -T 0 -f 0.5 -t 0.5 -u to-adams-od -o ./adams-tee/\" \\ tee \\ -f \"scale -m replace -T 0 -f 0.25 -t 0.25 -u to-yolo-od -o ./yolo-tee/ --labels ./yolo-tee/labels.txt\"","title":"Execution control"},{"location":"execution_control/#sub-pipelines","text":"With the tee meta-filter, it is possible to filter the images coming through with a separate sub-pipeline. That allows converting the incoming data into multiple output formats with their own preprocessing. The following command loads the VOC XML annotations and saves them in ADAMS and YOLO format in one command, but each scaled differently: idc-convert \\ -l INFO \\ from-voc-od \\ -l INFO \\ -i \"./voc/*.xml\" \\ tee \\ -f \"scale -m replace -T 0 -f 0.5 -t 0.5 -u to-adams-od -o ./adams-tee/\" \\ tee \\ -f \"scale -m replace -T 0 -f 0.25 -t 0.25 -u to-yolo-od -o ./yolo-tee/ --labels ./yolo-tee/labels.txt\"","title":"Sub-pipelines"},{"location":"file_handling/","text":"Reading/writing text files # from-text-file - this reader reads the file line by line and forwards them to-text-file - writes the incoming strings to the specified text file Listing files # list-files - simply lists files in a directory forwards the list poll-dir - polls a directory for files to process, can move or delete them after they were processed by the specified base-reader watch-dir - uses a file-system watchdog to look for changes to files (events: created or modified) and forwards these to the base-reader, can move or delete these files after processing as well Others # copy-files - copies the incoming files into the specified target directory delete-files - deletes the incoming files move-files - moves the incoming files into the specified target directory","title":"File handling"},{"location":"file_handling/#readingwriting-text-files","text":"from-text-file - this reader reads the file line by line and forwards them to-text-file - writes the incoming strings to the specified text file","title":"Reading/writing text files"},{"location":"file_handling/#listing-files","text":"list-files - simply lists files in a directory forwards the list poll-dir - polls a directory for files to process, can move or delete them after they were processed by the specified base-reader watch-dir - uses a file-system watchdog to look for changes to files (events: created or modified) and forwards these to the base-reader, can move or delete these files after processing as well","title":"Listing files"},{"location":"file_handling/#others","text":"copy-files - copies the incoming files into the specified target directory delete-files - deletes the incoming files move-files - moves the incoming files into the specified target directory","title":"Others"},{"location":"filters/","text":"The following sections only show snippets of commands, as there are quite a number of filters available. Bounding box / polygon # With the coerce-bbox filter, you can force annotations to be bounding box only. The reverse is the coerce-mask filter, which ensures that all annotations are available as polygons. Too small or too large? # Using the dimension-discarder filter, you can filter out too large or too small images quite easily: only allow within certain width/height constraint ... dimension-discarder \\ -l INFO \\ --min_height 100 \\ --max_height 200 \\ --min_width 100 \\ --max_width 200 \\ ... only a certain area, but the shape is irrelevant ... dimension-discarder \\ -l INFO \\ --min_area 10000 \\ --max_area 50000 \\ ... Domain conversion # object detection to image classification: With the od-to-ic filter you can convert object detection annotations to image classification. How multiple differing labels are handled can be specified. object detection to image segmentation: The od-to-is filter generates image segmentation data from the bbox/polygon annotations. Annotation management # filter-labels - leaves only the matching labels in the annotations map-labels - for renaming labels remove-classes - removes the specified labels strip-annotations - removes all annotations write-labels - outputs a list of all the encountered labels Meta-data management # metadata - allows comparisons on meta-data values and whether to keep or discard a record in case of a match metadata-from-name - allows extraction of meta-data value from the image name via a regular expression metadata-to-placeholder - sets the specified placeholder using the data from the meta-data passing through set-metadata - sets the meta-data key/value pair as data passes through, can make use of data passing through as well split-records - adds a field to the meta-data (default: split ) of the record passing through, which can be acted on with other filters (or stored in the output) Record management # A number of generic record management filters are available: check-duplicate-filenames - when using multiple batches as input, duplicate file names can be an issue when creating a combined output discard-by-name - discards images based on their name, either using explicit names or regular expressions discard-invalid-images - attempts to load the image and discards them in case the loading fails (useful when data acquisition can generate invalid images) discard-negatives - removes records from the stream that have no annotations max-records - limits the number of records passing through randomize-records - when processing batches, this filter can randomize them (seeded or unseeded) record-window - only lets a certain window of records pass through (e.g., the first 1000) rename - allows renaming of images, e.g., prefixing them with a batch number/ID sample - for selecting a random sub-sample from the stream","title":"Filter usage"},{"location":"filters/#bounding-box-polygon","text":"With the coerce-bbox filter, you can force annotations to be bounding box only. The reverse is the coerce-mask filter, which ensures that all annotations are available as polygons.","title":"Bounding box / polygon"},{"location":"filters/#too-small-or-too-large","text":"Using the dimension-discarder filter, you can filter out too large or too small images quite easily: only allow within certain width/height constraint ... dimension-discarder \\ -l INFO \\ --min_height 100 \\ --max_height 200 \\ --min_width 100 \\ --max_width 200 \\ ... only a certain area, but the shape is irrelevant ... dimension-discarder \\ -l INFO \\ --min_area 10000 \\ --max_area 50000 \\ ...","title":"Too small or too large?"},{"location":"filters/#domain-conversion","text":"object detection to image classification: With the od-to-ic filter you can convert object detection annotations to image classification. How multiple differing labels are handled can be specified. object detection to image segmentation: The od-to-is filter generates image segmentation data from the bbox/polygon annotations.","title":"Domain conversion"},{"location":"filters/#annotation-management","text":"filter-labels - leaves only the matching labels in the annotations map-labels - for renaming labels remove-classes - removes the specified labels strip-annotations - removes all annotations write-labels - outputs a list of all the encountered labels","title":"Annotation management"},{"location":"filters/#meta-data-management","text":"metadata - allows comparisons on meta-data values and whether to keep or discard a record in case of a match metadata-from-name - allows extraction of meta-data value from the image name via a regular expression metadata-to-placeholder - sets the specified placeholder using the data from the meta-data passing through set-metadata - sets the meta-data key/value pair as data passes through, can make use of data passing through as well split-records - adds a field to the meta-data (default: split ) of the record passing through, which can be acted on with other filters (or stored in the output)","title":"Meta-data management"},{"location":"filters/#record-management","text":"A number of generic record management filters are available: check-duplicate-filenames - when using multiple batches as input, duplicate file names can be an issue when creating a combined output discard-by-name - discards images based on their name, either using explicit names or regular expressions discard-invalid-images - attempts to load the image and discards them in case the loading fails (useful when data acquisition can generate invalid images) discard-negatives - removes records from the stream that have no annotations max-records - limits the number of records passing through randomize-records - when processing batches, this filter can randomize them (seeded or unseeded) record-window - only lets a certain window of records pass through (e.g., the first 1000) rename - allows renaming of images, e.g., prefixing them with a batch number/ID sample - for selecting a random sub-sample from the stream","title":"Record management"},{"location":"image_classification/","text":"Readers and writers for image classification have the -ic suffix. Download the 17 flowers image classification dataset and extract it. Plugins # sub-dir to ADAMS # The following converts an image classification dataset from the sub-dir format (sub-directory names represent the image classification labels) into the ADAMS format , which stores the label in an associated .report file (Java properties file): idc-convert -l INFO \\ from-subdir-ic \\ -l INFO \\ -i ./subdir/ \\ to-adams-ic \\ -l INFO \\ -o ./adams \\ -c classification sub-dir (randomized train/val/test splits) # By enforcing batch-processing --force_batch and using the randomize-records filter, randomized train/val/test splits (writers typically support generating splits) can be generated like this: idc-convert -l INFO --force_batch \\ from-subdir-ic \\ -l INFO \\ -i ./subdir/ \\ randomize-records \\ -s 42 \\ to-subdir-ic \\ -l INFO \\ -o ./subdir-split \\ --split_names train val test \\ --split_ratios 70 15 15","title":"Image classification"},{"location":"image_classification/#plugins","text":"","title":"Plugins"},{"location":"image_classification/#sub-dir-to-adams","text":"The following converts an image classification dataset from the sub-dir format (sub-directory names represent the image classification labels) into the ADAMS format , which stores the label in an associated .report file (Java properties file): idc-convert -l INFO \\ from-subdir-ic \\ -l INFO \\ -i ./subdir/ \\ to-adams-ic \\ -l INFO \\ -o ./adams \\ -c classification","title":"sub-dir to ADAMS"},{"location":"image_classification/#sub-dir-randomized-trainvaltest-splits","text":"By enforcing batch-processing --force_batch and using the randomize-records filter, randomized train/val/test splits (writers typically support generating splits) can be generated like this: idc-convert -l INFO --force_batch \\ from-subdir-ic \\ -l INFO \\ -i ./subdir/ \\ randomize-records \\ -s 42 \\ to-subdir-ic \\ -l INFO \\ -o ./subdir-split \\ --split_names train val test \\ --split_ratios 70 15 15","title":"sub-dir (randomized train/val/test splits)"},{"location":"image_segmentation/","text":"Readers and writers for image segmentation have the -is suffix. Download the blue channel archive of the camvid dataset and extract it. Plugins # Blue channel to Indexed PNG # The following command-line will convert it into a dataset using indexed PNG files: idc-convert \\ -l INFO \\ from-blue-channel-is \\ -l INFO \\ -i \"./bluechannel/*.png\" \\ --labels Animal Archway Bicyclist Bridge Building Car CartLuggagePram Child Column_Pole \\ Fence LaneMkgsDriv LaneMkgsNonDriv Misc_Text MotorcycleScooter OtherMoving ParkingBlock \\ Pedestrian Road RoadShoulder Sidewalk SignSymbol Sky SUVPickupTruck TrafficCone \\ TrafficLight Train Tree Truck_Bus Tunnel VegetationMisc Void Wall \\ to-indexed-png-is \\ -l INFO \\ -p x11 \\ -o ./indexedpng NB: Uses the X11 color palette for the palette in the PNGs. Here is an example (0001TP_007050.png): Blue channel to Indexed PNG (cyclists only) # By applying filters, you can also generate subsets, e.g., for building more specialized models. The following will extract only images that have cyclists and discard all other annotations ( filter-labels ). Images with no annotations left will get discarded ( discard-negatives ): idc-convert \\ -l INFO \\ from-blue-channel-is \\ -l INFO \\ -i \"./bluechannel/*.png\" \\ --labels Animal Archway Bicyclist Bridge Building Car CartLuggagePram Child Column_Pole \\ Fence LaneMkgsDriv LaneMkgsNonDriv Misc_Text MotorcycleScooter OtherMoving ParkingBlock \\ Pedestrian Road RoadShoulder Sidewalk SignSymbol Sky SUVPickupTruck TrafficCone \\ TrafficLight Train Tree Truck_Bus Tunnel VegetationMisc Void Wall \\ filter-labels \\ -l INFO \\ --labels Bicyclist \\ discard-negatives \\ -l INFO \\ to-indexed-png-is \\ -l INFO \\ -p x11 \\ -o ./indexedpng-cyclists Here is an example (0001TP_007380.png):","title":"Image segmentation"},{"location":"image_segmentation/#plugins","text":"","title":"Plugins"},{"location":"image_segmentation/#blue-channel-to-indexed-png","text":"The following command-line will convert it into a dataset using indexed PNG files: idc-convert \\ -l INFO \\ from-blue-channel-is \\ -l INFO \\ -i \"./bluechannel/*.png\" \\ --labels Animal Archway Bicyclist Bridge Building Car CartLuggagePram Child Column_Pole \\ Fence LaneMkgsDriv LaneMkgsNonDriv Misc_Text MotorcycleScooter OtherMoving ParkingBlock \\ Pedestrian Road RoadShoulder Sidewalk SignSymbol Sky SUVPickupTruck TrafficCone \\ TrafficLight Train Tree Truck_Bus Tunnel VegetationMisc Void Wall \\ to-indexed-png-is \\ -l INFO \\ -p x11 \\ -o ./indexedpng NB: Uses the X11 color palette for the palette in the PNGs. Here is an example (0001TP_007050.png):","title":"Blue channel to Indexed PNG"},{"location":"image_segmentation/#blue-channel-to-indexed-png-cyclists-only","text":"By applying filters, you can also generate subsets, e.g., for building more specialized models. The following will extract only images that have cyclists and discard all other annotations ( filter-labels ). Images with no annotations left will get discarded ( discard-negatives ): idc-convert \\ -l INFO \\ from-blue-channel-is \\ -l INFO \\ -i \"./bluechannel/*.png\" \\ --labels Animal Archway Bicyclist Bridge Building Car CartLuggagePram Child Column_Pole \\ Fence LaneMkgsDriv LaneMkgsNonDriv Misc_Text MotorcycleScooter OtherMoving ParkingBlock \\ Pedestrian Road RoadShoulder Sidewalk SignSymbol Sky SUVPickupTruck TrafficCone \\ TrafficLight Train Tree Truck_Bus Tunnel VegetationMisc Void Wall \\ filter-labels \\ -l INFO \\ --labels Bicyclist \\ discard-negatives \\ -l INFO \\ to-indexed-png-is \\ -l INFO \\ -p x11 \\ -o ./indexedpng-cyclists Here is an example (0001TP_007380.png):","title":"Blue channel to Indexed PNG (cyclists only)"},{"location":"imgaug/","text":"Requirements # Requires the image-dataset-converter-imgaug library. Download the 17 flowers object detection VOC XML dataset and extract it. Plugins # Convert VOC XML to YOLO (crop/rotate) # The following converts VOC XML annotations into YOLO ones. It also augments the dataset with randomly adding cropped/rotated images to the stream ( -m add ), with the annotations getting processed accordingly: idc-convert \\ -l INFO \\ from-voc-od \\ -l INFO \\ -i \"./voc/*.xml\" \\ crop \\ -m add \\ -f 0.1 \\ -t 0.2 \\ -u \\ rotate \\ -m add \\ -f \"-45\" \\ -t 45 \\ to-yolo-od \\ -l INFO \\ -o ./yolo-split-augmented \\ --labels ./yolo-split-augmented/labels.txt \\ --split_names train val test \\ --split_ratios 70 15 15 Here is an example of a processed image (image_0001.jpg): Convert VOC XML to MS COCO (resize) # The following converts VOC XML annotations into smaller MS COCO ones, using a maximum width of 300 while keeping the aspect ratio intact. The data also gets split into train/validation/test: idc-convert \\ -l INFO \\ from-voc-od \\ -l INFO \\ -i \"./voc/*.xml\" \\ resize \\ -W 300 \\ -H keep-aspect-ratio \\ to-coco-od \\ -l INFO \\ -o ./coco-split-small \\ --split_names train val test \\ --split_ratios 70 15 15 Split images into smaller ones # With the sub-images filter it is possible to split images into smaller ones or extract just specific regions of interest from images. The idc-generate-regions tool can be used to generate regions for a set number of rows/columns or fixed row heights/column widths. For images that are at maximum 800x800, we can generate regions for a 2x2 grid as follows: idc-generate-regions \\ -l INFO \\ -W 800 \\ -H 800 \\ -r 2 \\ -c 2 Will output this, with each quadruplet consisting of x, y, width and height: 0,0,400,400 400,0,400,400 0,400,400,400 400,400,400,400 These regions we can now use with the sub-images filter to generate smaller images: idc-convert \\ -l INFO \\ from-voc-od \\ -l INFO \\ -i \"./voc/*.xml\" \\ sub-images \\ -l INFO \\ -r 0,0,400,400 400,0,400,400 0,400,400,400 400,400,400,400 \\ -p \\ -e \\ to-yolo-od \\ -l INFO \\ -o ./yolo-sub \\ --labels ./yolo-sub/labels.txt Using -p we will keep partial annotations, ones that got cut off a bit, and with -e we will suppress images that have no annotations in them. Using the meta-sub-images filter, you can feed the small images that were generated through the specified base filter. The output of this filter gets reassembled and forwarded. This base filter can be a simple filter or a more complex pipeline, which passes the images through a model via a Redis backend , for instance.","title":"Image augmentation"},{"location":"imgaug/#requirements","text":"Requires the image-dataset-converter-imgaug library. Download the 17 flowers object detection VOC XML dataset and extract it.","title":"Requirements"},{"location":"imgaug/#plugins","text":"","title":"Plugins"},{"location":"imgaug/#convert-voc-xml-to-yolo-croprotate","text":"The following converts VOC XML annotations into YOLO ones. It also augments the dataset with randomly adding cropped/rotated images to the stream ( -m add ), with the annotations getting processed accordingly: idc-convert \\ -l INFO \\ from-voc-od \\ -l INFO \\ -i \"./voc/*.xml\" \\ crop \\ -m add \\ -f 0.1 \\ -t 0.2 \\ -u \\ rotate \\ -m add \\ -f \"-45\" \\ -t 45 \\ to-yolo-od \\ -l INFO \\ -o ./yolo-split-augmented \\ --labels ./yolo-split-augmented/labels.txt \\ --split_names train val test \\ --split_ratios 70 15 15 Here is an example of a processed image (image_0001.jpg):","title":"Convert VOC XML to YOLO (crop/rotate)"},{"location":"imgaug/#convert-voc-xml-to-ms-coco-resize","text":"The following converts VOC XML annotations into smaller MS COCO ones, using a maximum width of 300 while keeping the aspect ratio intact. The data also gets split into train/validation/test: idc-convert \\ -l INFO \\ from-voc-od \\ -l INFO \\ -i \"./voc/*.xml\" \\ resize \\ -W 300 \\ -H keep-aspect-ratio \\ to-coco-od \\ -l INFO \\ -o ./coco-split-small \\ --split_names train val test \\ --split_ratios 70 15 15","title":"Convert VOC XML to MS COCO (resize)"},{"location":"imgaug/#split-images-into-smaller-ones","text":"With the sub-images filter it is possible to split images into smaller ones or extract just specific regions of interest from images. The idc-generate-regions tool can be used to generate regions for a set number of rows/columns or fixed row heights/column widths. For images that are at maximum 800x800, we can generate regions for a 2x2 grid as follows: idc-generate-regions \\ -l INFO \\ -W 800 \\ -H 800 \\ -r 2 \\ -c 2 Will output this, with each quadruplet consisting of x, y, width and height: 0,0,400,400 400,0,400,400 0,400,400,400 400,400,400,400 These regions we can now use with the sub-images filter to generate smaller images: idc-convert \\ -l INFO \\ from-voc-od \\ -l INFO \\ -i \"./voc/*.xml\" \\ sub-images \\ -l INFO \\ -r 0,0,400,400 400,0,400,400 0,400,400,400 400,400,400,400 \\ -p \\ -e \\ to-yolo-od \\ -l INFO \\ -o ./yolo-sub \\ --labels ./yolo-sub/labels.txt Using -p we will keep partial annotations, ones that got cut off a bit, and with -e we will suppress images that have no annotations in them. Using the meta-sub-images filter, you can feed the small images that were generated through the specified base filter. The output of this filter gets reassembled and forwarded. This base filter can be a simple filter or a more complex pipeline, which passes the images through a model via a Redis backend , for instance.","title":"Split images into smaller ones"},{"location":"imgstats/","text":"Requirements # Requires the image-dataset-converter-imgstats library. Download the 17 flowers object detection VOC XML dataset and extract it. Plugins # Label distribution # Using the label-dist writer, you can output a distribution of the labels within a dataset as follows: idc-convert \\ -l INFO \\ from-voc-od \\ -l INFO \\ -i \"./voc/*.xml\" \\ label-dist \\ -l INFO This will output the following: Bluebell: 28 Buttercup: 54 ColtsFoot: 55 Crocus: 50 Daffodil: 71 Daisy: 57 Dandelion: 43 Fritillary: 65 Iris: 77 LilyValley: 17 Pansy: 56 Snowdrop: 50 Sunflower: 71 Tigerlily: 50 Tulip: 41 Windflower: 63 NB: You can also generate CSV or JSON output. Area histogram # Knowing the distribution of the extends of your annotations can be quite useful, e.g., for determining whether down-sampling can be an option. The area-histogram writer generates histograms for all objects combined and one for each category: idc-convert \\ -l INFO \\ from-voc-od \\ -l INFO \\ -i \"./voc/*.xml\" \\ area-histogram \\ -l INFO The output for the 17 flowers dataset looks like this: ALL: +1.26e+04 - +3.29e+04 [ 14] \u2588\u2588\u2588\u2588\u2588\u258e +3.29e+04 - +5.33e+04 [ 30] \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258d +5.33e+04 - +7.36e+04 [ 36] \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258b +7.36e+04 - +9.40e+04 [ 54] \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258d +9.40e+04 - +1.14e+05 [ 71] \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258a +1.14e+05 - +1.35e+05 [ 71] \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258a +1.35e+05 - +1.55e+05 [ 95] \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2589 +1.55e+05 - +1.75e+05 [104] \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258e +1.75e+05 - +1.96e+05 [ 94] \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258c +1.96e+05 - +2.16e+05 [106] \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588 +2.16e+05 - +2.37e+05 [ 77] \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588 +2.37e+05 - +2.57e+05 [ 48] \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258f +2.57e+05 - +2.77e+05 [ 14] \u2588\u2588\u2588\u2588\u2588\u258e +2.77e+05 - +2.98e+05 [ 10] \u2588\u2588\u2588\u258a +2.98e+05 - +3.18e+05 [ 9] \u2588\u2588\u2588\u258d +3.18e+05 - +3.38e+05 [ 6] \u2588\u2588\u258e +3.38e+05 - +3.59e+05 [ 2] \u258a +3.59e+05 - +3.79e+05 [ 5] \u2588\u2589 +3.79e+05 - +3.99e+05 [ 1] \u258d +3.99e+05 - +4.20e+05 [ 1] \u258d Bluebell: +7.68e+04 - +9.18e+04 [1] \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588 +9.18e+04 - +1.07e+05 [1] \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588 +1.07e+05 - +1.22e+05 [2] \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588 +1.22e+05 - +1.37e+05 [0] +1.37e+05 - +1.51e+05 [0] +1.51e+05 - +1.66e+05 [2] \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588 +1.66e+05 - +1.81e+05 [2] \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588 +1.81e+05 - +1.96e+05 [0] +1.96e+05 - +2.11e+05 [5] \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588 +2.11e+05 - +2.26e+05 [1] \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588 +2.26e+05 - +2.41e+05 [4] \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588 +2.41e+05 - +2.56e+05 [3] \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588 +2.56e+05 - +2.71e+05 [1] \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588 +2.71e+05 - +2.86e+05 [0] +2.86e+05 - +3.01e+05 [2] \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588 +3.01e+05 - +3.16e+05 [0] +3.16e+05 - +3.31e+05 [1] \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588 +3.31e+05 - +3.46e+05 [0] +3.46e+05 - +3.61e+05 [0] +3.61e+05 - +3.76e+05 [3] \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588 Buttercup: ... NB: You can also generate CSV or JSON output.","title":"Image statistics"},{"location":"imgstats/#requirements","text":"Requires the image-dataset-converter-imgstats library. Download the 17 flowers object detection VOC XML dataset and extract it.","title":"Requirements"},{"location":"imgstats/#plugins","text":"","title":"Plugins"},{"location":"imgstats/#label-distribution","text":"Using the label-dist writer, you can output a distribution of the labels within a dataset as follows: idc-convert \\ -l INFO \\ from-voc-od \\ -l INFO \\ -i \"./voc/*.xml\" \\ label-dist \\ -l INFO This will output the following: Bluebell: 28 Buttercup: 54 ColtsFoot: 55 Crocus: 50 Daffodil: 71 Daisy: 57 Dandelion: 43 Fritillary: 65 Iris: 77 LilyValley: 17 Pansy: 56 Snowdrop: 50 Sunflower: 71 Tigerlily: 50 Tulip: 41 Windflower: 63 NB: You can also generate CSV or JSON output.","title":"Label distribution"},{"location":"imgstats/#area-histogram","text":"Knowing the distribution of the extends of your annotations can be quite useful, e.g., for determining whether down-sampling can be an option. The area-histogram writer generates histograms for all objects combined and one for each category: idc-convert \\ -l INFO \\ from-voc-od \\ -l INFO \\ -i \"./voc/*.xml\" \\ area-histogram \\ -l INFO The output for the 17 flowers dataset looks like this: ALL: +1.26e+04 - +3.29e+04 [ 14] \u2588\u2588\u2588\u2588\u2588\u258e +3.29e+04 - +5.33e+04 [ 30] \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258d +5.33e+04 - +7.36e+04 [ 36] \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258b +7.36e+04 - +9.40e+04 [ 54] \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258d +9.40e+04 - +1.14e+05 [ 71] \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258a +1.14e+05 - +1.35e+05 [ 71] \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258a +1.35e+05 - +1.55e+05 [ 95] \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2589 +1.55e+05 - +1.75e+05 [104] \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258e +1.75e+05 - +1.96e+05 [ 94] \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258c +1.96e+05 - +2.16e+05 [106] \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588 +2.16e+05 - +2.37e+05 [ 77] \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588 +2.37e+05 - +2.57e+05 [ 48] \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258f +2.57e+05 - +2.77e+05 [ 14] \u2588\u2588\u2588\u2588\u2588\u258e +2.77e+05 - +2.98e+05 [ 10] \u2588\u2588\u2588\u258a +2.98e+05 - +3.18e+05 [ 9] \u2588\u2588\u2588\u258d +3.18e+05 - +3.38e+05 [ 6] \u2588\u2588\u258e +3.38e+05 - +3.59e+05 [ 2] \u258a +3.59e+05 - +3.79e+05 [ 5] \u2588\u2589 +3.79e+05 - +3.99e+05 [ 1] \u258d +3.99e+05 - +4.20e+05 [ 1] \u258d Bluebell: +7.68e+04 - +9.18e+04 [1] \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588 +9.18e+04 - +1.07e+05 [1] \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588 +1.07e+05 - +1.22e+05 [2] \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588 +1.22e+05 - +1.37e+05 [0] +1.37e+05 - +1.51e+05 [0] +1.51e+05 - +1.66e+05 [2] \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588 +1.66e+05 - +1.81e+05 [2] \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588 +1.81e+05 - +1.96e+05 [0] +1.96e+05 - +2.11e+05 [5] \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588 +2.11e+05 - +2.26e+05 [1] \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588 +2.26e+05 - +2.41e+05 [4] \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588 +2.41e+05 - +2.56e+05 [3] \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588 +2.56e+05 - +2.71e+05 [1] \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588 +2.71e+05 - +2.86e+05 [0] +2.86e+05 - +3.01e+05 [2] \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588 +3.01e+05 - +3.16e+05 [0] +3.16e+05 - +3.31e+05 [1] \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588 +3.31e+05 - +3.46e+05 [0] +3.46e+05 - +3.61e+05 [0] +3.61e+05 - +3.76e+05 [3] \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588 Buttercup: ... NB: You can also generate CSV or JSON output.","title":"Area histogram"},{"location":"imgvis/","text":"Requirements # Requires the image-dataset-converter-imgvis module. Plugins # Annotation overlays (image classification) # Download the 17 flowers image classification dataset and extract it. Having annotations separate from the images is a necessity when training models, but it can be a hindrance when trying to inspect the data. Adding overlays with the annotations is therefore a useful step sometimes. The following command using the add-annotation-overlay-ic filter adds the labels to the images and just outputs these modified images: idc-convert -l INFO \\ from-subdir-ic \\ -l INFO \\ -i ./subdir/ \\ add-annotation-overlay-ic \\ -l INFO \\ to-data \\ -l INFO \\ -o ./17flowers-ic-overlay Here is an example (image_0008.jpg): Annotation overlays (object detection) # Download the 17 flowers object detection VOC XML dataset and extract it. With the add-annotation-overlay-od filter you can overlay the objects on the images: idc-convert -l INFO \\ from-voc-od \\ -l INFO \\ -i \"./voc/*.xml\" \\ add-annotation-overlay-od \\ -l INFO \\ to-data \\ -l INFO \\ -o ./17flowers-od-overlay Here is an example (image_0014.jpg): Annotation overlays (image segmentation) # Download the blue channel archive of the camvid dataset and extract it. With the add-annotation-overlay-is filter you can overlay the segmentation layers: idc-convert -l INFO \\ from-blue-channel-is \\ -l INFO \\ -i \"./bluechannel/*.png\" \\ --labels Animal Archway Bicyclist Bridge Building Car CartLuggagePram Child Column_Pole \\ Fence LaneMkgsDriv LaneMkgsNonDriv Misc_Text MotorcycleScooter OtherMoving ParkingBlock \\ Pedestrian Road RoadShoulder Sidewalk SignSymbol Sky SUVPickupTruck TrafficCone \\ TrafficLight Train Tree Truck_Bus Tunnel VegetationMisc Void Wall \\ add-annotation-overlay-is \\ -l INFO \\ to-data \\ -l INFO \\ -o ./camvid-is-overlay Here is an example (0001TP_006900.jpg): Combining all annotations in one image # Using the to-annotation-overlay-od writer, you can generate a single PNG file that contains the outlines (bbox or polygon) of your object detection annotations. That way, you can see whether certain areas are under-represented with annotations and where hotspots are: idc-convert -l INFO \\ from-voc-od \\ -l INFO \\ -i \"./voc/*.xml\" \\ to-annotation-overlay-od \\ -l INFO \\ -o ./17flowers-annotations-overlay.png The 17 flowers dataset looks like this:","title":"Image visualizations"},{"location":"imgvis/#requirements","text":"Requires the image-dataset-converter-imgvis module.","title":"Requirements"},{"location":"imgvis/#plugins","text":"","title":"Plugins"},{"location":"imgvis/#annotation-overlays-image-classification","text":"Download the 17 flowers image classification dataset and extract it. Having annotations separate from the images is a necessity when training models, but it can be a hindrance when trying to inspect the data. Adding overlays with the annotations is therefore a useful step sometimes. The following command using the add-annotation-overlay-ic filter adds the labels to the images and just outputs these modified images: idc-convert -l INFO \\ from-subdir-ic \\ -l INFO \\ -i ./subdir/ \\ add-annotation-overlay-ic \\ -l INFO \\ to-data \\ -l INFO \\ -o ./17flowers-ic-overlay Here is an example (image_0008.jpg):","title":"Annotation overlays (image classification)"},{"location":"imgvis/#annotation-overlays-object-detection","text":"Download the 17 flowers object detection VOC XML dataset and extract it. With the add-annotation-overlay-od filter you can overlay the objects on the images: idc-convert -l INFO \\ from-voc-od \\ -l INFO \\ -i \"./voc/*.xml\" \\ add-annotation-overlay-od \\ -l INFO \\ to-data \\ -l INFO \\ -o ./17flowers-od-overlay Here is an example (image_0014.jpg):","title":"Annotation overlays (object detection)"},{"location":"imgvis/#annotation-overlays-image-segmentation","text":"Download the blue channel archive of the camvid dataset and extract it. With the add-annotation-overlay-is filter you can overlay the segmentation layers: idc-convert -l INFO \\ from-blue-channel-is \\ -l INFO \\ -i \"./bluechannel/*.png\" \\ --labels Animal Archway Bicyclist Bridge Building Car CartLuggagePram Child Column_Pole \\ Fence LaneMkgsDriv LaneMkgsNonDriv Misc_Text MotorcycleScooter OtherMoving ParkingBlock \\ Pedestrian Road RoadShoulder Sidewalk SignSymbol Sky SUVPickupTruck TrafficCone \\ TrafficLight Train Tree Truck_Bus Tunnel VegetationMisc Void Wall \\ add-annotation-overlay-is \\ -l INFO \\ to-data \\ -l INFO \\ -o ./camvid-is-overlay Here is an example (0001TP_006900.jpg):","title":"Annotation overlays (image segmentation)"},{"location":"imgvis/#combining-all-annotations-in-one-image","text":"Using the to-annotation-overlay-od writer, you can generate a single PNG file that contains the outlines (bbox or polygon) of your object detection annotations. That way, you can see whether certain areas are under-represented with annotations and where hotspots are: idc-convert -l INFO \\ from-voc-od \\ -l INFO \\ -i \"./voc/*.xml\" \\ to-annotation-overlay-od \\ -l INFO \\ -o ./17flowers-annotations-overlay.png The 17 flowers dataset looks like this:","title":"Combining all annotations in one image"},{"location":"labelme/","text":"Requirements # labelme support requires the image-dataset-converter-labelme library. Image classification # The following converts all labelme .json files with image/label definitions into sub-directory format: idc-convert -l INFO \\ from-labelme-ic \\ -l INFO \\ -i ./labelme/*.json \\ to-subdir-ic \\ -l INFO \\ -o ./subdir/ And this reads annotations in ADAMS .report format and stores in labelme json format and also specifies all possible labels: idc-convert -l INFO \\ from-adams-ic \\ -l INFO \\ -i ./adams/*.report \\ -c classification \\ to-labelme-ic \\ -l INFO \\ --labels cat dog bunny bird \\ -o ./labelme Object detection # The following converts ADAMS object annotations to labelme rectangle shapes: idc-convert -l INFO \\ from-adams-od \\ -l INFO \\ -i ./adams/*.report \\ coerce-box \\ to-labelme-od \\ -l INFO \\ -o ./labelme NB: coerce-box removes any polygon annotations and only leaves the bbox. Instance segmentation # The following converts ADAMS object annotations to labelme polygon shapes: idc-convert -l INFO \\ from-adams-od \\ -l INFO \\ -i ./adams/*.report \\ coerce-mask \\ to-labelme-od \\ -l INFO \\ -o ./labelme NB: coerce-mask ensures that polygon annotations are present, even if it is only the outline of a bbox.","title":"labelme"},{"location":"labelme/#requirements","text":"labelme support requires the image-dataset-converter-labelme library.","title":"Requirements"},{"location":"labelme/#image-classification","text":"The following converts all labelme .json files with image/label definitions into sub-directory format: idc-convert -l INFO \\ from-labelme-ic \\ -l INFO \\ -i ./labelme/*.json \\ to-subdir-ic \\ -l INFO \\ -o ./subdir/ And this reads annotations in ADAMS .report format and stores in labelme json format and also specifies all possible labels: idc-convert -l INFO \\ from-adams-ic \\ -l INFO \\ -i ./adams/*.report \\ -c classification \\ to-labelme-ic \\ -l INFO \\ --labels cat dog bunny bird \\ -o ./labelme","title":"Image classification"},{"location":"labelme/#object-detection","text":"The following converts ADAMS object annotations to labelme rectangle shapes: idc-convert -l INFO \\ from-adams-od \\ -l INFO \\ -i ./adams/*.report \\ coerce-box \\ to-labelme-od \\ -l INFO \\ -o ./labelme NB: coerce-box removes any polygon annotations and only leaves the bbox.","title":"Object detection"},{"location":"labelme/#instance-segmentation","text":"The following converts ADAMS object annotations to labelme polygon shapes: idc-convert -l INFO \\ from-adams-od \\ -l INFO \\ -i ./adams/*.report \\ coerce-mask \\ to-labelme-od \\ -l INFO \\ -o ./labelme NB: coerce-mask ensures that polygon annotations are present, even if it is only the outline of a bbox.","title":"Instance segmentation"},{"location":"multi/","text":"Most of the time, conversion pipelines will only need to read from one source and output to a single target. However, there can be cases where datasets of different types need merging ( multiple inputs ) or datasets of different types need to generated for different frameworks ( multiple outputs ). To cater for these scenarios, the following two meta plugins are available: from-multi - reads from one or more sources using the specified readers to-multi - forwards the incoming data to one or more writers There is one restriction, each of the base reader/writer must be from the same data domain. Multiple inputs # The following command reads a dataset in ADAMS object detection format and MS COCO , with the combined output being saved in OPEX JSON format: idc-convert \\ -l INFO \\ from-multi \\ -l INFO \\ -t od \\ -r \"from-adams-od -l INFO -i {CWD}/input/*.report\" \\ \"from-coco-od -l INFO -i {CWD}/input/*.json\" \\ to-opex-od \\ -l INFO \\ -o \"{CWD}/output\" Multiple outputs # Below, the source data is in ADAMS object detection format and will be converted to OPEX JSON and MS COCO : idc-convert \\ -l INFO \\ from-adams-od \\ -l INFO \\ -i {CWD}/input/*.report \\ to-multi \\ -l INFO \\ -t od \\ -w \"to-opex-od -l INFO -o {CWD}/output/opex\" \\ \"to-coco-od -l INFO -o {CWD}/output/coco\"","title":"Multiple I/O"},{"location":"multi/#multiple-inputs","text":"The following command reads a dataset in ADAMS object detection format and MS COCO , with the combined output being saved in OPEX JSON format: idc-convert \\ -l INFO \\ from-multi \\ -l INFO \\ -t od \\ -r \"from-adams-od -l INFO -i {CWD}/input/*.report\" \\ \"from-coco-od -l INFO -i {CWD}/input/*.json\" \\ to-opex-od \\ -l INFO \\ -o \"{CWD}/output\"","title":"Multiple inputs"},{"location":"multi/#multiple-outputs","text":"Below, the source data is in ADAMS object detection format and will be converted to OPEX JSON and MS COCO : idc-convert \\ -l INFO \\ from-adams-od \\ -l INFO \\ -i {CWD}/input/*.report \\ to-multi \\ -l INFO \\ -t od \\ -w \"to-opex-od -l INFO -o {CWD}/output/opex\" \\ \"to-coco-od -l INFO -o {CWD}/output/coco\"","title":"Multiple outputs"},{"location":"object_detection/","text":"Readers and writers for object detection have the -od suffix. Download the 17 flowers object detection VOC XML dataset and extract it. Plugins # VOC XML to MS COCO # The following converts the VOC XML dataset into MS COCO format, a format used by frameworks like Detectron2 or MMDetection : idc-convert \\ -l INFO \\ from-voc-od \\ -l INFO \\ -i \"./voc/*.xml\" \\ to-coco-od \\ -l INFO \\ -o ./coco VOC XML to YOLO (train/val/test splits) # You can also split the data, e.g., into train, validation and test subsets. The following converts the VOC XML now into YOLO format: idc-convert \\ -l INFO \\ from-voc-od \\ -l INFO \\ -i \"./voc/*.xml\" \\ to-yolo-od \\ -l INFO \\ -o ./yolo-split \\ --labels ./yolo-split/labels.txt \\ --split_names train val test \\ --split_ratios 70 15 15 NB: The subsets will be placed into sub-directories according to the split name.","title":"Object detection"},{"location":"object_detection/#plugins","text":"","title":"Plugins"},{"location":"object_detection/#voc-xml-to-ms-coco","text":"The following converts the VOC XML dataset into MS COCO format, a format used by frameworks like Detectron2 or MMDetection : idc-convert \\ -l INFO \\ from-voc-od \\ -l INFO \\ -i \"./voc/*.xml\" \\ to-coco-od \\ -l INFO \\ -o ./coco","title":"VOC XML to MS COCO"},{"location":"object_detection/#voc-xml-to-yolo-trainvaltest-splits","text":"You can also split the data, e.g., into train, validation and test subsets. The following converts the VOC XML now into YOLO format: idc-convert \\ -l INFO \\ from-voc-od \\ -l INFO \\ -i \"./voc/*.xml\" \\ to-yolo-od \\ -l INFO \\ -o ./yolo-split \\ --labels ./yolo-split/labels.txt \\ --split_names train val test \\ --split_ratios 70 15 15 NB: The subsets will be placed into sub-directories according to the split name.","title":"VOC XML to YOLO (train/val/test splits)"},{"location":"object_detection_in_videos/","text":"In this example we are using a prebuilt yolov5 model (using MS-COCO) to make predictions on the frames that come from a dashcam video, overlay the predictions on the images and display them. For the model we will be using an existing docker container. Requirements # NB: No GPU required. Additional image-dataset-converter libraries: image-dataset-converter-imgvis image-dataset-converter-video Data # Input # Output # Preparation # NB: Place all the downloads in the current directory Download the dashcam01.mp4 video from the BoofCV project Download the yolo5n.pt model Download the coco.yaml data description for the yolo5n model Create a directory called cache in the current directory The host machine must have a Redis server instance running. Two options: Install it natively via sudo apt-get install redis (and then restart it with sudo systemctl restart redis ) Spin up a docker container with: docker run --net=host --name redis-server -d redis Yolov5 model # The following command launches a Yolov5 model via the container's yolov5_predict_redis command, running on the CPU: docker run \\ --net=host -u $(id -u):$(id -g) \\ -v `pwd`:/workspace \\ -it waikatodatamining/pytorch-yolov5:2022-01-21_cpu \\ yolov5_predict_redis \\ --redis_in images \\ --redis_out predictions \\ --model /workspace/yolov5n.pt \\ --data /workspace/coco.yaml image-dataset-converter (direct prediction) # The following pipeline loads every 2nd frame from the dashcam01.mp4 video, obtains predictions from the Yolov5 model (using the Redis backend), overlays the predictions and then displays them: idc-convert \\ -l INFO \\ from-video-file \\ -i ./dashcam01.mp4 \\ -n 2 \\ -t od \\ redis-predict-od \\ --channel_out images \\ --channel_in predictions \\ --timeout 1.0 \\ add-annotation-overlay-od \\ --outline_alpha 255 \\ --outline_thickness 1 \\ --fill \\ --fill_alpha 128 \\ --vary_colors \\ --font_size 10 \\ --text_format \"{label}: {score}\" \\ --text_placement T,L \\ --force_bbox \\ image-viewer \\ --size 800,224 \\ --delay 1 image-dataset-converter (via meta-sub-images) # Like the above example, but uses the meta-sub-images filter to split the incoming images into two and sends them to the model one-by-one, then assembles the predictions and forwards them: idc-convert \\ -l INFO \\ from-video-file \\ -i ./dashcam01.mp4 \\ -n 2 \\ -t od \\ meta-sub-images \\ -l INFO \\ -r 0,0,400,224 400,0,400,224 \\ --merge_adjacent_polygons \\ --base_filter \"redis-predict-od --channel_out images --channel_in predictions --timeout 1.0\" \\ add-annotation-overlay-od \\ --outline_alpha 255 \\ --outline_thickness 1 \\ --fill \\ --fill_alpha 128 \\ --vary_colors \\ --font_size 10 \\ --text_format \"{label}: {score}\" \\ --text_placement T,L \\ --force_bbox \\ image-viewer \\ --size 800,224 \\ --delay 1 NB: Requires the image-dataset-converter-imgaug library. Meta-data of merged polygons gets lost apart from the label ( type ), which has to be the same, and any score values, which get averaged.","title":"Object detection"},{"location":"object_detection_in_videos/#requirements","text":"NB: No GPU required. Additional image-dataset-converter libraries: image-dataset-converter-imgvis image-dataset-converter-video","title":"Requirements"},{"location":"object_detection_in_videos/#data","text":"","title":"Data"},{"location":"object_detection_in_videos/#input","text":"","title":"Input"},{"location":"object_detection_in_videos/#output","text":"","title":"Output"},{"location":"object_detection_in_videos/#preparation","text":"NB: Place all the downloads in the current directory Download the dashcam01.mp4 video from the BoofCV project Download the yolo5n.pt model Download the coco.yaml data description for the yolo5n model Create a directory called cache in the current directory The host machine must have a Redis server instance running. Two options: Install it natively via sudo apt-get install redis (and then restart it with sudo systemctl restart redis ) Spin up a docker container with: docker run --net=host --name redis-server -d redis","title":"Preparation"},{"location":"object_detection_in_videos/#yolov5-model","text":"The following command launches a Yolov5 model via the container's yolov5_predict_redis command, running on the CPU: docker run \\ --net=host -u $(id -u):$(id -g) \\ -v `pwd`:/workspace \\ -it waikatodatamining/pytorch-yolov5:2022-01-21_cpu \\ yolov5_predict_redis \\ --redis_in images \\ --redis_out predictions \\ --model /workspace/yolov5n.pt \\ --data /workspace/coco.yaml","title":"Yolov5 model"},{"location":"object_detection_in_videos/#image-dataset-converter-direct-prediction","text":"The following pipeline loads every 2nd frame from the dashcam01.mp4 video, obtains predictions from the Yolov5 model (using the Redis backend), overlays the predictions and then displays them: idc-convert \\ -l INFO \\ from-video-file \\ -i ./dashcam01.mp4 \\ -n 2 \\ -t od \\ redis-predict-od \\ --channel_out images \\ --channel_in predictions \\ --timeout 1.0 \\ add-annotation-overlay-od \\ --outline_alpha 255 \\ --outline_thickness 1 \\ --fill \\ --fill_alpha 128 \\ --vary_colors \\ --font_size 10 \\ --text_format \"{label}: {score}\" \\ --text_placement T,L \\ --force_bbox \\ image-viewer \\ --size 800,224 \\ --delay 1","title":"image-dataset-converter (direct prediction)"},{"location":"object_detection_in_videos/#image-dataset-converter-via-meta-sub-images","text":"Like the above example, but uses the meta-sub-images filter to split the incoming images into two and sends them to the model one-by-one, then assembles the predictions and forwards them: idc-convert \\ -l INFO \\ from-video-file \\ -i ./dashcam01.mp4 \\ -n 2 \\ -t od \\ meta-sub-images \\ -l INFO \\ -r 0,0,400,224 400,0,400,224 \\ --merge_adjacent_polygons \\ --base_filter \"redis-predict-od --channel_out images --channel_in predictions --timeout 1.0\" \\ add-annotation-overlay-od \\ --outline_alpha 255 \\ --outline_thickness 1 \\ --fill \\ --fill_alpha 128 \\ --vary_colors \\ --font_size 10 \\ --text_format \"{label}: {score}\" \\ --text_placement T,L \\ --force_bbox \\ image-viewer \\ --size 800,224 \\ --delay 1 NB: Requires the image-dataset-converter-imgaug library. Meta-data of merged polygons gets lost apart from the label ( type ), which has to be the same, and any score values, which get averaged.","title":"image-dataset-converter (via meta-sub-images)"},{"location":"paddle/","text":"Requirements # Paddle support requires the image-dataset-converter-paddle library. Image classification # With the following command, a sub-dir structured is converted into Paddle format. The integer ID/label text mapping is stored in ./paddle/labels.map , the label ID/file mapping in files.txt and the images are stored relatively to the output directory in the jpg sub-directory: idc-convert -l INFO \\ from-subdir-ic \\ -l INFO \\ -i ./subdir \\ to-paddle-ic \\ -l INFO \\ -o ./paddle \\ --id_label_map ./paddle/labels.map \\ --file_label_map files.txt \\ --relative_path jpg The following converts all Paddle .txt files with image/label definitions back into sub-directory format: idc-convert -l INFO \\ from-paddle-ic \\ -l INFO \\ -i ./paddle/*.txt \\ -m ./paddle/label_list.map \\ to-subdir-ic \\ -l INFO \\ -o ./subdir/ Image segmentation # The following converts indexed PNGs into Paddle's image segmentation format: idc-convert -l INFO \\ from-indexed-png-is \\ -l INFO \\ -i ./indexed/*.png \\ --labels l1 l2 l3 l4 \\ to-paddle-is \\ -l INFO \\ -o ./paddle And this command converts the Paddle format into indexed PNGs again: idc-convert -l INFO \\ from-paddle-is \\ -l INFO \\ -i ./paddle/*_list.txt \\ --labels_file ./paddle/labels.txt \\ to-indexed-png-is \\ -l INFO \\ -o ./indexed Plotting VisualDL log files # When building, e.g., image segmentation models, Paddle can output the progress in VisualDL log files. Typically, these are visualized with the visualdl binary and viewed in the browser. When this is not an option, e.g., when in a headless environment on a server, then you can still generate simple text-based plots in the terminal via the to-terminal-plot writer of the kasperl-plots library. The example pipeline below monitors the ./logs/ directory for any modifications to .log files. It then reads the modified file using the from-visualdl reader, generating a plot for the Evaluate/mIoU scalar and displays it using to-terminal-plot : idc-convert \\ watch-dir \\ -i \"./logs/\" \\ -e \".log\" \\ -E modified \\ -a nothing \\ -p never \\ -b \"from-visualdl -l INFO -c scalar -t Evaluate/mIoU\" \\ to-terminal-plot Below is a screenshot with two terminal sessions, one training a PaddleSeg model, and the other one monitoring/plotting the log file of the training run:","title":"Paddle"},{"location":"paddle/#requirements","text":"Paddle support requires the image-dataset-converter-paddle library.","title":"Requirements"},{"location":"paddle/#image-classification","text":"With the following command, a sub-dir structured is converted into Paddle format. The integer ID/label text mapping is stored in ./paddle/labels.map , the label ID/file mapping in files.txt and the images are stored relatively to the output directory in the jpg sub-directory: idc-convert -l INFO \\ from-subdir-ic \\ -l INFO \\ -i ./subdir \\ to-paddle-ic \\ -l INFO \\ -o ./paddle \\ --id_label_map ./paddle/labels.map \\ --file_label_map files.txt \\ --relative_path jpg The following converts all Paddle .txt files with image/label definitions back into sub-directory format: idc-convert -l INFO \\ from-paddle-ic \\ -l INFO \\ -i ./paddle/*.txt \\ -m ./paddle/label_list.map \\ to-subdir-ic \\ -l INFO \\ -o ./subdir/","title":"Image classification"},{"location":"paddle/#image-segmentation","text":"The following converts indexed PNGs into Paddle's image segmentation format: idc-convert -l INFO \\ from-indexed-png-is \\ -l INFO \\ -i ./indexed/*.png \\ --labels l1 l2 l3 l4 \\ to-paddle-is \\ -l INFO \\ -o ./paddle And this command converts the Paddle format into indexed PNGs again: idc-convert -l INFO \\ from-paddle-is \\ -l INFO \\ -i ./paddle/*_list.txt \\ --labels_file ./paddle/labels.txt \\ to-indexed-png-is \\ -l INFO \\ -o ./indexed","title":"Image segmentation"},{"location":"paddle/#plotting-visualdl-log-files","text":"When building, e.g., image segmentation models, Paddle can output the progress in VisualDL log files. Typically, these are visualized with the visualdl binary and viewed in the browser. When this is not an option, e.g., when in a headless environment on a server, then you can still generate simple text-based plots in the terminal via the to-terminal-plot writer of the kasperl-plots library. The example pipeline below monitors the ./logs/ directory for any modifications to .log files. It then reads the modified file using the from-visualdl reader, generating a plot for the Evaluate/mIoU scalar and displays it using to-terminal-plot : idc-convert \\ watch-dir \\ -i \"./logs/\" \\ -e \".log\" \\ -E modified \\ -a nothing \\ -p never \\ -b \"from-visualdl -l INFO -c scalar -t Evaluate/mIoU\" \\ to-terminal-plot Below is a screenshot with two terminal sessions, one training a PaddleSeg model, and the other one monitoring/plotting the log file of the training run:","title":"Plotting VisualDL log files"},{"location":"pdf/","text":"Requirements # General PDF support requires the image-dataset-converter-pdf library. Extract images from PDF files # The pipeline below extracts any images from the PDF files and places them in the output directory: idc-convert \\ -l INFO \\ from-pdf \\ -l INFO \\ -i ./input/*.pdf \\ -t od \\ to-data \\ -o ./output Object detection overlay example # Requirements # Additional image-dataset-converter library for generating the overlays: image-dataset-converter-imgvis PDF generation # The following pipeline loads predictions in OPEX format, filters out any annotations that are not of type car or truck , generates overlays of bounding boxes with label/score and creates a PDF from these: idc-convert \\ -l INFO \\ --force_batch \\ from-opex-od \\ -l INFO \\ -i ./predictions \\ filter-labels \\ -l INFO \\ --labels car truck \\ add-annotation-overlay-od \\ --outline_alpha 255 \\ --outline_thickness 1 \\ --fill \\ --fill_alpha 128 \\ --vary_colors \\ --font_size 10 \\ --text_format \"{label}: {score}\" \\ --text_placement T,L \\ --force_bbox \\ to-pdf \\ -l INFO \\ -o ./output/vehicles.pdf \\ -t --image_scale=-1","title":"PDF"},{"location":"pdf/#requirements","text":"General PDF support requires the image-dataset-converter-pdf library.","title":"Requirements"},{"location":"pdf/#extract-images-from-pdf-files","text":"The pipeline below extracts any images from the PDF files and places them in the output directory: idc-convert \\ -l INFO \\ from-pdf \\ -l INFO \\ -i ./input/*.pdf \\ -t od \\ to-data \\ -o ./output","title":"Extract images from PDF files"},{"location":"pdf/#object-detection-overlay-example","text":"","title":"Object detection overlay example"},{"location":"pdf/#requirements_1","text":"Additional image-dataset-converter library for generating the overlays: image-dataset-converter-imgvis","title":"Requirements"},{"location":"pdf/#pdf-generation","text":"The following pipeline loads predictions in OPEX format, filters out any annotations that are not of type car or truck , generates overlays of bounding boxes with label/score and creates a PDF from these: idc-convert \\ -l INFO \\ --force_batch \\ from-opex-od \\ -l INFO \\ -i ./predictions \\ filter-labels \\ -l INFO \\ --labels car truck \\ add-annotation-overlay-od \\ --outline_alpha 255 \\ --outline_thickness 1 \\ --fill \\ --fill_alpha 128 \\ --vary_colors \\ --font_size 10 \\ --text_format \"{label}: {score}\" \\ --text_placement T,L \\ --force_bbox \\ to-pdf \\ -l INFO \\ -o ./output/vehicles.pdf \\ -t --image_scale=-1","title":"PDF generation"},{"location":"placeholders/","text":"Juggling longs paths in command-lines can be nightmare, which is the reason the image-dataset-converter library offers support for placeholders . Placeholders can be used to shorten paths and making command-lines easier to transfer to another environment or user. Placeholders (format {PH} ) get expanded dynamically at runtime, taking the current state into account. Placeholder types # There are different types of placeholders: System-defined ones: {HOME} - the user's home directory {CWD} - the current working directory {TMP} - the temporary directory Input-based ones, which are based on the current input file being processed: {INPUT_PATH} - the directory component of the current file {INPUT_NAMEEXT} - the name (incl ext) of the current file {INPUT_NAMENOEXT} - the name (excl ext) of the current file {INPUT_EXT} - the extension of the current file {INPUT_PARENT_PATH} - the path of the file's parent {INPUT_PARENT_NAME} - the name of the file's parent User-defined ones, which are supplied to the tool itself, e.g., via the -p/--placeholders option of the idc-convert tool. The same script can be executed using different directories when using different placeholder setups. The format for the placeholders files is simple, one placeholder per line using placeholder=value as format. Empty lines and ones starting with # get ignored. Runtime ones, which can be set with the set-placeholder plugin. These placeholders can be based on other placeholders. The reason for this plugin is that the output of filters like meta-sub-images don't have any directory associated with them anymore, only a file name. That renders all the input-based placeholders unusable. Using set-placeholder beforehand allows saving the input directory in another placeholder for later use. Meta-data can be used as placeholders as well using the metadata-to-placeholder plugin, which extracts a particular key from the metadata passing through and updates the specified placeholder accordingly. Examples # Relative to input # The following command places the converted data on the same level as the input directory indexed in a directory called grayscale : idc-convert \\ -l INFO \\ from-indexed-png-is \\ -l INFO \\ -i \"/some/where/indexed/*.png\" \\ --labels a b c \\ to-grayscale-is \\ -l INFO \\ -o {INPUT_PARENT_PATH}/grayscale In-place predictions # When trying to add predictions from a model to a directory structure of images, it can be tedious copying images around. Instead, if the model can communicate via Redis, the predictions can be placed in the same location as the input file even if the filter like meta-sub-images should invalidate the input-based placeholders like {INPUT_PATH} . The following command saves the current input directory in {OUTPUT_DIR} , which is then used to save the generated prediction: idc-convert -l INFO \\ from-data \\ -t is \\ -i \"/some/where/**/*.jpg\" \\ set-placeholder \\ -l INFO \\ -p OUTPUT_DIR \\ -v \"{INPUT_PATH}\" \\ meta-sub-images \\ -l INFO \\ --num_rows 2 \\ --num_cols 2 \\ -b \"redis-predict-is -l INFO --labels a b c -o image -i prediction -t 10\" \\ discard-negatives \\ to-indexed-png-is \\ -l INFO \\ -o \"{OUTPUT_DIR}\" \\ --annotations_only","title":"Placeholders"},{"location":"placeholders/#placeholder-types","text":"There are different types of placeholders: System-defined ones: {HOME} - the user's home directory {CWD} - the current working directory {TMP} - the temporary directory Input-based ones, which are based on the current input file being processed: {INPUT_PATH} - the directory component of the current file {INPUT_NAMEEXT} - the name (incl ext) of the current file {INPUT_NAMENOEXT} - the name (excl ext) of the current file {INPUT_EXT} - the extension of the current file {INPUT_PARENT_PATH} - the path of the file's parent {INPUT_PARENT_NAME} - the name of the file's parent User-defined ones, which are supplied to the tool itself, e.g., via the -p/--placeholders option of the idc-convert tool. The same script can be executed using different directories when using different placeholder setups. The format for the placeholders files is simple, one placeholder per line using placeholder=value as format. Empty lines and ones starting with # get ignored. Runtime ones, which can be set with the set-placeholder plugin. These placeholders can be based on other placeholders. The reason for this plugin is that the output of filters like meta-sub-images don't have any directory associated with them anymore, only a file name. That renders all the input-based placeholders unusable. Using set-placeholder beforehand allows saving the input directory in another placeholder for later use. Meta-data can be used as placeholders as well using the metadata-to-placeholder plugin, which extracts a particular key from the metadata passing through and updates the specified placeholder accordingly.","title":"Placeholder types"},{"location":"placeholders/#examples","text":"","title":"Examples"},{"location":"placeholders/#relative-to-input","text":"The following command places the converted data on the same level as the input directory indexed in a directory called grayscale : idc-convert \\ -l INFO \\ from-indexed-png-is \\ -l INFO \\ -i \"/some/where/indexed/*.png\" \\ --labels a b c \\ to-grayscale-is \\ -l INFO \\ -o {INPUT_PARENT_PATH}/grayscale","title":"Relative to input"},{"location":"placeholders/#in-place-predictions","text":"When trying to add predictions from a model to a directory structure of images, it can be tedious copying images around. Instead, if the model can communicate via Redis, the predictions can be placed in the same location as the input file even if the filter like meta-sub-images should invalidate the input-based placeholders like {INPUT_PATH} . The following command saves the current input directory in {OUTPUT_DIR} , which is then used to save the generated prediction: idc-convert -l INFO \\ from-data \\ -t is \\ -i \"/some/where/**/*.jpg\" \\ set-placeholder \\ -l INFO \\ -p OUTPUT_DIR \\ -v \"{INPUT_PATH}\" \\ meta-sub-images \\ -l INFO \\ --num_rows 2 \\ --num_cols 2 \\ -b \"redis-predict-is -l INFO --labels a b c -o image -i prediction -t 10\" \\ discard-negatives \\ to-indexed-png-is \\ -l INFO \\ -o \"{OUTPUT_DIR}\" \\ --annotations_only","title":"In-place predictions"},{"location":"plantcv/","text":"Requirements # Requires the image-dataset-converter-plantcv library. Example input # The following binary image of a plant has been taken from this plantcv tutorial : Skeletonize a plant # The pipeline below turns the binary image of a plant into a skeleton representation, i.e., 1-pixel wide: idc-convert \\ -l INFO \\ from-data \\ -l INFO \\ -t od \\ -i {CWD}/input/plant_binary.png pcv-dilate \\ -k 3 \\ -i 1 \\ pcv-erode \\ -k 3 \\ -i 1 \\ pcv-fill \\ -s 30 \\ pcv-fill-holes \\ pcv-skeletonize \\ -p \\ -s 50 \\ pcv-skeletonize \\ -p \\ -s 50 \\ to-data \\ -l INFO \\ -o {CWD}/output Generates the following output: Find branches # The pipeline below turns the binary image of a plant into a skeleton representation, i.e., 1-pixel wide: idc-convert \\ -l INFO \\ from-data \\ -l INFO \\ -t od \\ -i {CWD}/input/plant_binary.png pcv-dilate \\ -k 3 \\ -i 1 \\ pcv-erode \\ -k 3 \\ -i 1 \\ pcv-fill \\ -s 30 \\ pcv-fill-holes \\ pcv-skeletonize \\ -p \\ -s 50 \\ pcv-find-branch-points \\ any-to-rgb \\ add-annotation-overlay-od \\ --text_format=\"\" \\ --outline_thickness 3 \\ --bbox_outline_outwards \\ -c 255,0,0 \\ to-data \\ -l INFO \\ -o {CWD}/output NB:: add-annotation-overlay-od requires the imgvis plugins. Generates the following output (annotations overlaid onto skeletonized image):","title":"PlantCV"},{"location":"plantcv/#requirements","text":"Requires the image-dataset-converter-plantcv library.","title":"Requirements"},{"location":"plantcv/#example-input","text":"The following binary image of a plant has been taken from this plantcv tutorial :","title":"Example input"},{"location":"plantcv/#skeletonize-a-plant","text":"The pipeline below turns the binary image of a plant into a skeleton representation, i.e., 1-pixel wide: idc-convert \\ -l INFO \\ from-data \\ -l INFO \\ -t od \\ -i {CWD}/input/plant_binary.png pcv-dilate \\ -k 3 \\ -i 1 \\ pcv-erode \\ -k 3 \\ -i 1 \\ pcv-fill \\ -s 30 \\ pcv-fill-holes \\ pcv-skeletonize \\ -p \\ -s 50 \\ pcv-skeletonize \\ -p \\ -s 50 \\ to-data \\ -l INFO \\ -o {CWD}/output Generates the following output:","title":"Skeletonize a plant"},{"location":"plantcv/#find-branches","text":"The pipeline below turns the binary image of a plant into a skeleton representation, i.e., 1-pixel wide: idc-convert \\ -l INFO \\ from-data \\ -l INFO \\ -t od \\ -i {CWD}/input/plant_binary.png pcv-dilate \\ -k 3 \\ -i 1 \\ pcv-erode \\ -k 3 \\ -i 1 \\ pcv-fill \\ -s 30 \\ pcv-fill-holes \\ pcv-skeletonize \\ -p \\ -s 50 \\ pcv-find-branch-points \\ any-to-rgb \\ add-annotation-overlay-od \\ --text_format=\"\" \\ --outline_thickness 3 \\ --bbox_outline_outwards \\ -c 255,0,0 \\ to-data \\ -l INFO \\ -o {CWD}/output NB:: add-annotation-overlay-od requires the imgvis plugins. Generates the following output (annotations overlaid onto skeletonized image):","title":"Find branches"},{"location":"pyfunc/","text":"No library can dream of offering all the required functionality. Especially for one-off tasks, it makes no sense to develop a whole new plugin library. Hence, there are the following generic plugins that allow the user to utilize custom Python functions: reader: from-pyfunc - takes a single string as input and outputs an iterable of image containers (as per specified data type) filter: pyfunc-filter - takes a single image container or an iterable of them as input and outputs a single container or an iterable of them (as per specified input and output data types) writer: to-pyfunc - processes a single image container or an iterable of them as per specified data type and an optional split name In order to use such a custom function, they must be specified in the following format (option: -f/--function ): module_name:function_name If the code below were available through module my.code , then the function specifications would be as follows: reader: my.code:pyfunc_reader filter: my.code:pyfunc_filter writer: my.code:pyfunc_writer from typing import Iterable from kasperl.api import make_list, flatten_list from idc.api import ImageClassificationData # reader: generates image classification containers from the path def pyfunc_reader(path: str) -> Iterable[ImageClassificationData]: return [ImageClassificationData(source=path)] # filter: simply adds a note to the meta-data def pyfunc_filter(data): result = [] for item in make_list(data): if not item.has_metadata(): meta = dict() else: meta = item.get_metadata() meta[\"note\"] = \"filtered by a python function!\" item.set_metadata(meta) result.append(item) return flatten_list(result) # writer: simply outputs name and meta-data and, if present, also the split def pyfunc_writer(data: ImageClassificationData, split: str = None): if split is None: print(\"name: \", data.image_name, \", meta:\", data.get_metadata()) else: print(\"split:\", split, \", name:\", data.image_name, \", meta:\", data.get_metadata())","title":"External functions"},{"location":"redis/","text":"Requirements # Requires the image-dataset-converter-redis library. Plugins # The following plugins can be used for generating domain specific predictions: redis-predict-dp - generate predictions using a depth estimation model, e.g., like these ones redis-predict-ic - classify images, e.g., using MMClassification or PaddleClas redis-predict-is - perform image segmentation, e.g., using MMSegmentation or PaddleSeg redis-predict-od - for applying object detection and instance segmentation, e.g., MMDetection , PaddleDetection , various YOLO variants ( Yolov5 , Yolov7 , Yolov10 ) or Detectron2 Practical examples # The following articles describe how image-dataset-converter and Docker images can be integrated into pipelines for processing data: Depth estimation in videos Object detection in videos","title":"Redis"},{"location":"redis/#requirements","text":"Requires the image-dataset-converter-redis library.","title":"Requirements"},{"location":"redis/#plugins","text":"The following plugins can be used for generating domain specific predictions: redis-predict-dp - generate predictions using a depth estimation model, e.g., like these ones redis-predict-ic - classify images, e.g., using MMClassification or PaddleClas redis-predict-is - perform image segmentation, e.g., using MMSegmentation or PaddleSeg redis-predict-od - for applying object detection and instance segmentation, e.g., MMDetection , PaddleDetection , various YOLO variants ( Yolov5 , Yolov7 , Yolov10 ) or Detectron2","title":"Plugins"},{"location":"redis/#practical-examples","text":"The following articles describe how image-dataset-converter and Docker images can be integrated into pipelines for processing data: Depth estimation in videos Object detection in videos","title":"Practical examples"},{"location":"speciesnet/","text":"In this example we will process trail cam videos, detect animals in them using the SpeciesNet model and store the frames with detections for further use. Requirements # Requires the following libraries: image-dataset-converter-redis image-dataset-converter-video Or simply install image-dataset-converter-all>=0.0.8 : pip install \"image-dataset-converter-all>=0.0.8\" Since we are using Redis as a backend for exchanging images and predictions, you either have to have it installed on your system or you can run it in a Docker container as follows: docker run --net=host --name redis-server -d redis Either one of the SpeciesNet Docker images: CUDA CPU NB: If you haven't used Docker before, then have a look at this tutorial . The following directory structure below your current directory: . | +- cache | +- config | +- input | +- output With input containing one or more video files in AVI format, and output for storing the predictions in OPEX format . NB: MP4 files can be processed as well. SpeciesNet model # Start the SpeciesNet model in Redis mode: CUDA docker run --rm --gpus=all --shm-size 8G --net=host \\ -u $(id -u):$(id -g) -e USER=$USER \\ -v `pwd`:/workspace \\ -v `pwd`/cache:/.cache \\ -v `pwd`/config:/.config \\ -v `pwd`/cache:/.torch \\ -it waikatodatamining/speciesnet:4.0.1_cuda12.1 speciesnet_predict_redis \\ --redis_in images \\ --redis_out predictions \\ --verbose CPU docker run --rm --shm-size 8G --net=host \\ -u $(id -u):$(id -g) -e USER=$USER \\ -v `pwd`:/workspace \\ -v `pwd`/cache:/.cache \\ -v `pwd`/config:/.config \\ -v `pwd`/cache:/.torch \\ -it waikatodatamining/speciesnet:4.0.1_cpu \\ speciesnet_predict_redis \\ --redis_in images \\ --redis_out predictions \\ --verbose Processing single video file # Determine changes between frames # If you want to skip similar frames, e.g., leaves moving in the wind, it pays to look at the frame changes in a video using the calc-frame-changes filter. However, if you are looking for small animals like rodents, you may need to process all frames to avoid missing these. The following command processes the file VID0003.AVI and calculates the changes between frames as a ratio (0-1) with a minimum change of 0.000001 required: idc-convert -l INFO \\ from-video-file \\ -l INFO \\ -i \"./input/VID0003.AVI\" \\ -t od \\ calc-frame-changes \\ -t 0.000001 This will output a histogram similar to this: +1.09e-06 - +3.22e-06 [57] \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588 +3.22e-06 - +5.35e-06 [29] \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258d +5.35e-06 - +7.49e-06 [13] \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258f +7.49e-06 - +9.62e-06 [14] \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2589 +9.62e-06 - +1.18e-05 [15] \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258c +1.18e-05 - +1.39e-05 [10] \u2588\u2588\u2588\u2588\u2588\u2588\u2588 +1.39e-05 - +1.60e-05 [11] \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258a +1.60e-05 - +1.82e-05 [ 7] \u2588\u2588\u2588\u2588\u2589 +1.82e-05 - +2.03e-05 [ 3] \u2588\u2588\u258f +2.03e-05 - +2.24e-05 [ 2] \u2588\u258d +2.24e-05 - +2.46e-05 [ 2] \u2588\u258d +2.46e-05 - +2.67e-05 [ 2] \u2588\u258d +2.67e-05 - +2.88e-05 [ 1] \u258a +2.88e-05 - +3.10e-05 [ 1] \u258a +3.10e-05 - +3.31e-05 [ 5] \u2588\u2588\u2588\u258c +3.31e-05 - +3.52e-05 [ 0] +3.52e-05 - +3.74e-05 [ 0] +3.74e-05 - +3.95e-05 [ 0] +3.95e-05 - +4.16e-05 [ 2] \u2588\u258d +4.16e-05 - +4.38e-05 [ 1] \u258a Using the threshold 5.35e-06 , you will skip frames that would fall into the top two bins of the above histogram. Extracting frames # With the threshold for our video determined, we can now extract relevant frames and push them through the SpeciesNet model. The following command processes VID0003.AVI from the input directory ( from-video-file ), determines frames that differ enough from each other ( skip-similar-frames ) to send to the model ( redis-predict-od ), removes any predictions with a score lower than 0.8 ( metadata-od ), discards any results from the model with no predictions ( discard-negatives ) and stores any images with predictions in the output directory in OPEX format : idc-convert -l INFO \\ from-video-file \\ -l INFO \\ -i \"./input/VID0003.AVI\" \\ -t od \\ skip-similar-frames \\ -l INFO \\ -t 5.35e-06 \\ redis-predict-od \\ -l INFO \\ -o images \\ -i predictions \\ -t 10 \\ metadata-od \\ -l INFO \\ -f score \\ -c gt \\ -v 0.8 \\ -a keep \\ discard-negatives \\ -l INFO \\ to-opex-od \\ -l INFO \\ -o ./output Processing multiple video files # The from-video-file reader is not limited to just processing a single file, e.g., you can supply multiple file names to the -i option or use a glob like *.AVI . However, with different videos having different lighting and therefore different changes between frames, it makes more sense returning only every nth frame to reduce the processing time. You can either use -n/--nth_frame regardless of the frame-rate of the video or -f/--fps_factor which calculates the actual nth frame to return based on the frame-rate of the video multiplied by this factor. The command below processes all .AVI files from the input directory and retrieves one frame for every second of video footage ( -f 1 ): idc-convert -l INFO \\ from-video-file \\ -l INFO \\ -i \"./input/*.AVI\" \\ -f 1 \\ -t od \\ redis-predict-od \\ -l INFO \\ -o images \\ -i predictions \\ -t 10 \\ metadata-od \\ -l INFO \\ -f score \\ -c gt \\ -v 0.8 \\ -a keep \\ discard-negatives \\ -l INFO \\ to-opex-od \\ -l INFO \\ -o ./output Visualizing the predictions # If you want to quickly generate some composite images, you can use the following command. It will create a new directory overlays in which it will save the generated images. idc-convert -l INFO \\ from-opex-od \\ -l INFO \\ -i \"./output/*.json\" \\ add-annotation-overlay-od \\ -l INFO \\ to-data \\ -l INFO \\ -o ./overlays Redirecting frames based on detected labels # When identifying species in a frame, it can be helpful automatically sorting the frames into sub-directories based on the labels that were identified within the frame. This can be done by utilizing the placeholder functionality . For this to work, we need to employ two plugins: first, the label needs to be transferred into the metadata of the data container using the label-to-metadata plugin. This plugin outputs a container for as many labels there are in the data container, which can be none, one or many in case of object detection. Second, the metadata value needs to be turned into a placeholder using the metadata-to-placeholder plugin. Therefore, we can change the output part of the Extracting frames pipeline from: ... discard-negatives \\ -l INFO \\ to-opex-od \\ -l INFO \\ -o ./output To this: ... discard-negatives \\ -l INFO \\ label-to-metadata \\ -l INFO \\ -k type \\ metadata-to-placeholder \\ -l INFO \\ -k type \\ -p TYPE \\ to-opex-od \\ -l INFO \\ -o ./output/{TYPE}","title":"SpeciesNet"},{"location":"speciesnet/#requirements","text":"Requires the following libraries: image-dataset-converter-redis image-dataset-converter-video Or simply install image-dataset-converter-all>=0.0.8 : pip install \"image-dataset-converter-all>=0.0.8\" Since we are using Redis as a backend for exchanging images and predictions, you either have to have it installed on your system or you can run it in a Docker container as follows: docker run --net=host --name redis-server -d redis Either one of the SpeciesNet Docker images: CUDA CPU NB: If you haven't used Docker before, then have a look at this tutorial . The following directory structure below your current directory: . | +- cache | +- config | +- input | +- output With input containing one or more video files in AVI format, and output for storing the predictions in OPEX format . NB: MP4 files can be processed as well.","title":"Requirements"},{"location":"speciesnet/#speciesnet-model","text":"Start the SpeciesNet model in Redis mode: CUDA docker run --rm --gpus=all --shm-size 8G --net=host \\ -u $(id -u):$(id -g) -e USER=$USER \\ -v `pwd`:/workspace \\ -v `pwd`/cache:/.cache \\ -v `pwd`/config:/.config \\ -v `pwd`/cache:/.torch \\ -it waikatodatamining/speciesnet:4.0.1_cuda12.1 speciesnet_predict_redis \\ --redis_in images \\ --redis_out predictions \\ --verbose CPU docker run --rm --shm-size 8G --net=host \\ -u $(id -u):$(id -g) -e USER=$USER \\ -v `pwd`:/workspace \\ -v `pwd`/cache:/.cache \\ -v `pwd`/config:/.config \\ -v `pwd`/cache:/.torch \\ -it waikatodatamining/speciesnet:4.0.1_cpu \\ speciesnet_predict_redis \\ --redis_in images \\ --redis_out predictions \\ --verbose","title":"SpeciesNet model"},{"location":"speciesnet/#processing-single-video-file","text":"","title":"Processing single video file"},{"location":"speciesnet/#determine-changes-between-frames","text":"If you want to skip similar frames, e.g., leaves moving in the wind, it pays to look at the frame changes in a video using the calc-frame-changes filter. However, if you are looking for small animals like rodents, you may need to process all frames to avoid missing these. The following command processes the file VID0003.AVI and calculates the changes between frames as a ratio (0-1) with a minimum change of 0.000001 required: idc-convert -l INFO \\ from-video-file \\ -l INFO \\ -i \"./input/VID0003.AVI\" \\ -t od \\ calc-frame-changes \\ -t 0.000001 This will output a histogram similar to this: +1.09e-06 - +3.22e-06 [57] \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588 +3.22e-06 - +5.35e-06 [29] \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258d +5.35e-06 - +7.49e-06 [13] \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258f +7.49e-06 - +9.62e-06 [14] \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2589 +9.62e-06 - +1.18e-05 [15] \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258c +1.18e-05 - +1.39e-05 [10] \u2588\u2588\u2588\u2588\u2588\u2588\u2588 +1.39e-05 - +1.60e-05 [11] \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258a +1.60e-05 - +1.82e-05 [ 7] \u2588\u2588\u2588\u2588\u2589 +1.82e-05 - +2.03e-05 [ 3] \u2588\u2588\u258f +2.03e-05 - +2.24e-05 [ 2] \u2588\u258d +2.24e-05 - +2.46e-05 [ 2] \u2588\u258d +2.46e-05 - +2.67e-05 [ 2] \u2588\u258d +2.67e-05 - +2.88e-05 [ 1] \u258a +2.88e-05 - +3.10e-05 [ 1] \u258a +3.10e-05 - +3.31e-05 [ 5] \u2588\u2588\u2588\u258c +3.31e-05 - +3.52e-05 [ 0] +3.52e-05 - +3.74e-05 [ 0] +3.74e-05 - +3.95e-05 [ 0] +3.95e-05 - +4.16e-05 [ 2] \u2588\u258d +4.16e-05 - +4.38e-05 [ 1] \u258a Using the threshold 5.35e-06 , you will skip frames that would fall into the top two bins of the above histogram.","title":"Determine changes between frames"},{"location":"speciesnet/#extracting-frames","text":"With the threshold for our video determined, we can now extract relevant frames and push them through the SpeciesNet model. The following command processes VID0003.AVI from the input directory ( from-video-file ), determines frames that differ enough from each other ( skip-similar-frames ) to send to the model ( redis-predict-od ), removes any predictions with a score lower than 0.8 ( metadata-od ), discards any results from the model with no predictions ( discard-negatives ) and stores any images with predictions in the output directory in OPEX format : idc-convert -l INFO \\ from-video-file \\ -l INFO \\ -i \"./input/VID0003.AVI\" \\ -t od \\ skip-similar-frames \\ -l INFO \\ -t 5.35e-06 \\ redis-predict-od \\ -l INFO \\ -o images \\ -i predictions \\ -t 10 \\ metadata-od \\ -l INFO \\ -f score \\ -c gt \\ -v 0.8 \\ -a keep \\ discard-negatives \\ -l INFO \\ to-opex-od \\ -l INFO \\ -o ./output","title":"Extracting frames"},{"location":"speciesnet/#processing-multiple-video-files","text":"The from-video-file reader is not limited to just processing a single file, e.g., you can supply multiple file names to the -i option or use a glob like *.AVI . However, with different videos having different lighting and therefore different changes between frames, it makes more sense returning only every nth frame to reduce the processing time. You can either use -n/--nth_frame regardless of the frame-rate of the video or -f/--fps_factor which calculates the actual nth frame to return based on the frame-rate of the video multiplied by this factor. The command below processes all .AVI files from the input directory and retrieves one frame for every second of video footage ( -f 1 ): idc-convert -l INFO \\ from-video-file \\ -l INFO \\ -i \"./input/*.AVI\" \\ -f 1 \\ -t od \\ redis-predict-od \\ -l INFO \\ -o images \\ -i predictions \\ -t 10 \\ metadata-od \\ -l INFO \\ -f score \\ -c gt \\ -v 0.8 \\ -a keep \\ discard-negatives \\ -l INFO \\ to-opex-od \\ -l INFO \\ -o ./output","title":"Processing multiple video files"},{"location":"speciesnet/#visualizing-the-predictions","text":"If you want to quickly generate some composite images, you can use the following command. It will create a new directory overlays in which it will save the generated images. idc-convert -l INFO \\ from-opex-od \\ -l INFO \\ -i \"./output/*.json\" \\ add-annotation-overlay-od \\ -l INFO \\ to-data \\ -l INFO \\ -o ./overlays","title":"Visualizing the predictions"},{"location":"speciesnet/#redirecting-frames-based-on-detected-labels","text":"When identifying species in a frame, it can be helpful automatically sorting the frames into sub-directories based on the labels that were identified within the frame. This can be done by utilizing the placeholder functionality . For this to work, we need to employ two plugins: first, the label needs to be transferred into the metadata of the data container using the label-to-metadata plugin. This plugin outputs a container for as many labels there are in the data container, which can be none, one or many in case of object detection. Second, the metadata value needs to be turned into a placeholder using the metadata-to-placeholder plugin. Therefore, we can change the output part of the Extracting frames pipeline from: ... discard-negatives \\ -l INFO \\ to-opex-od \\ -l INFO \\ -o ./output To this: ... discard-negatives \\ -l INFO \\ label-to-metadata \\ -l INFO \\ -k type \\ metadata-to-placeholder \\ -l INFO \\ -k type \\ -p TYPE \\ to-opex-od \\ -l INFO \\ -o ./output/{TYPE}","title":"Redirecting frames based on detected labels"},{"location":"storage/","text":"The following pipeline components can be used for storing and retrieving data from temporary storage (i.e., a dictionary) that is available through the session object of the pipeline: from-storage - reader retrieves the named storage item set-storage - filter that updates the storage with the data passing through to-storage - writer that updates the storage with the data arriving","title":"Temporary storage"},{"location":"video/","text":"Requirements # Requires the image-dataset-converter-video library. Plugins # Extract frames from video file # The following extracts a total of 100 frames from ./input/video.mp4 , using every 20th frame and stores the images in the ./output directory: idc-convert \\ -l INFO \\ from-video-file \\ -l INFO \\ -i ./input/video.mp4 \\ -t od \\ -n 20 \\ -m 100 \\ to-data \\ -l INFO \\ -o ./output/ Though the data type is object detection ( -t od ), this is not really relevant in this example. If there were other filters, e.g., for applying object detection models via Redis , then it would be necessary to specify the correct type. Capture frames from a webcam (individual JPG files) # The following captures a total of 10 images from a webcam (using every 20th frame) and stores them as individual JPG files in the ./output directory: idc-convert \\ -l INFO \\ from-webcam \\ -l INFO \\ -i 0 \\ -t od \\ -n 20 \\ -m 10 \\ to-data \\ -l INFO \\ -o ./output Capture frames from a webcam (store as MJPEG file) # The following captures a total of 100 images from a webcam (using every 10th frame) and generates a MJPEG file, which can be viewed with, e.g., ffplay : idc-convert \\ -l INFO \\ from-webcam \\ -l INFO \\ -i 0 \\ -t od \\ -n 10 \\ -m 100 \\ to-video-file \\ -l INFO \\ -o ./output/webcam.mjpeg Calculating differences between frames # Depending on the scene, frames may not differ much between each other, making it hard to extract frames by using specific intervals. The calc-frame-changes writer computes differences between frames and output a histogram of these differences. Using the min/max of the bins of the histogram, it is possible to find a suitable change threshold. This threshold can be utilized by the skip-similar-frames filter to discard too similar frames, whittling down the number of extracted frames automatically. The histogram can be output in textual form, CSV or JSON; either on stdout or stored in a file. idc-convert \\ -l INFO \\ from-video-file \\ -l INFO \\ -i ./input/video.mp4 \\ -t od \\ -n 10 \\ -m 2000 \\ calc-frame-changes \\ -f text Skipping similar frames # The following command keeps frames that differ enough from each other and discards all others, using the skip-similar-frames filter. The threshold applied by the filter was previously determined by the calc-frame-changes filter. idc-convert \\ -l INFO \\ from-video-file \\ -l INFO \\ -i ./input/video.mp4 \\ -t od \\ skip-similar-frames \\ -t 0.005 \\ to-data \\ -l INFO \\ -o ./output Youtube video # The from-youtube reader allows you to grab frames from a Youtube video: idc-convert \\ -l INFO \\ from-youtube \\ -l INFO \\ -i \"https://www.youtube.com/watch?v=CFdZWgiAj8I\" \\ -p the_cloud-kitty_flanagan- \\ -t od \\ -n 20 \\ -m 10 \\ to-data \\ -l INFO \\ -o ./output NB: Please make sure not to infringe on any copyright. Youtube live stream # The from-youtube-live reader allows you to grab frames from a Youtube live stream, e.g., wildlife cameras: idc-convert \\ -l INFO \\ from-youtube-live \\ -l INFO \\ -i \"https://www.youtube.com/watch?v=2swy9gysvOY\" \\ -p african_river_wildlife_camera- \\ -t od \\ -n 20 \\ -m 10 \\ to-data \\ -l INFO \\ -o ./output NB: Please make sure not to infringe on any copyright.","title":"Video"},{"location":"video/#requirements","text":"Requires the image-dataset-converter-video library.","title":"Requirements"},{"location":"video/#plugins","text":"","title":"Plugins"},{"location":"video/#extract-frames-from-video-file","text":"The following extracts a total of 100 frames from ./input/video.mp4 , using every 20th frame and stores the images in the ./output directory: idc-convert \\ -l INFO \\ from-video-file \\ -l INFO \\ -i ./input/video.mp4 \\ -t od \\ -n 20 \\ -m 100 \\ to-data \\ -l INFO \\ -o ./output/ Though the data type is object detection ( -t od ), this is not really relevant in this example. If there were other filters, e.g., for applying object detection models via Redis , then it would be necessary to specify the correct type.","title":"Extract frames from video file"},{"location":"video/#capture-frames-from-a-webcam-individual-jpg-files","text":"The following captures a total of 10 images from a webcam (using every 20th frame) and stores them as individual JPG files in the ./output directory: idc-convert \\ -l INFO \\ from-webcam \\ -l INFO \\ -i 0 \\ -t od \\ -n 20 \\ -m 10 \\ to-data \\ -l INFO \\ -o ./output","title":"Capture frames from a webcam (individual JPG files)"},{"location":"video/#capture-frames-from-a-webcam-store-as-mjpeg-file","text":"The following captures a total of 100 images from a webcam (using every 10th frame) and generates a MJPEG file, which can be viewed with, e.g., ffplay : idc-convert \\ -l INFO \\ from-webcam \\ -l INFO \\ -i 0 \\ -t od \\ -n 10 \\ -m 100 \\ to-video-file \\ -l INFO \\ -o ./output/webcam.mjpeg","title":"Capture frames from a webcam (store as MJPEG file)"},{"location":"video/#calculating-differences-between-frames","text":"Depending on the scene, frames may not differ much between each other, making it hard to extract frames by using specific intervals. The calc-frame-changes writer computes differences between frames and output a histogram of these differences. Using the min/max of the bins of the histogram, it is possible to find a suitable change threshold. This threshold can be utilized by the skip-similar-frames filter to discard too similar frames, whittling down the number of extracted frames automatically. The histogram can be output in textual form, CSV or JSON; either on stdout or stored in a file. idc-convert \\ -l INFO \\ from-video-file \\ -l INFO \\ -i ./input/video.mp4 \\ -t od \\ -n 10 \\ -m 2000 \\ calc-frame-changes \\ -f text","title":"Calculating differences between frames"},{"location":"video/#skipping-similar-frames","text":"The following command keeps frames that differ enough from each other and discards all others, using the skip-similar-frames filter. The threshold applied by the filter was previously determined by the calc-frame-changes filter. idc-convert \\ -l INFO \\ from-video-file \\ -l INFO \\ -i ./input/video.mp4 \\ -t od \\ skip-similar-frames \\ -t 0.005 \\ to-data \\ -l INFO \\ -o ./output","title":"Skipping similar frames"},{"location":"video/#youtube-video","text":"The from-youtube reader allows you to grab frames from a Youtube video: idc-convert \\ -l INFO \\ from-youtube \\ -l INFO \\ -i \"https://www.youtube.com/watch?v=CFdZWgiAj8I\" \\ -p the_cloud-kitty_flanagan- \\ -t od \\ -n 20 \\ -m 10 \\ to-data \\ -l INFO \\ -o ./output NB: Please make sure not to infringe on any copyright.","title":"Youtube video"},{"location":"video/#youtube-live-stream","text":"The from-youtube-live reader allows you to grab frames from a Youtube live stream, e.g., wildlife cameras: idc-convert \\ -l INFO \\ from-youtube-live \\ -l INFO \\ -i \"https://www.youtube.com/watch?v=2swy9gysvOY\" \\ -p african_river_wildlife_camera- \\ -t od \\ -n 20 \\ -m 10 \\ to-data \\ -l INFO \\ -o ./output NB: Please make sure not to infringe on any copyright.","title":"Youtube live stream"}]}