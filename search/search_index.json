{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"The image-dataset-converter library (and its dependent libraries) can be used for converting image datasets from one format into another. It has I/O support for the following domains: Image classification Object detection Image segmentation Please refer to the dataset formats section for more details on supported formats. But the library does not just convert datasets, you can also slot in complex filter pipelines to process/clean the data. On this website you can find examples for: Image classification Object detection Image segmentation Filter usage Docker usage Examples for the additional libraries: Image augmentation Image statistics Image visualizations labelme Paddle PDF Redis Video Practical examples: Detecting animals in trail cameras (SpeciesNet)","title":"Home"},{"location":"docker/","text":"Below are examples for using the image-dataset-converter library via its Docker images . Interactive session # The following command starts an interactive session, mapping the current working directory to /workspace : docker run --rm -u $(id -u):$(id -g) \\ -v `pwd`:/workspace \\ -it waikatodatamining/image-dataset-converter:latest Conversion pipeline # The following converts an image classification dataset from the sub-dir format (sub-directory names represent the image classification labels) into the ADAMS format , which stores the label in an associated .report file (Java properties file): docker run --rm -u $(id -u):$(id -g) \\ -v `pwd`:/workspace \\ -it waikatodatamining/image-dataset-converter:latest \\ idc-convert -l INFO \\ from-subdir-ic \\ -l INFO \\ -i /workspace/input/ \\ to-adams-ic \\ -l INFO \\ -o /workspace/output \\ -c classification NB: The input and output directories are located below the current working directory ( pwd ).","title":"Docker usage"},{"location":"docker/#interactive-session","text":"The following command starts an interactive session, mapping the current working directory to /workspace : docker run --rm -u $(id -u):$(id -g) \\ -v `pwd`:/workspace \\ -it waikatodatamining/image-dataset-converter:latest","title":"Interactive session"},{"location":"docker/#conversion-pipeline","text":"The following converts an image classification dataset from the sub-dir format (sub-directory names represent the image classification labels) into the ADAMS format , which stores the label in an associated .report file (Java properties file): docker run --rm -u $(id -u):$(id -g) \\ -v `pwd`:/workspace \\ -it waikatodatamining/image-dataset-converter:latest \\ idc-convert -l INFO \\ from-subdir-ic \\ -l INFO \\ -i /workspace/input/ \\ to-adams-ic \\ -l INFO \\ -o /workspace/output \\ -c classification NB: The input and output directories are located below the current working directory ( pwd ).","title":"Conversion pipeline"},{"location":"filters/","text":"The following sections only show snippets of commands, as there are quite a number of filters available. Bounding box / polygon # With the coerce-bbox filter, you can force annotations to be bounding box only. The reverse is the coerce-mask filter, which ensures that all annotations are available as polygons. Too small or too large? # Using the dimension-discarder filter, you can filter out too large or too small images quite easily: only allow within certain width/height constraint ... dimension-discarder \\ -l INFO \\ --min_height 100 \\ --max_height 200 \\ --min_width 100 \\ --max_width 200 \\ ... only a certain area, but the shape is irrelevant ... dimension-discarder \\ -l INFO \\ --min_area 10000 \\ --max_area 50000 \\ ... Domain conversion # object detection to image classification: With the od-to-ic filter you can convert object detection annotations to image classification. How multiple differing labels are handled can be specified. object detection to image segmentation: The od-to-is filter generates image segmentation data from the bbox/polygon annotations. Annotation management # filter-labels - leaves only the matching labels in the annotations map-labels - for renaming labels remove-classes - removes the specified labels strip-annotations - removes all annotations write-labels - outputs a list of all the encountered labels Meta-data management # metadata - allows comparisons on meta-data values and whether to keep or discard a record in case of a match metadata-from-name - allows extraction of meta-data value from the image name via a regular expression split - adds the field split to the meta-data of the record passing through, which can be acted on with other filters (or stored in the output) Record management # A number of generic record management filters are available: check-duplicate-filenames - when using multiple batches as input, duplicate file names can be an issue when creating a combined output discard-invalid-images - attempts to load the image and discards them in case the loading fails (useful when data acquisition can generate invalid images) discard-negatives - removes records from the stream that have no annotations max-records - limits the number of records passing through randomize-records - when processing batches, this filter can randomize them (seeded or unseeded) record-window - only lets a certain window of records pass through (e.g., the first 1000) rename - allows renaming of images, e.g., prefixing them with a batch number/ID sample - for selecting a random sub-sample from the stream Sub-pipelines # With the tee meta-filter, it is possible to filter the images coming through with a separate sub-pipeline. E.g., converting the incoming data into multiple output formats. The following command loads the VOC XML annotations and saves them in ADAMS and YOLO format in one command: idc-convert \\ -l INFO \\ from-voc-od \\ -l INFO \\ -i \"./voc/*.xml\" \\ tee \\ -f \"to-adams-od -o ./adams-tee/\" \\ tee \\ -f \"to-yolo-od -o ./yolo-tee/ --labels ./yolo-tee/labels.txt\"","title":"Filter usage"},{"location":"filters/#bounding-box-polygon","text":"With the coerce-bbox filter, you can force annotations to be bounding box only. The reverse is the coerce-mask filter, which ensures that all annotations are available as polygons.","title":"Bounding box / polygon"},{"location":"filters/#too-small-or-too-large","text":"Using the dimension-discarder filter, you can filter out too large or too small images quite easily: only allow within certain width/height constraint ... dimension-discarder \\ -l INFO \\ --min_height 100 \\ --max_height 200 \\ --min_width 100 \\ --max_width 200 \\ ... only a certain area, but the shape is irrelevant ... dimension-discarder \\ -l INFO \\ --min_area 10000 \\ --max_area 50000 \\ ...","title":"Too small or too large?"},{"location":"filters/#domain-conversion","text":"object detection to image classification: With the od-to-ic filter you can convert object detection annotations to image classification. How multiple differing labels are handled can be specified. object detection to image segmentation: The od-to-is filter generates image segmentation data from the bbox/polygon annotations.","title":"Domain conversion"},{"location":"filters/#annotation-management","text":"filter-labels - leaves only the matching labels in the annotations map-labels - for renaming labels remove-classes - removes the specified labels strip-annotations - removes all annotations write-labels - outputs a list of all the encountered labels","title":"Annotation management"},{"location":"filters/#meta-data-management","text":"metadata - allows comparisons on meta-data values and whether to keep or discard a record in case of a match metadata-from-name - allows extraction of meta-data value from the image name via a regular expression split - adds the field split to the meta-data of the record passing through, which can be acted on with other filters (or stored in the output)","title":"Meta-data management"},{"location":"filters/#record-management","text":"A number of generic record management filters are available: check-duplicate-filenames - when using multiple batches as input, duplicate file names can be an issue when creating a combined output discard-invalid-images - attempts to load the image and discards them in case the loading fails (useful when data acquisition can generate invalid images) discard-negatives - removes records from the stream that have no annotations max-records - limits the number of records passing through randomize-records - when processing batches, this filter can randomize them (seeded or unseeded) record-window - only lets a certain window of records pass through (e.g., the first 1000) rename - allows renaming of images, e.g., prefixing them with a batch number/ID sample - for selecting a random sub-sample from the stream","title":"Record management"},{"location":"filters/#sub-pipelines","text":"With the tee meta-filter, it is possible to filter the images coming through with a separate sub-pipeline. E.g., converting the incoming data into multiple output formats. The following command loads the VOC XML annotations and saves them in ADAMS and YOLO format in one command: idc-convert \\ -l INFO \\ from-voc-od \\ -l INFO \\ -i \"./voc/*.xml\" \\ tee \\ -f \"to-adams-od -o ./adams-tee/\" \\ tee \\ -f \"to-yolo-od -o ./yolo-tee/ --labels ./yolo-tee/labels.txt\"","title":"Sub-pipelines"},{"location":"image_classification/","text":"Readers and writers for image classification have the -ic suffix. Download the 17 flowers image classification dataset and extract it. Plugins # sub-dir to ADAMS # The following converts an image classification dataset from the sub-dir format (sub-directory names represent the image classification labels) into the ADAMS format , which stores the label in an associated .report file (Java properties file): idc-convert -l INFO \\ from-subdir-ic \\ -l INFO \\ -i ./subdir/ \\ to-adams-ic \\ -l INFO \\ -o ./adams \\ -c classification sub-dir (randomized train/val/test splits) # By enforcing batch-processing --force_batch and using the randomize-records filter, randomized train/val/test splits (writers typically support generating splits) can be generated like this: idc-convert -l INFO --force_batch \\ from-subdir-ic \\ -l INFO \\ -i ./subdir/ \\ randomize-records \\ -s 42 \\ to-subdir-ic \\ -l INFO \\ -o ./subdir-split \\ --split_names train val test \\ --split_ratios 70 15 15","title":"Image classification"},{"location":"image_classification/#plugins","text":"","title":"Plugins"},{"location":"image_classification/#sub-dir-to-adams","text":"The following converts an image classification dataset from the sub-dir format (sub-directory names represent the image classification labels) into the ADAMS format , which stores the label in an associated .report file (Java properties file): idc-convert -l INFO \\ from-subdir-ic \\ -l INFO \\ -i ./subdir/ \\ to-adams-ic \\ -l INFO \\ -o ./adams \\ -c classification","title":"sub-dir to ADAMS"},{"location":"image_classification/#sub-dir-randomized-trainvaltest-splits","text":"By enforcing batch-processing --force_batch and using the randomize-records filter, randomized train/val/test splits (writers typically support generating splits) can be generated like this: idc-convert -l INFO --force_batch \\ from-subdir-ic \\ -l INFO \\ -i ./subdir/ \\ randomize-records \\ -s 42 \\ to-subdir-ic \\ -l INFO \\ -o ./subdir-split \\ --split_names train val test \\ --split_ratios 70 15 15","title":"sub-dir (randomized train/val/test splits)"},{"location":"image_segmentation/","text":"Readers and writers for image segmentation have the -is suffix. Download the blue channel archive of the camvid dataset and extract it. Plugins # Blue channel to Indexed PNG # The following command-line will convert it into a dataset using indexed PNG files: idc-convert \\ -l INFO \\ from-blue-channel-is \\ -l INFO \\ -i \"./bluechannel/*.png\" \\ --labels Animal Archway Bicyclist Bridge Building Car CartLuggagePram Child Column_Pole \\ Fence LaneMkgsDriv LaneMkgsNonDriv Misc_Text MotorcycleScooter OtherMoving ParkingBlock \\ Pedestrian Road RoadShoulder Sidewalk SignSymbol Sky SUVPickupTruck TrafficCone \\ TrafficLight Train Tree Truck_Bus Tunnel VegetationMisc Void Wall \\ to-indexed-png-is \\ -l INFO \\ -p x11 \\ -o ./indexedpng NB: Uses the X11 color palette for the palette in the PNGs. Here is an example (0001TP_007050.png): Blue channel to Indexed PNG (cyclists only) # By applying filters, you can also generate subsets, e.g., for building more specialized models. The following will extract only images that have cyclists and discard all other annotations ( filter-labels ). Images with no annotations left will get discarded ( discard-negatives ): idc-convert \\ -l INFO \\ from-blue-channel-is \\ -l INFO \\ -i \"./bluechannel/*.png\" \\ --labels Animal Archway Bicyclist Bridge Building Car CartLuggagePram Child Column_Pole \\ Fence LaneMkgsDriv LaneMkgsNonDriv Misc_Text MotorcycleScooter OtherMoving ParkingBlock \\ Pedestrian Road RoadShoulder Sidewalk SignSymbol Sky SUVPickupTruck TrafficCone \\ TrafficLight Train Tree Truck_Bus Tunnel VegetationMisc Void Wall \\ filter-labels \\ -l INFO \\ --labels Bicyclist \\ discard-negatives \\ -l INFO \\ to-indexed-png-is \\ -l INFO \\ -p x11 \\ -o ./indexedpng-cyclists Here is an example (0001TP_007380.png):","title":"Image segmentation"},{"location":"image_segmentation/#plugins","text":"","title":"Plugins"},{"location":"image_segmentation/#blue-channel-to-indexed-png","text":"The following command-line will convert it into a dataset using indexed PNG files: idc-convert \\ -l INFO \\ from-blue-channel-is \\ -l INFO \\ -i \"./bluechannel/*.png\" \\ --labels Animal Archway Bicyclist Bridge Building Car CartLuggagePram Child Column_Pole \\ Fence LaneMkgsDriv LaneMkgsNonDriv Misc_Text MotorcycleScooter OtherMoving ParkingBlock \\ Pedestrian Road RoadShoulder Sidewalk SignSymbol Sky SUVPickupTruck TrafficCone \\ TrafficLight Train Tree Truck_Bus Tunnel VegetationMisc Void Wall \\ to-indexed-png-is \\ -l INFO \\ -p x11 \\ -o ./indexedpng NB: Uses the X11 color palette for the palette in the PNGs. Here is an example (0001TP_007050.png):","title":"Blue channel to Indexed PNG"},{"location":"image_segmentation/#blue-channel-to-indexed-png-cyclists-only","text":"By applying filters, you can also generate subsets, e.g., for building more specialized models. The following will extract only images that have cyclists and discard all other annotations ( filter-labels ). Images with no annotations left will get discarded ( discard-negatives ): idc-convert \\ -l INFO \\ from-blue-channel-is \\ -l INFO \\ -i \"./bluechannel/*.png\" \\ --labels Animal Archway Bicyclist Bridge Building Car CartLuggagePram Child Column_Pole \\ Fence LaneMkgsDriv LaneMkgsNonDriv Misc_Text MotorcycleScooter OtherMoving ParkingBlock \\ Pedestrian Road RoadShoulder Sidewalk SignSymbol Sky SUVPickupTruck TrafficCone \\ TrafficLight Train Tree Truck_Bus Tunnel VegetationMisc Void Wall \\ filter-labels \\ -l INFO \\ --labels Bicyclist \\ discard-negatives \\ -l INFO \\ to-indexed-png-is \\ -l INFO \\ -p x11 \\ -o ./indexedpng-cyclists Here is an example (0001TP_007380.png):","title":"Blue channel to Indexed PNG (cyclists only)"},{"location":"imgaug/","text":"Requirements # Requires the image-dataset-converter-imgaug library. Download the 17 flowers object detection VOC XML dataset and extract it. Plugins # Convert VOC XML to YOLO (crop/rotate) # The following converts VOC XML annotations into YOLO ones. It also augments the dataset with randomly adding cropped/rotated images to the stream ( -m add ), with the annotations getting processed accordingly: idc-convert \\ -l INFO \\ from-voc-od \\ -l INFO \\ -i \"./voc/*.xml\" \\ crop \\ -m add \\ -f 0.1 \\ -t 0.2 \\ -u \\ rotate \\ -m add \\ -f \"-45\" \\ -t 45 \\ to-yolo-od \\ -l INFO \\ -o ./yolo-split-augmented \\ --labels ./yolo-split-augmented/labels.txt \\ --split_names train val test \\ --split_ratios 70 15 15 Here is an example of a processed image (image_0001.jpg): Convert VOC XML to MS COCO (resize) # The following converts VOC XML annotations into smaller MS COCO ones, using a maximum width of 300 while keeping the aspect ratio intact. The data also gets split into train/validation/test: idc-convert \\ -l INFO \\ from-voc-od \\ -l INFO \\ -i \"./voc/*.xml\" \\ resize \\ -W 300 \\ -H keep-aspect-ratio \\ to-coco-od \\ -l INFO \\ -o ./coco-split-small \\ --split_names train val test \\ --split_ratios 70 15 15 Split images into smaller ones # With the sub-images filter it is possible to split images into smaller ones or extract just specific regions of interest from images. The idc-generate-regions tool can be used to generate regions for a set number of rows/columns or fixed row heights/column widths. For images that are at maximum 800x800, we can generate regions for a 2x2 grid as follows: idc-generate-regions \\ -l INFO \\ -W 800 \\ -H 800 \\ -r 2 \\ -c 2 Will output this, with each quadruplet consisting of x, y, width and height: 0,0,400,400 400,0,400,400 0,400,400,400 400,400,400,400 These regions we can now use with the sub-images filter to generate smaller images: idc-convert \\ -l INFO \\ from-voc-od \\ -l INFO \\ -i \"./voc/*.xml\" \\ sub-images \\ -l INFO \\ -r 0,0,400,400 400,0,400,400 0,400,400,400 400,400,400,400 \\ -p \\ -e \\ to-yolo-od \\ -l INFO \\ -o ./yolo-sub \\ --labels ./yolo-sub/labels.txt Using -p we will keep partial annotations, ones that got cut off a bit, and with -e we will suppress images that have no annotations in them. Using the meta-sub-images filter, you can feed the small images that were generated through the specified base filter. The output of this filter gets reassembled and forwarded. This base filter can be a simple filter or a more complex pipeline, which passes the images through a Redis model , for instance.","title":"Image augmentation"},{"location":"imgaug/#requirements","text":"Requires the image-dataset-converter-imgaug library. Download the 17 flowers object detection VOC XML dataset and extract it.","title":"Requirements"},{"location":"imgaug/#plugins","text":"","title":"Plugins"},{"location":"imgaug/#convert-voc-xml-to-yolo-croprotate","text":"The following converts VOC XML annotations into YOLO ones. It also augments the dataset with randomly adding cropped/rotated images to the stream ( -m add ), with the annotations getting processed accordingly: idc-convert \\ -l INFO \\ from-voc-od \\ -l INFO \\ -i \"./voc/*.xml\" \\ crop \\ -m add \\ -f 0.1 \\ -t 0.2 \\ -u \\ rotate \\ -m add \\ -f \"-45\" \\ -t 45 \\ to-yolo-od \\ -l INFO \\ -o ./yolo-split-augmented \\ --labels ./yolo-split-augmented/labels.txt \\ --split_names train val test \\ --split_ratios 70 15 15 Here is an example of a processed image (image_0001.jpg):","title":"Convert VOC XML to YOLO (crop/rotate)"},{"location":"imgaug/#convert-voc-xml-to-ms-coco-resize","text":"The following converts VOC XML annotations into smaller MS COCO ones, using a maximum width of 300 while keeping the aspect ratio intact. The data also gets split into train/validation/test: idc-convert \\ -l INFO \\ from-voc-od \\ -l INFO \\ -i \"./voc/*.xml\" \\ resize \\ -W 300 \\ -H keep-aspect-ratio \\ to-coco-od \\ -l INFO \\ -o ./coco-split-small \\ --split_names train val test \\ --split_ratios 70 15 15","title":"Convert VOC XML to MS COCO (resize)"},{"location":"imgaug/#split-images-into-smaller-ones","text":"With the sub-images filter it is possible to split images into smaller ones or extract just specific regions of interest from images. The idc-generate-regions tool can be used to generate regions for a set number of rows/columns or fixed row heights/column widths. For images that are at maximum 800x800, we can generate regions for a 2x2 grid as follows: idc-generate-regions \\ -l INFO \\ -W 800 \\ -H 800 \\ -r 2 \\ -c 2 Will output this, with each quadruplet consisting of x, y, width and height: 0,0,400,400 400,0,400,400 0,400,400,400 400,400,400,400 These regions we can now use with the sub-images filter to generate smaller images: idc-convert \\ -l INFO \\ from-voc-od \\ -l INFO \\ -i \"./voc/*.xml\" \\ sub-images \\ -l INFO \\ -r 0,0,400,400 400,0,400,400 0,400,400,400 400,400,400,400 \\ -p \\ -e \\ to-yolo-od \\ -l INFO \\ -o ./yolo-sub \\ --labels ./yolo-sub/labels.txt Using -p we will keep partial annotations, ones that got cut off a bit, and with -e we will suppress images that have no annotations in them. Using the meta-sub-images filter, you can feed the small images that were generated through the specified base filter. The output of this filter gets reassembled and forwarded. This base filter can be a simple filter or a more complex pipeline, which passes the images through a Redis model , for instance.","title":"Split images into smaller ones"},{"location":"imgstats/","text":"Requirements # Requires the image-dataset-converter-imgstats library. Download the 17 flowers object detection VOC XML dataset and extract it. Plugins # Label distribution # Using the label-dist writer, you can output a distribution of the labels within a dataset as follows: idc-convert \\ -l INFO \\ from-voc-od \\ -l INFO \\ -i \"./voc/*.xml\" \\ label-dist \\ -l INFO This will output the following: Bluebell: 28 Buttercup: 54 ColtsFoot: 55 Crocus: 50 Daffodil: 71 Daisy: 57 Dandelion: 43 Fritillary: 65 Iris: 77 LilyValley: 17 Pansy: 56 Snowdrop: 50 Sunflower: 71 Tigerlily: 50 Tulip: 41 Windflower: 63 NB: You can also generate CSV or JSON output. Area histogram # Knowing the distribution of the extends of your annotations can be quite useful, e.g., for determining whether down-sampling can be an option. The area-histogram writer generates histograms for all objects combined and one for each category: idc-convert \\ -l INFO \\ from-voc-od \\ -l INFO \\ -i \"./voc/*.xml\" \\ area-histogram \\ -l INFO The output for the 17 flowers dataset looks like this: ALL: +1.26e+04 - +3.29e+04 [ 14] \u2588\u2588\u2588\u2588\u2588\u258e +3.29e+04 - +5.33e+04 [ 30] \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258d +5.33e+04 - +7.36e+04 [ 36] \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258b +7.36e+04 - +9.40e+04 [ 54] \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258d +9.40e+04 - +1.14e+05 [ 71] \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258a +1.14e+05 - +1.35e+05 [ 71] \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258a +1.35e+05 - +1.55e+05 [ 95] \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2589 +1.55e+05 - +1.75e+05 [104] \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258e +1.75e+05 - +1.96e+05 [ 94] \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258c +1.96e+05 - +2.16e+05 [106] \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588 +2.16e+05 - +2.37e+05 [ 77] \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588 +2.37e+05 - +2.57e+05 [ 48] \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258f +2.57e+05 - +2.77e+05 [ 14] \u2588\u2588\u2588\u2588\u2588\u258e +2.77e+05 - +2.98e+05 [ 10] \u2588\u2588\u2588\u258a +2.98e+05 - +3.18e+05 [ 9] \u2588\u2588\u2588\u258d +3.18e+05 - +3.38e+05 [ 6] \u2588\u2588\u258e +3.38e+05 - +3.59e+05 [ 2] \u258a +3.59e+05 - +3.79e+05 [ 5] \u2588\u2589 +3.79e+05 - +3.99e+05 [ 1] \u258d +3.99e+05 - +4.20e+05 [ 1] \u258d Bluebell: +7.68e+04 - +9.18e+04 [1] \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588 +9.18e+04 - +1.07e+05 [1] \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588 +1.07e+05 - +1.22e+05 [2] \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588 +1.22e+05 - +1.37e+05 [0] +1.37e+05 - +1.51e+05 [0] +1.51e+05 - +1.66e+05 [2] \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588 +1.66e+05 - +1.81e+05 [2] \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588 +1.81e+05 - +1.96e+05 [0] +1.96e+05 - +2.11e+05 [5] \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588 +2.11e+05 - +2.26e+05 [1] \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588 +2.26e+05 - +2.41e+05 [4] \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588 +2.41e+05 - +2.56e+05 [3] \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588 +2.56e+05 - +2.71e+05 [1] \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588 +2.71e+05 - +2.86e+05 [0] +2.86e+05 - +3.01e+05 [2] \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588 +3.01e+05 - +3.16e+05 [0] +3.16e+05 - +3.31e+05 [1] \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588 +3.31e+05 - +3.46e+05 [0] +3.46e+05 - +3.61e+05 [0] +3.61e+05 - +3.76e+05 [3] \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588 Buttercup: ... NB: You can also generate CSV or JSON output.","title":"Image statistics"},{"location":"imgstats/#requirements","text":"Requires the image-dataset-converter-imgstats library. Download the 17 flowers object detection VOC XML dataset and extract it.","title":"Requirements"},{"location":"imgstats/#plugins","text":"","title":"Plugins"},{"location":"imgstats/#label-distribution","text":"Using the label-dist writer, you can output a distribution of the labels within a dataset as follows: idc-convert \\ -l INFO \\ from-voc-od \\ -l INFO \\ -i \"./voc/*.xml\" \\ label-dist \\ -l INFO This will output the following: Bluebell: 28 Buttercup: 54 ColtsFoot: 55 Crocus: 50 Daffodil: 71 Daisy: 57 Dandelion: 43 Fritillary: 65 Iris: 77 LilyValley: 17 Pansy: 56 Snowdrop: 50 Sunflower: 71 Tigerlily: 50 Tulip: 41 Windflower: 63 NB: You can also generate CSV or JSON output.","title":"Label distribution"},{"location":"imgstats/#area-histogram","text":"Knowing the distribution of the extends of your annotations can be quite useful, e.g., for determining whether down-sampling can be an option. The area-histogram writer generates histograms for all objects combined and one for each category: idc-convert \\ -l INFO \\ from-voc-od \\ -l INFO \\ -i \"./voc/*.xml\" \\ area-histogram \\ -l INFO The output for the 17 flowers dataset looks like this: ALL: +1.26e+04 - +3.29e+04 [ 14] \u2588\u2588\u2588\u2588\u2588\u258e +3.29e+04 - +5.33e+04 [ 30] \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258d +5.33e+04 - +7.36e+04 [ 36] \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258b +7.36e+04 - +9.40e+04 [ 54] \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258d +9.40e+04 - +1.14e+05 [ 71] \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258a +1.14e+05 - +1.35e+05 [ 71] \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258a +1.35e+05 - +1.55e+05 [ 95] \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2589 +1.55e+05 - +1.75e+05 [104] \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258e +1.75e+05 - +1.96e+05 [ 94] \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258c +1.96e+05 - +2.16e+05 [106] \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588 +2.16e+05 - +2.37e+05 [ 77] \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588 +2.37e+05 - +2.57e+05 [ 48] \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258f +2.57e+05 - +2.77e+05 [ 14] \u2588\u2588\u2588\u2588\u2588\u258e +2.77e+05 - +2.98e+05 [ 10] \u2588\u2588\u2588\u258a +2.98e+05 - +3.18e+05 [ 9] \u2588\u2588\u2588\u258d +3.18e+05 - +3.38e+05 [ 6] \u2588\u2588\u258e +3.38e+05 - +3.59e+05 [ 2] \u258a +3.59e+05 - +3.79e+05 [ 5] \u2588\u2589 +3.79e+05 - +3.99e+05 [ 1] \u258d +3.99e+05 - +4.20e+05 [ 1] \u258d Bluebell: +7.68e+04 - +9.18e+04 [1] \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588 +9.18e+04 - +1.07e+05 [1] \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588 +1.07e+05 - +1.22e+05 [2] \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588 +1.22e+05 - +1.37e+05 [0] +1.37e+05 - +1.51e+05 [0] +1.51e+05 - +1.66e+05 [2] \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588 +1.66e+05 - +1.81e+05 [2] \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588 +1.81e+05 - +1.96e+05 [0] +1.96e+05 - +2.11e+05 [5] \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588 +2.11e+05 - +2.26e+05 [1] \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588 +2.26e+05 - +2.41e+05 [4] \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588 +2.41e+05 - +2.56e+05 [3] \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588 +2.56e+05 - +2.71e+05 [1] \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588 +2.71e+05 - +2.86e+05 [0] +2.86e+05 - +3.01e+05 [2] \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588 +3.01e+05 - +3.16e+05 [0] +3.16e+05 - +3.31e+05 [1] \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588 +3.31e+05 - +3.46e+05 [0] +3.46e+05 - +3.61e+05 [0] +3.61e+05 - +3.76e+05 [3] \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588 Buttercup: ... NB: You can also generate CSV or JSON output.","title":"Area histogram"},{"location":"imgvis/","text":"Requirements # Requires the image-dataset-converter-imgvis module. Plugins # Annotation overlays (image classification) # Download the 17 flowers image classification dataset and extract it. Having annotations separate from the images is a necessity when training models, but it can be a hindrance when trying to inspect the data. Adding overlays with the annotations is therefore a useful step sometimes. The following command using the add-annotation-overlay-ic filter adds the labels to the images and just outputs these modified images: idc-convert -l INFO \\ from-subdir-ic \\ -l INFO \\ -i ./subdir/ \\ add-annotation-overlay-ic \\ -l INFO \\ to-data \\ -l INFO \\ -o ./17flowers-ic-overlay Here is an example (image_0008.jpg): Annotation overlays (object detection) # Download the 17 flowers object detection VOC XML dataset and extract it. With the add-annotation-overlay-od filter you can overlay the objects on the images: idc-convert -l INFO \\ from-voc-od \\ -l INFO \\ -i \"./voc/*.xml\" \\ add-annotation-overlay-od \\ -l INFO \\ to-data \\ -l INFO \\ -o ./17flowers-od-overlay Here is an example (image_0014.jpg): Annotation overlays (image segmentation) # Download the blue channel archive of the camvid dataset and extract it. With the add-annotation-overlay-is filter you can overlay the segmentation layers: idc-convert -l INFO \\ from-blue-channel-is \\ -l INFO \\ -i \"./bluechannel/*.png\" \\ --labels Animal Archway Bicyclist Bridge Building Car CartLuggagePram Child Column_Pole \\ Fence LaneMkgsDriv LaneMkgsNonDriv Misc_Text MotorcycleScooter OtherMoving ParkingBlock \\ Pedestrian Road RoadShoulder Sidewalk SignSymbol Sky SUVPickupTruck TrafficCone \\ TrafficLight Train Tree Truck_Bus Tunnel VegetationMisc Void Wall \\ add-annotation-overlay-is \\ -l INFO \\ to-data \\ -l INFO \\ -o ./camvid-is-overlay Here is an example (0001TP_006900.jpg): Combining all annotations in one image # Using the to-annotation-overlay-od writer, you can generate a single PNG file that contains the outlines (bbox or polygon) of your object detection annotations. That way, you can see whether certain areas are under-represented with annotations and where hotspots are: idc-convert -l INFO \\ from-voc-od \\ -l INFO \\ -i \"./voc/*.xml\" \\ to-annotation-overlay-od \\ -l INFO \\ -o ./17flowers-annotations-overlay.png The 17 flowers dataset looks like this:","title":"Image visualizations"},{"location":"imgvis/#requirements","text":"Requires the image-dataset-converter-imgvis module.","title":"Requirements"},{"location":"imgvis/#plugins","text":"","title":"Plugins"},{"location":"imgvis/#annotation-overlays-image-classification","text":"Download the 17 flowers image classification dataset and extract it. Having annotations separate from the images is a necessity when training models, but it can be a hindrance when trying to inspect the data. Adding overlays with the annotations is therefore a useful step sometimes. The following command using the add-annotation-overlay-ic filter adds the labels to the images and just outputs these modified images: idc-convert -l INFO \\ from-subdir-ic \\ -l INFO \\ -i ./subdir/ \\ add-annotation-overlay-ic \\ -l INFO \\ to-data \\ -l INFO \\ -o ./17flowers-ic-overlay Here is an example (image_0008.jpg):","title":"Annotation overlays (image classification)"},{"location":"imgvis/#annotation-overlays-object-detection","text":"Download the 17 flowers object detection VOC XML dataset and extract it. With the add-annotation-overlay-od filter you can overlay the objects on the images: idc-convert -l INFO \\ from-voc-od \\ -l INFO \\ -i \"./voc/*.xml\" \\ add-annotation-overlay-od \\ -l INFO \\ to-data \\ -l INFO \\ -o ./17flowers-od-overlay Here is an example (image_0014.jpg):","title":"Annotation overlays (object detection)"},{"location":"imgvis/#annotation-overlays-image-segmentation","text":"Download the blue channel archive of the camvid dataset and extract it. With the add-annotation-overlay-is filter you can overlay the segmentation layers: idc-convert -l INFO \\ from-blue-channel-is \\ -l INFO \\ -i \"./bluechannel/*.png\" \\ --labels Animal Archway Bicyclist Bridge Building Car CartLuggagePram Child Column_Pole \\ Fence LaneMkgsDriv LaneMkgsNonDriv Misc_Text MotorcycleScooter OtherMoving ParkingBlock \\ Pedestrian Road RoadShoulder Sidewalk SignSymbol Sky SUVPickupTruck TrafficCone \\ TrafficLight Train Tree Truck_Bus Tunnel VegetationMisc Void Wall \\ add-annotation-overlay-is \\ -l INFO \\ to-data \\ -l INFO \\ -o ./camvid-is-overlay Here is an example (0001TP_006900.jpg):","title":"Annotation overlays (image segmentation)"},{"location":"imgvis/#combining-all-annotations-in-one-image","text":"Using the to-annotation-overlay-od writer, you can generate a single PNG file that contains the outlines (bbox or polygon) of your object detection annotations. That way, you can see whether certain areas are under-represented with annotations and where hotspots are: idc-convert -l INFO \\ from-voc-od \\ -l INFO \\ -i \"./voc/*.xml\" \\ to-annotation-overlay-od \\ -l INFO \\ -o ./17flowers-annotations-overlay.png The 17 flowers dataset looks like this:","title":"Combining all annotations in one image"},{"location":"labelme/","text":"Requirements # labelme support requires the image-dataset-converter-labelme library. Image classification # The following converts all labelme .json files with image/label definitions into sub-directory format: idc-convert -l INFO \\ from-labelme-ic \\ -l INFO \\ -i ./labelme/*.json \\ to-subdir-ic \\ -l INFO \\ -o ./subdir/ And this reads annotations in ADAMS .report format and stores in labelme json format and also specifies all possible labels: idc-convert -l INFO \\ from-adams-ic \\ -l INFO \\ -i ./adams/*.report \\ -c classification \\ to-labelme-ic \\ -l INFO \\ --labels cat dog bunny bird \\ -o ./labelme Object detection # The following converts ADAMS object annotations to labelme rectangle shapes: idc-convert -l INFO \\ from-adams-od \\ -l INFO \\ -i ./adams/*.report \\ coerce-box \\ to-labelme-od \\ -l INFO \\ -o ./labelme NB: coerce-box removes any polygon annotations and only leaves the bbox. Instance segmentation # The following converts ADAMS object annotations to labelme polygon shapes: idc-convert -l INFO \\ from-adams-od \\ -l INFO \\ -i ./adams/*.report \\ coerce-mask \\ to-labelme-od \\ -l INFO \\ -o ./labelme NB: coerce-mask ensures that polygon annotations are present, even if it is only the outline of a bbox.","title":"Requirements"},{"location":"labelme/#requirements","text":"labelme support requires the image-dataset-converter-labelme library.","title":"Requirements"},{"location":"labelme/#image-classification","text":"The following converts all labelme .json files with image/label definitions into sub-directory format: idc-convert -l INFO \\ from-labelme-ic \\ -l INFO \\ -i ./labelme/*.json \\ to-subdir-ic \\ -l INFO \\ -o ./subdir/ And this reads annotations in ADAMS .report format and stores in labelme json format and also specifies all possible labels: idc-convert -l INFO \\ from-adams-ic \\ -l INFO \\ -i ./adams/*.report \\ -c classification \\ to-labelme-ic \\ -l INFO \\ --labels cat dog bunny bird \\ -o ./labelme","title":"Image classification"},{"location":"labelme/#object-detection","text":"The following converts ADAMS object annotations to labelme rectangle shapes: idc-convert -l INFO \\ from-adams-od \\ -l INFO \\ -i ./adams/*.report \\ coerce-box \\ to-labelme-od \\ -l INFO \\ -o ./labelme NB: coerce-box removes any polygon annotations and only leaves the bbox.","title":"Object detection"},{"location":"labelme/#instance-segmentation","text":"The following converts ADAMS object annotations to labelme polygon shapes: idc-convert -l INFO \\ from-adams-od \\ -l INFO \\ -i ./adams/*.report \\ coerce-mask \\ to-labelme-od \\ -l INFO \\ -o ./labelme NB: coerce-mask ensures that polygon annotations are present, even if it is only the outline of a bbox.","title":"Instance segmentation"},{"location":"multi/","text":"Most of the time, conversion pipelines will only need to read from one source and output to a single target. However, there can be cases where datasets of different types need merging ( multiple inputs ) or datasets of different types need to generated for different frameworks ( multiple outputs ). To cater for these scenarios, the following two meta plugins are available: from-multi - reads from one or more sources using the specified readers to-multi - forwards the incoming data to the one or more writers There is one restriction, each of the base reader/writer must be from the same data domain. Multiple inputs # The following command reads a dataset in ADAMS object detection format and MS COCO, with the combined output being saved in OPEX JSON format: idc-convert \\ -l INFO \\ from-multi \\ -l INFO \\ -t od \\ -r \"from-adams-od -l INFO -i {CWD}/input/*.report\" \\ \"from-coco-od -l INFO -i {CWD}/input/*.json\" \\ to-opex-od \\ -l INFO \\ -o \"{CWD}/output\" Multiple outputs # Below, the source data is in ADAMS object detection format and will be converted to OPEX JSON and MS COCO: idc-convert \\ -l INFO \\ from-adams-od \\ -l INFO \\ -i {CWD}/input/*.report \\ to-multi \\ -l INFO \\ -t od \\ -w \"to-opex-od -l INFO -o {CWD}/output/opex\" \\ \"to-coco-od -l INFO -o {CWD}/output/coco\"","title":"Multiple I/O"},{"location":"multi/#multiple-inputs","text":"The following command reads a dataset in ADAMS object detection format and MS COCO, with the combined output being saved in OPEX JSON format: idc-convert \\ -l INFO \\ from-multi \\ -l INFO \\ -t od \\ -r \"from-adams-od -l INFO -i {CWD}/input/*.report\" \\ \"from-coco-od -l INFO -i {CWD}/input/*.json\" \\ to-opex-od \\ -l INFO \\ -o \"{CWD}/output\"","title":"Multiple inputs"},{"location":"multi/#multiple-outputs","text":"Below, the source data is in ADAMS object detection format and will be converted to OPEX JSON and MS COCO: idc-convert \\ -l INFO \\ from-adams-od \\ -l INFO \\ -i {CWD}/input/*.report \\ to-multi \\ -l INFO \\ -t od \\ -w \"to-opex-od -l INFO -o {CWD}/output/opex\" \\ \"to-coco-od -l INFO -o {CWD}/output/coco\"","title":"Multiple outputs"},{"location":"object_detection/","text":"Readers and writers for object detection have the -od suffix. Download the 17 flowers object detection VOC XML dataset and extract it. Plugins # VOC XML to MS COCO # The following converts the VOC XML dataset into MS COCO format, a format used by frameworks like Detectron2 or MMDetection : idc-convert \\ -l INFO \\ from-voc-od \\ -l INFO \\ -i \"./voc/*.xml\" \\ to-coco-od \\ -l INFO \\ -o ./coco VOC XML to YOLO (train/val/test splits) # You can also split the data, e.g., into train, validation and test subsets. The following converts the VOC XML now into YOLO format: idc-convert \\ -l INFO \\ from-voc-od \\ -l INFO \\ -i \"./voc/*.xml\" \\ to-yolo-od \\ -l INFO \\ -o ./yolo-split \\ --labels ./yolo-split/labels.txt \\ --split_names train val test \\ --split_ratios 70 15 15 NB: The subsets will be placed into sub-directories according to the split name.","title":"Object detection"},{"location":"object_detection/#plugins","text":"","title":"Plugins"},{"location":"object_detection/#voc-xml-to-ms-coco","text":"The following converts the VOC XML dataset into MS COCO format, a format used by frameworks like Detectron2 or MMDetection : idc-convert \\ -l INFO \\ from-voc-od \\ -l INFO \\ -i \"./voc/*.xml\" \\ to-coco-od \\ -l INFO \\ -o ./coco","title":"VOC XML to MS COCO"},{"location":"object_detection/#voc-xml-to-yolo-trainvaltest-splits","text":"You can also split the data, e.g., into train, validation and test subsets. The following converts the VOC XML now into YOLO format: idc-convert \\ -l INFO \\ from-voc-od \\ -l INFO \\ -i \"./voc/*.xml\" \\ to-yolo-od \\ -l INFO \\ -o ./yolo-split \\ --labels ./yolo-split/labels.txt \\ --split_names train val test \\ --split_ratios 70 15 15 NB: The subsets will be placed into sub-directories according to the split name.","title":"VOC XML to YOLO (train/val/test splits)"},{"location":"paddle/","text":"Requirements # Paddle support requires the image-dataset-converter-paddle library. Image classification # With the following command, a sub-dir structured is converted into Paddle format. The integer ID/label text mapping is stored in ./paddle/labels.map , the label ID/file mapping in files.txt and the images are stored relatively to the output directory in the jpg sub-directory: idc-convert -l INFO \\ from-subdir-ic \\ -l INFO \\ -i ./subdir \\ to-paddle-ic \\ -l INFO \\ -o ./paddle \\ --id_label_map ./paddle/labels.map \\ --file_label_map files.txt \\ --relative_path jpg The following converts all Paddle .txt files with image/label definitions back into sub-directory format: idc-convert -l INFO \\ from-paddle-ic \\ -l INFO \\ -i ./paddle/*.txt \\ -m ./paddle/label_list.map \\ to-subdir-ic \\ -l INFO \\ -o ./subdir/ Image segmentation # The following converts indexed PNGs into Paddle's image segmentation format: idc-convert -l INFO \\ from-indexed-png-is \\ -l INFO \\ -i ./indexed/*.png \\ --labels l1 l2 l3 l4 \\ to-paddle-is \\ -l INFO \\ -o ./paddle And this command converts the Paddle format into indexed PNGs again: idc-convert -l INFO \\ from-paddle-is \\ -l INFO \\ -i ./paddle/*_list.txt \\ --labels_file ./paddle/labels.txt \\ to-indexed-png-is \\ -l INFO \\ -o ./indexed","title":"Paddle"},{"location":"paddle/#requirements","text":"Paddle support requires the image-dataset-converter-paddle library.","title":"Requirements"},{"location":"paddle/#image-classification","text":"With the following command, a sub-dir structured is converted into Paddle format. The integer ID/label text mapping is stored in ./paddle/labels.map , the label ID/file mapping in files.txt and the images are stored relatively to the output directory in the jpg sub-directory: idc-convert -l INFO \\ from-subdir-ic \\ -l INFO \\ -i ./subdir \\ to-paddle-ic \\ -l INFO \\ -o ./paddle \\ --id_label_map ./paddle/labels.map \\ --file_label_map files.txt \\ --relative_path jpg The following converts all Paddle .txt files with image/label definitions back into sub-directory format: idc-convert -l INFO \\ from-paddle-ic \\ -l INFO \\ -i ./paddle/*.txt \\ -m ./paddle/label_list.map \\ to-subdir-ic \\ -l INFO \\ -o ./subdir/","title":"Image classification"},{"location":"paddle/#image-segmentation","text":"The following converts indexed PNGs into Paddle's image segmentation format: idc-convert -l INFO \\ from-indexed-png-is \\ -l INFO \\ -i ./indexed/*.png \\ --labels l1 l2 l3 l4 \\ to-paddle-is \\ -l INFO \\ -o ./paddle And this command converts the Paddle format into indexed PNGs again: idc-convert -l INFO \\ from-paddle-is \\ -l INFO \\ -i ./paddle/*_list.txt \\ --labels_file ./paddle/labels.txt \\ to-indexed-png-is \\ -l INFO \\ -o ./indexed","title":"Image segmentation"},{"location":"pdf/","text":"Requirements # General PDF support requires the image-dataset-converter-pdf library. Extract images from PDF files # The pipeline below extracts any images from the PDF files and places them in the output directory: idc-convert \\ -l INFO \\ from-pdf \\ -l INFO \\ -i ./input/*.pdf \\ -t od \\ to-data \\ -o ./output Object detection overlay example # Requirements # Additional image-dataset-converter library for generating the overlays: image-dataset-converter-imgvis PDF generation # The following pipeline loads predictions in OPEX format, filters out any annotations that are not of type car or truck , generates overlays of bounding boxes with label/score and creates a PDF from these: idc-convert \\ -l INFO \\ --force_batch \\ from-opex-od \\ -l INFO \\ -i ./predictions \\ filter-labels \\ -l INFO \\ --labels car truck \\ add-annotation-overlay-od \\ --outline_alpha 255 \\ --outline_thickness 1 \\ --fill \\ --fill_alpha 128 \\ --vary_colors \\ --font_size 10 \\ --text_format \"{label}: {score}\" \\ --text_placement T,L \\ --force_bbox \\ to-pdf \\ -l INFO \\ -o ./output/vehicles.pdf \\ -t --image_scale=-1","title":"PDF"},{"location":"pdf/#requirements","text":"General PDF support requires the image-dataset-converter-pdf library.","title":"Requirements"},{"location":"pdf/#extract-images-from-pdf-files","text":"The pipeline below extracts any images from the PDF files and places them in the output directory: idc-convert \\ -l INFO \\ from-pdf \\ -l INFO \\ -i ./input/*.pdf \\ -t od \\ to-data \\ -o ./output","title":"Extract images from PDF files"},{"location":"pdf/#object-detection-overlay-example","text":"","title":"Object detection overlay example"},{"location":"pdf/#requirements_1","text":"Additional image-dataset-converter library for generating the overlays: image-dataset-converter-imgvis","title":"Requirements"},{"location":"pdf/#pdf-generation","text":"The following pipeline loads predictions in OPEX format, filters out any annotations that are not of type car or truck , generates overlays of bounding boxes with label/score and creates a PDF from these: idc-convert \\ -l INFO \\ --force_batch \\ from-opex-od \\ -l INFO \\ -i ./predictions \\ filter-labels \\ -l INFO \\ --labels car truck \\ add-annotation-overlay-od \\ --outline_alpha 255 \\ --outline_thickness 1 \\ --fill \\ --fill_alpha 128 \\ --vary_colors \\ --font_size 10 \\ --text_format \"{label}: {score}\" \\ --text_placement T,L \\ --force_bbox \\ to-pdf \\ -l INFO \\ -o ./output/vehicles.pdf \\ -t --image_scale=-1","title":"PDF generation"},{"location":"pyfunc/","text":"No library can dream of offering all the required functionality. Especially for one-off tasks, it makes no sense to develop a whole new plugin library. Hence, there are the following generic plugins that allow the user to utilize custom Python functions: reader: from-pyfunc - takes a single string as input and outputs an iterable of image containers (as per specified data type) filter: pyfunc-filter - takes a single image container or an iterable of them as input and outputs a single container or an iterable of them (as per specified input and output data types) writer: to-pyfunc - processes a single image container or an iterable of them as per specified data type and an optional split name In order to use such a custom function, they must be specified in the following format (option: -f/--function ): module_name:function_name If the code below were available through module my.code , then the function specifications would be as follows: reader: my.code:pyfunc_reader filter: my.code:pyfunc_filter writer: my.code:pyfunc_writer from typing import Iterable from idc.api import ImageClassificationData, make_list, flatten_list # reader: generates image classification containers from the path def pyfunc_reader(path: str) -> Iterable[ImageClassificationData]: return [ImageClassificationData(source=path)] # filter: simply adds a note to the meta-data def pyfunc_filter(data): result = [] for item in make_list(data): if not item.has_metadata(): meta = dict() else: meta = item.get_metadata() meta[\"note\"] = \"filtered by a python function!\" item.set_metadata(meta) result.append(item) return flatten_list(result) # writer: simply outputs name and meta-data and, if present, also the split def pyfunc_writer(data: ImageClassificationData, split: str = None): if split is None: print(\"name: \", data.image_name, \", meta:\", data.get_metadata()) else: print(\"split:\", split, \", name:\", data.image_name, \", meta:\", data.get_metadata())","title":"External functions"},{"location":"redis/","text":"Requirements # Requires the image-dataset-converter-redis library. Object detection example # In this example we are using a prebuilt yolov5 model (using MS-COCO) to make predictions on the frames that come from a dashcam video, overlay the predictions on the images and display them. For the model we will be using an existing docker container. Requirements # NB: No GPU required. Additional image-dataset-converter libraries: image-dataset-converter-imgvis image-dataset-converter-video Data # Input # Output # Preparation # NB: Place all the downloads in the current directory Download the dashcam01.mp4 video from the BoofCV project Download the yolo5n.pt model Download the coco.yaml data description for the yolo5n model The host machine must have a Redis server instance running. Two options: Install it natively via sudo apt-get install redis (and then restart it with sudo systemctl restart redis ) Spin up a docker container with: docker run --net=host --name redis-server -d redis Yolov5 model # The following command launches a Yolov5 model via the container's yolov5_predict_redis command, running on the CPU: docker run \\ --net=host -u $(id -u):$(id -g) \\ -v `pwd`:/workspace \\ -it waikatodatamining/pytorch-yolov5:2022-01-21_cpu \\ yolov5_predict_redis \\ --redis_in images \\ --redis_out predictions \\ --model /workspace/yolov5n.pt \\ --data /workspace/coco.yaml image-dataset-converter (direct prediction) # The following pipeline loads every 2nd frame from the dashcam01.mp4 video, obtains predictions from the Yolov5 model (using the Redis backend), overlays the predictions and then displays them: idc-convert \\ -l INFO \\ from-video-file \\ -i ./dashcam01.mp4 \\ -n 2 \\ -t od \\ redis-predict-od \\ --channel_out images \\ --channel_in predictions \\ --timeout 1.0 \\ add-annotation-overlay-od \\ --outline_alpha 255 \\ --outline_thickness 1 \\ --fill \\ --fill_alpha 128 \\ --vary_colors \\ --font_size 10 \\ --text_format \"{label}: {score}\" \\ --text_placement T,L \\ --force_bbox \\ image-viewer \\ --size 800,224 \\ --delay 1 image-dataset-converter (via meta-sub-images) # Like the above example, but uses the meta-sub-images filter to split the incoming images into two and sends them to the model one-by-one, then assembles the predictions and forwards them: idc-convert \\ -l INFO \\ from-video-file \\ -i ./dashcam01.mp4 \\ -n 2 \\ -t od \\ meta-sub-images \\ -l INFO \\ -r 0,0,400,224 400,0,400,224 \\ --merge_adjacent_polygons \\ --base_filter \"redis-predict-od --channel_out images --channel_in predictions --timeout 1.0\" \\ add-annotation-overlay-od \\ --outline_alpha 255 \\ --outline_thickness 1 \\ --fill \\ --fill_alpha 128 \\ --vary_colors \\ --font_size 10 \\ --text_format \"{label}: {score}\" \\ --text_placement T,L \\ --force_bbox \\ image-viewer \\ --size 800,224 \\ --delay 1 NB: Requires the image-dataset-converter-imgaug library. Meta-data of merged polygons gets lost apart from the label ( type ), which has to be the same, and any score values, which get averaged.","title":"Redis"},{"location":"redis/#requirements","text":"Requires the image-dataset-converter-redis library.","title":"Requirements"},{"location":"redis/#object-detection-example","text":"In this example we are using a prebuilt yolov5 model (using MS-COCO) to make predictions on the frames that come from a dashcam video, overlay the predictions on the images and display them. For the model we will be using an existing docker container.","title":"Object detection example"},{"location":"redis/#requirements_1","text":"NB: No GPU required. Additional image-dataset-converter libraries: image-dataset-converter-imgvis image-dataset-converter-video","title":"Requirements"},{"location":"redis/#data","text":"","title":"Data"},{"location":"redis/#input","text":"","title":"Input"},{"location":"redis/#output","text":"","title":"Output"},{"location":"redis/#preparation","text":"NB: Place all the downloads in the current directory Download the dashcam01.mp4 video from the BoofCV project Download the yolo5n.pt model Download the coco.yaml data description for the yolo5n model The host machine must have a Redis server instance running. Two options: Install it natively via sudo apt-get install redis (and then restart it with sudo systemctl restart redis ) Spin up a docker container with: docker run --net=host --name redis-server -d redis","title":"Preparation"},{"location":"redis/#yolov5-model","text":"The following command launches a Yolov5 model via the container's yolov5_predict_redis command, running on the CPU: docker run \\ --net=host -u $(id -u):$(id -g) \\ -v `pwd`:/workspace \\ -it waikatodatamining/pytorch-yolov5:2022-01-21_cpu \\ yolov5_predict_redis \\ --redis_in images \\ --redis_out predictions \\ --model /workspace/yolov5n.pt \\ --data /workspace/coco.yaml","title":"Yolov5 model"},{"location":"redis/#image-dataset-converter-direct-prediction","text":"The following pipeline loads every 2nd frame from the dashcam01.mp4 video, obtains predictions from the Yolov5 model (using the Redis backend), overlays the predictions and then displays them: idc-convert \\ -l INFO \\ from-video-file \\ -i ./dashcam01.mp4 \\ -n 2 \\ -t od \\ redis-predict-od \\ --channel_out images \\ --channel_in predictions \\ --timeout 1.0 \\ add-annotation-overlay-od \\ --outline_alpha 255 \\ --outline_thickness 1 \\ --fill \\ --fill_alpha 128 \\ --vary_colors \\ --font_size 10 \\ --text_format \"{label}: {score}\" \\ --text_placement T,L \\ --force_bbox \\ image-viewer \\ --size 800,224 \\ --delay 1","title":"image-dataset-converter (direct prediction)"},{"location":"redis/#image-dataset-converter-via-meta-sub-images","text":"Like the above example, but uses the meta-sub-images filter to split the incoming images into two and sends them to the model one-by-one, then assembles the predictions and forwards them: idc-convert \\ -l INFO \\ from-video-file \\ -i ./dashcam01.mp4 \\ -n 2 \\ -t od \\ meta-sub-images \\ -l INFO \\ -r 0,0,400,224 400,0,400,224 \\ --merge_adjacent_polygons \\ --base_filter \"redis-predict-od --channel_out images --channel_in predictions --timeout 1.0\" \\ add-annotation-overlay-od \\ --outline_alpha 255 \\ --outline_thickness 1 \\ --fill \\ --fill_alpha 128 \\ --vary_colors \\ --font_size 10 \\ --text_format \"{label}: {score}\" \\ --text_placement T,L \\ --force_bbox \\ image-viewer \\ --size 800,224 \\ --delay 1 NB: Requires the image-dataset-converter-imgaug library. Meta-data of merged polygons gets lost apart from the label ( type ), which has to be the same, and any score values, which get averaged.","title":"image-dataset-converter (via meta-sub-images)"},{"location":"speciesnet/","text":"In this example we will process trail cam videos, detect animals in them using the SpeciesNet model and store the frames with detections for further use. Requirements # Requires the following libraries: image-dataset-converter-redis image-dataset-converter-video Or simply install image-dataset-converter-all>=0.0.8 : pip install \"image-dataset-converter-all>=0.0.8\" Since we are using Redis as a backend for exchanging images and predictions, you either have to have it installed on your system or you can run it in a Docker container as follows: docker run --net=host --name redis-server -d redis Either one of the SpeciesNet Docker images: CUDA CPU NB: If you haven't used Docker before, then have a look at this tutorial . The following directory structure below your current directory: . | +- cache | +- config | +- input | +- output With input containing one or more video files in AVI format, and output for storing the predictions in OPEX format . NB: MP4 files can be processed as well. SpeciesNet model # Start the SpeciesNet model in Redis mode: CUDA docker run --rm --gpus=all --shm-size 8G --net=host \\ -u $(id -u):$(id -g) -e USER=$USER \\ -v `pwd`:/workspace \\ -v `pwd`/cache:/.cache \\ -v `pwd`/config:/.config \\ -v `pwd`/cache:/.torch \\ -it waikatodatamining/speciesnet:4.0.1_cuda12.1 speciesnet_predict_redis \\ --redis_in images \\ --redis_out predictions \\ --verbose CPU docker run --rm --shm-size 8G --net=host \\ -u $(id -u):$(id -g) -e USER=$USER \\ -v `pwd`:/workspace \\ -v `pwd`/cache:/.cache \\ -v `pwd`/config:/.config \\ -v `pwd`/cache:/.torch \\ -it waikatodatamining/speciesnet:4.0.1_cpu \\ speciesnet_predict_redis \\ --redis_in images \\ --redis_out predictions \\ --verbose Processing single video file # Determine changes between frames # If you want to skip similar frames, e.g., leaves moving in the wind, it pays to look at the frame changes in a video using the calc-frame-changes filter. However, if you are looking for small animals like rodents, you may need to process all frames to avoid missing these. The following command processes the file VID0003.AVI and calculates the changes between frames as a ratio (0-1) with a minimum change of 0.000001 required: idc-convert -l INFO \\ from-video-file \\ -l INFO \\ -i \"./input/VID0003.AVI\" \\ -t od \\ calc-frame-changes \\ -t 0.000001 This will output a histogram similar to this: +1.09e-06 - +3.22e-06 [57] \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588 +3.22e-06 - +5.35e-06 [29] \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258d +5.35e-06 - +7.49e-06 [13] \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258f +7.49e-06 - +9.62e-06 [14] \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2589 +9.62e-06 - +1.18e-05 [15] \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258c +1.18e-05 - +1.39e-05 [10] \u2588\u2588\u2588\u2588\u2588\u2588\u2588 +1.39e-05 - +1.60e-05 [11] \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258a +1.60e-05 - +1.82e-05 [ 7] \u2588\u2588\u2588\u2588\u2589 +1.82e-05 - +2.03e-05 [ 3] \u2588\u2588\u258f +2.03e-05 - +2.24e-05 [ 2] \u2588\u258d +2.24e-05 - +2.46e-05 [ 2] \u2588\u258d +2.46e-05 - +2.67e-05 [ 2] \u2588\u258d +2.67e-05 - +2.88e-05 [ 1] \u258a +2.88e-05 - +3.10e-05 [ 1] \u258a +3.10e-05 - +3.31e-05 [ 5] \u2588\u2588\u2588\u258c +3.31e-05 - +3.52e-05 [ 0] +3.52e-05 - +3.74e-05 [ 0] +3.74e-05 - +3.95e-05 [ 0] +3.95e-05 - +4.16e-05 [ 2] \u2588\u258d +4.16e-05 - +4.38e-05 [ 1] \u258a Using the threshold 5.35e-06 , you will skip frames that would fall into the top two bins of the above histogram. Extracting frames # With the threshold for our video determined, we can now extract relevant frames and push them through the SpeciesNet model. The following command processes VID0003.AVI from the input directory ( from-video-file ), determines frames that differ enough from each other ( skip-similar-frames ) to send to the model ( redis-predict-od ), removes any predictions with a score lower than 0.8 ( metadata-od ), discards any results from the model with no predictions ( discard-negatives ) and stores any images with predictions in the output directory in OPEX format : idc-convert -l INFO \\ from-video-file \\ -l INFO \\ -i \"./input/VID0003.AVI\" \\ -t od \\ skip-similar-frames \\ -l INFO \\ -t 5.35e-06 \\ redis-predict-od \\ -l INFO \\ -o images \\ -i predictions \\ -t 10 \\ metadata-od \\ -l INFO \\ -f score \\ -c gt \\ -v 0.8 \\ -a keep \\ discard-negatives \\ -l INFO \\ to-opex-od \\ -l INFO \\ -o ./output Processing multiple video files # The from-video-file reader is not limited to just processing a single file, e.g., you can supply multiple file names to the -i option or use a glob like *.AVI . However, with different videos having different lighting and therefore different changes between frames, it makes more sense returning only every nth frame to reduce the processing time. You can either use -n/--nth_frame regardless of the frame-rate of the video or -f/--fps_factor which calculates the actual nth frame to return based on the frame-rate of the video multiplied by this factor. The command below processes all .AVI files from the input directory and retrieves one frame for every second of video footage ( -f 1 ): idc-convert -l INFO \\ from-video-file \\ -l INFO \\ -i \"./input/*.AVI\" \\ -f 1 \\ -t od \\ redis-predict-od \\ -l INFO \\ -o images \\ -i predictions \\ -t 10 \\ metadata-od \\ -l INFO \\ -f score \\ -c gt \\ -v 0.8 \\ -a keep \\ discard-negatives \\ -l INFO \\ to-opex-od \\ -l INFO \\ -o ./output Visualizing the predictions # If you want to quickly generate some composite images, you can use the following command. It will create a new directory overlays in which it will save the generated images. idc-convert -l INFO \\ from-opex-od \\ -l INFO \\ -i \"./output/*.json\" \\ add-annotation-overlay-od \\ -l INFO \\ to-data \\ -l INFO \\ -o ./overlays","title":"SpeciesNet"},{"location":"speciesnet/#requirements","text":"Requires the following libraries: image-dataset-converter-redis image-dataset-converter-video Or simply install image-dataset-converter-all>=0.0.8 : pip install \"image-dataset-converter-all>=0.0.8\" Since we are using Redis as a backend for exchanging images and predictions, you either have to have it installed on your system or you can run it in a Docker container as follows: docker run --net=host --name redis-server -d redis Either one of the SpeciesNet Docker images: CUDA CPU NB: If you haven't used Docker before, then have a look at this tutorial . The following directory structure below your current directory: . | +- cache | +- config | +- input | +- output With input containing one or more video files in AVI format, and output for storing the predictions in OPEX format . NB: MP4 files can be processed as well.","title":"Requirements"},{"location":"speciesnet/#speciesnet-model","text":"Start the SpeciesNet model in Redis mode: CUDA docker run --rm --gpus=all --shm-size 8G --net=host \\ -u $(id -u):$(id -g) -e USER=$USER \\ -v `pwd`:/workspace \\ -v `pwd`/cache:/.cache \\ -v `pwd`/config:/.config \\ -v `pwd`/cache:/.torch \\ -it waikatodatamining/speciesnet:4.0.1_cuda12.1 speciesnet_predict_redis \\ --redis_in images \\ --redis_out predictions \\ --verbose CPU docker run --rm --shm-size 8G --net=host \\ -u $(id -u):$(id -g) -e USER=$USER \\ -v `pwd`:/workspace \\ -v `pwd`/cache:/.cache \\ -v `pwd`/config:/.config \\ -v `pwd`/cache:/.torch \\ -it waikatodatamining/speciesnet:4.0.1_cpu \\ speciesnet_predict_redis \\ --redis_in images \\ --redis_out predictions \\ --verbose","title":"SpeciesNet model"},{"location":"speciesnet/#processing-single-video-file","text":"","title":"Processing single video file"},{"location":"speciesnet/#determine-changes-between-frames","text":"If you want to skip similar frames, e.g., leaves moving in the wind, it pays to look at the frame changes in a video using the calc-frame-changes filter. However, if you are looking for small animals like rodents, you may need to process all frames to avoid missing these. The following command processes the file VID0003.AVI and calculates the changes between frames as a ratio (0-1) with a minimum change of 0.000001 required: idc-convert -l INFO \\ from-video-file \\ -l INFO \\ -i \"./input/VID0003.AVI\" \\ -t od \\ calc-frame-changes \\ -t 0.000001 This will output a histogram similar to this: +1.09e-06 - +3.22e-06 [57] \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588 +3.22e-06 - +5.35e-06 [29] \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258d +5.35e-06 - +7.49e-06 [13] \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258f +7.49e-06 - +9.62e-06 [14] \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2589 +9.62e-06 - +1.18e-05 [15] \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258c +1.18e-05 - +1.39e-05 [10] \u2588\u2588\u2588\u2588\u2588\u2588\u2588 +1.39e-05 - +1.60e-05 [11] \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258a +1.60e-05 - +1.82e-05 [ 7] \u2588\u2588\u2588\u2588\u2589 +1.82e-05 - +2.03e-05 [ 3] \u2588\u2588\u258f +2.03e-05 - +2.24e-05 [ 2] \u2588\u258d +2.24e-05 - +2.46e-05 [ 2] \u2588\u258d +2.46e-05 - +2.67e-05 [ 2] \u2588\u258d +2.67e-05 - +2.88e-05 [ 1] \u258a +2.88e-05 - +3.10e-05 [ 1] \u258a +3.10e-05 - +3.31e-05 [ 5] \u2588\u2588\u2588\u258c +3.31e-05 - +3.52e-05 [ 0] +3.52e-05 - +3.74e-05 [ 0] +3.74e-05 - +3.95e-05 [ 0] +3.95e-05 - +4.16e-05 [ 2] \u2588\u258d +4.16e-05 - +4.38e-05 [ 1] \u258a Using the threshold 5.35e-06 , you will skip frames that would fall into the top two bins of the above histogram.","title":"Determine changes between frames"},{"location":"speciesnet/#extracting-frames","text":"With the threshold for our video determined, we can now extract relevant frames and push them through the SpeciesNet model. The following command processes VID0003.AVI from the input directory ( from-video-file ), determines frames that differ enough from each other ( skip-similar-frames ) to send to the model ( redis-predict-od ), removes any predictions with a score lower than 0.8 ( metadata-od ), discards any results from the model with no predictions ( discard-negatives ) and stores any images with predictions in the output directory in OPEX format : idc-convert -l INFO \\ from-video-file \\ -l INFO \\ -i \"./input/VID0003.AVI\" \\ -t od \\ skip-similar-frames \\ -l INFO \\ -t 5.35e-06 \\ redis-predict-od \\ -l INFO \\ -o images \\ -i predictions \\ -t 10 \\ metadata-od \\ -l INFO \\ -f score \\ -c gt \\ -v 0.8 \\ -a keep \\ discard-negatives \\ -l INFO \\ to-opex-od \\ -l INFO \\ -o ./output","title":"Extracting frames"},{"location":"speciesnet/#processing-multiple-video-files","text":"The from-video-file reader is not limited to just processing a single file, e.g., you can supply multiple file names to the -i option or use a glob like *.AVI . However, with different videos having different lighting and therefore different changes between frames, it makes more sense returning only every nth frame to reduce the processing time. You can either use -n/--nth_frame regardless of the frame-rate of the video or -f/--fps_factor which calculates the actual nth frame to return based on the frame-rate of the video multiplied by this factor. The command below processes all .AVI files from the input directory and retrieves one frame for every second of video footage ( -f 1 ): idc-convert -l INFO \\ from-video-file \\ -l INFO \\ -i \"./input/*.AVI\" \\ -f 1 \\ -t od \\ redis-predict-od \\ -l INFO \\ -o images \\ -i predictions \\ -t 10 \\ metadata-od \\ -l INFO \\ -f score \\ -c gt \\ -v 0.8 \\ -a keep \\ discard-negatives \\ -l INFO \\ to-opex-od \\ -l INFO \\ -o ./output","title":"Processing multiple video files"},{"location":"speciesnet/#visualizing-the-predictions","text":"If you want to quickly generate some composite images, you can use the following command. It will create a new directory overlays in which it will save the generated images. idc-convert -l INFO \\ from-opex-od \\ -l INFO \\ -i \"./output/*.json\" \\ add-annotation-overlay-od \\ -l INFO \\ to-data \\ -l INFO \\ -o ./overlays","title":"Visualizing the predictions"},{"location":"video/","text":"Requirements # Requires the image-dataset-converter-video library. Plugins # Extract frames from video file # The following extracts a total of 100 frames from ./input/video.mp4 , using every 20th frame and stores the images in the ./output directory: idc-convert \\ -l INFO \\ from-video-file \\ -l INFO \\ -i ./input/video.mp4 \\ -t od \\ -n 20 \\ -m 100 \\ to-data \\ -l INFO \\ -o ./output/ Though the data type is object detection ( -t od ), this is not really relevant in this example. If there were other filters, e.g., for applying object detection Redis models , then it would be necessary to specify the correct type. Capture frames from a webcam (individual JPG files) # The following captures a total of 10 images from a webcam (using every 20th frame) and stores them as individual JPG files in the ./output directory: idc-convert \\ -l INFO \\ from-webcam \\ -l INFO \\ -i 0 \\ -t od \\ -n 20 \\ -m 10 \\ to-data \\ -l INFO \\ -o ./output Capture frames from a webcam (store as MJPEG file) # The following captures a total of 100 images from a webcam (using every 10th frame) and generates a MJPEG file, which can be viewed with, e.g., ffplay : idc-convert \\ -l INFO \\ from-webcam \\ -l INFO \\ -i 0 \\ -t od \\ -n 10 \\ -m 100 \\ to-video-file \\ -l INFO \\ -o ./output/webcam.mjpeg Calculating differences between frames # Depending on the scene, frames may not differ much between each other, making it hard to extract frames by using specific intervals. The calc-frame-changes writer computes differences between frames and output a histogram of these differences. Using the min/max of the bins of the histogram, it is possible to find a suitable change threshold. This threshold can be utilized by the skip-similar-frames filter to discard too similar frames, whittling down the number of extracted frames automatically. The histogram can be output in textual form, CSV or JSON; either on stdout or stored in a file. idc-convert \\ -l INFO \\ from-video-file \\ -l INFO \\ -i ./input/video.mp4 \\ -t od \\ -n 10 \\ -m 2000 \\ calc-frame-changes \\ -f text Skipping similar frames # The following command keeps frames that differ enough from each other and discards all others, using the skip-similar-frames filter. The threshold applied by the filter was previously determined by the calc-frame-changes filter. idc-convert \\ -l INFO \\ from-video-file \\ -l INFO \\ -i ./input/video.mp4 \\ -t od \\ skip-similar-frames \\ -t 0.005 \\ to-data \\ -l INFO \\ -o ./output Youtube video # The from-youtube reader allows you to grab frames from a Youtube video: idc-convert \\ -l INFO \\ from-youtube \\ -l INFO \\ -i \"https://www.youtube.com/watch?v=CFdZWgiAj8I\" \\ -p the_cloud-kitty_flanagan- \\ -t od \\ -n 20 \\ -m 10 \\ to-data \\ -l INFO \\ -o ./output NB: Please make sure not to infringe on any copyright. Youtube live stream # The from-youtube-live reader allows you to grab frames from a Youtube live stream, e.g., wildlife cameras: idc-convert \\ -l INFO \\ from-youtube-live \\ -l INFO \\ -i \"https://www.youtube.com/watch?v=2swy9gysvOY\" \\ -p african_river_wildlife_camera- \\ -t od \\ -n 20 \\ -m 10 \\ to-data \\ -l INFO \\ -o ./output NB: Please make sure not to infringe on any copyright.","title":"Video"},{"location":"video/#requirements","text":"Requires the image-dataset-converter-video library.","title":"Requirements"},{"location":"video/#plugins","text":"","title":"Plugins"},{"location":"video/#extract-frames-from-video-file","text":"The following extracts a total of 100 frames from ./input/video.mp4 , using every 20th frame and stores the images in the ./output directory: idc-convert \\ -l INFO \\ from-video-file \\ -l INFO \\ -i ./input/video.mp4 \\ -t od \\ -n 20 \\ -m 100 \\ to-data \\ -l INFO \\ -o ./output/ Though the data type is object detection ( -t od ), this is not really relevant in this example. If there were other filters, e.g., for applying object detection Redis models , then it would be necessary to specify the correct type.","title":"Extract frames from video file"},{"location":"video/#capture-frames-from-a-webcam-individual-jpg-files","text":"The following captures a total of 10 images from a webcam (using every 20th frame) and stores them as individual JPG files in the ./output directory: idc-convert \\ -l INFO \\ from-webcam \\ -l INFO \\ -i 0 \\ -t od \\ -n 20 \\ -m 10 \\ to-data \\ -l INFO \\ -o ./output","title":"Capture frames from a webcam (individual JPG files)"},{"location":"video/#capture-frames-from-a-webcam-store-as-mjpeg-file","text":"The following captures a total of 100 images from a webcam (using every 10th frame) and generates a MJPEG file, which can be viewed with, e.g., ffplay : idc-convert \\ -l INFO \\ from-webcam \\ -l INFO \\ -i 0 \\ -t od \\ -n 10 \\ -m 100 \\ to-video-file \\ -l INFO \\ -o ./output/webcam.mjpeg","title":"Capture frames from a webcam (store as MJPEG file)"},{"location":"video/#calculating-differences-between-frames","text":"Depending on the scene, frames may not differ much between each other, making it hard to extract frames by using specific intervals. The calc-frame-changes writer computes differences between frames and output a histogram of these differences. Using the min/max of the bins of the histogram, it is possible to find a suitable change threshold. This threshold can be utilized by the skip-similar-frames filter to discard too similar frames, whittling down the number of extracted frames automatically. The histogram can be output in textual form, CSV or JSON; either on stdout or stored in a file. idc-convert \\ -l INFO \\ from-video-file \\ -l INFO \\ -i ./input/video.mp4 \\ -t od \\ -n 10 \\ -m 2000 \\ calc-frame-changes \\ -f text","title":"Calculating differences between frames"},{"location":"video/#skipping-similar-frames","text":"The following command keeps frames that differ enough from each other and discards all others, using the skip-similar-frames filter. The threshold applied by the filter was previously determined by the calc-frame-changes filter. idc-convert \\ -l INFO \\ from-video-file \\ -l INFO \\ -i ./input/video.mp4 \\ -t od \\ skip-similar-frames \\ -t 0.005 \\ to-data \\ -l INFO \\ -o ./output","title":"Skipping similar frames"},{"location":"video/#youtube-video","text":"The from-youtube reader allows you to grab frames from a Youtube video: idc-convert \\ -l INFO \\ from-youtube \\ -l INFO \\ -i \"https://www.youtube.com/watch?v=CFdZWgiAj8I\" \\ -p the_cloud-kitty_flanagan- \\ -t od \\ -n 20 \\ -m 10 \\ to-data \\ -l INFO \\ -o ./output NB: Please make sure not to infringe on any copyright.","title":"Youtube video"},{"location":"video/#youtube-live-stream","text":"The from-youtube-live reader allows you to grab frames from a Youtube live stream, e.g., wildlife cameras: idc-convert \\ -l INFO \\ from-youtube-live \\ -l INFO \\ -i \"https://www.youtube.com/watch?v=2swy9gysvOY\" \\ -p african_river_wildlife_camera- \\ -t od \\ -n 20 \\ -m 10 \\ to-data \\ -l INFO \\ -o ./output NB: Please make sure not to infringe on any copyright.","title":"Youtube live stream"}]}