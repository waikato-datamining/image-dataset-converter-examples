{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"The image-dataset-converter library (and its dependent libraries) can be used for converting image datasets from one format into another. It has I/O support for the following domains: Image classification Object detection Image segmentation Please refer to the dataset formats section for more details on supported formats. But the library does not just convert datasets, you can also slot in complex filter pipelines to process/clean the data. On this website you can find examples for: Image classification Object detection Image segmentation Filter usage Docker usage Examples for the additional libraries: Image augmentation Image statistics Image visualizations PDF Redis Video","title":"Home"},{"location":"docker/","text":"Below are examples for using the image-dataset-converter library via its Docker images . Interactive session # The following command starts an interactive session, mapping the current working directory to /workspace : docker run --rm -u $(id -u):$(id -g) \\ -v `pwd`:/workspace \\ -it waikatodatamining/image-dataset-converter:latest Conversion pipeline # The following converts an image classification dataset from the sub-dir format (sub-directory names represent the image classification labels) into the ADAMS format , which stores the label in an associated .report file (Java properties file): docker run --rm -u $(id -u):$(id -g) \\ -v `pwd`:/workspace \\ -it waikatodatamining/image-dataset-converter:latest \\ idc-convert -l INFO \\ from-subdir-ic \\ -l INFO \\ -i /workspace/input/ \\ to-adams-ic \\ -l INFO \\ -o /workspace/output \\ -c classification NB: The input and output directories are located below the current working directory ( pwd ).","title":"Docker usage"},{"location":"docker/#interactive-session","text":"The following command starts an interactive session, mapping the current working directory to /workspace : docker run --rm -u $(id -u):$(id -g) \\ -v `pwd`:/workspace \\ -it waikatodatamining/image-dataset-converter:latest","title":"Interactive session"},{"location":"docker/#conversion-pipeline","text":"The following converts an image classification dataset from the sub-dir format (sub-directory names represent the image classification labels) into the ADAMS format , which stores the label in an associated .report file (Java properties file): docker run --rm -u $(id -u):$(id -g) \\ -v `pwd`:/workspace \\ -it waikatodatamining/image-dataset-converter:latest \\ idc-convert -l INFO \\ from-subdir-ic \\ -l INFO \\ -i /workspace/input/ \\ to-adams-ic \\ -l INFO \\ -o /workspace/output \\ -c classification NB: The input and output directories are located below the current working directory ( pwd ).","title":"Conversion pipeline"},{"location":"filters/","text":"The following sections only show snippets of commands, as there are quite a number of filters available. Bounding box / polygon # With the coerce-bbox filter, you can force annotations to be bounding box only. The reverse is the coerce-mask filter, which ensures that all annotations are available as polygons. Too small or too large? # Using the dimension-discarder filter, you can filter out too large or too small images quite easily: only allow within certain width/height constraint ... dimension-discarder \\ -l INFO \\ --min_height 100 \\ --max_height 200 \\ --min_width 100 \\ --max_width 200 \\ ... only a certain area, but the shape is irrelevant ... dimension-discarder \\ -l INFO \\ --min_area 10000 \\ --max_area 50000 \\ ... Domain conversion # object detection to image classification: With the od-to-ic filter you can convert object detection annotations to image classification. How multiple differing labels are handled can be specified. object detection to image segmentation: The od-to-is filter generates image segmentation data from the bbox/polygon annotations. Annotation management # filter-labels - leaves only the matching labels in the annotations map-labels - for renaming labels remove-classes - removes the specified labels strip-annotations - removes all annotations write-labels - outputs a list of all the encountered labels Meta-data management # metadata - allows comparisons on meta-data values and whether to keep or discard a record in case of a match metadata-from-name - allows extraction of meta-data value from the image name via a regular expression split - adds the field split to the meta-data of the record passing through, which can be acted on with other filters (or stored in the output) Record management # A number of generic record management filters are available: check-duplicate-filenames - when using multiple batches as input, duplicate file names can be an issue when creating a combined output discard-invalid-images - attempts to load the image and discards them in case the loading fails (useful when data acquisition can generate invalid images) discard-negatives - removes records from the stream that have no annotations max-records - limits the number of records passing through randomize-records - when processing batches, this filter can randomize them (seeded or unseeded) record-window - only lets a certain window of records pass through (e.g., the first 1000) rename - allows renaming of images, e.g., prefixing them with a batch number/ID sample - for selecting a random sub-sample from the stream Sub-pipelines # With the tee meta-filter, it is possible to filter the images coming through with a separate sub-pipeline. E.g., converting the incoming data into multiple output formats. The following command loads the VOC XML annotations and saves them in ADAMS and YOLO format in one command: idc-convert \\ -l INFO \\ from-voc-od \\ -l INFO \\ -i \"./voc/*.xml\" \\ tee \\ -f \"to-adams-od -o ./adams-tee/\" \\ tee \\ -f \"to-yolo-od -o ./yolo-tee/ --labels ./yolo-tee/labels.txt\"","title":"Filter usage"},{"location":"filters/#bounding-box-polygon","text":"With the coerce-bbox filter, you can force annotations to be bounding box only. The reverse is the coerce-mask filter, which ensures that all annotations are available as polygons.","title":"Bounding box / polygon"},{"location":"filters/#too-small-or-too-large","text":"Using the dimension-discarder filter, you can filter out too large or too small images quite easily: only allow within certain width/height constraint ... dimension-discarder \\ -l INFO \\ --min_height 100 \\ --max_height 200 \\ --min_width 100 \\ --max_width 200 \\ ... only a certain area, but the shape is irrelevant ... dimension-discarder \\ -l INFO \\ --min_area 10000 \\ --max_area 50000 \\ ...","title":"Too small or too large?"},{"location":"filters/#domain-conversion","text":"object detection to image classification: With the od-to-ic filter you can convert object detection annotations to image classification. How multiple differing labels are handled can be specified. object detection to image segmentation: The od-to-is filter generates image segmentation data from the bbox/polygon annotations.","title":"Domain conversion"},{"location":"filters/#annotation-management","text":"filter-labels - leaves only the matching labels in the annotations map-labels - for renaming labels remove-classes - removes the specified labels strip-annotations - removes all annotations write-labels - outputs a list of all the encountered labels","title":"Annotation management"},{"location":"filters/#meta-data-management","text":"metadata - allows comparisons on meta-data values and whether to keep or discard a record in case of a match metadata-from-name - allows extraction of meta-data value from the image name via a regular expression split - adds the field split to the meta-data of the record passing through, which can be acted on with other filters (or stored in the output)","title":"Meta-data management"},{"location":"filters/#record-management","text":"A number of generic record management filters are available: check-duplicate-filenames - when using multiple batches as input, duplicate file names can be an issue when creating a combined output discard-invalid-images - attempts to load the image and discards them in case the loading fails (useful when data acquisition can generate invalid images) discard-negatives - removes records from the stream that have no annotations max-records - limits the number of records passing through randomize-records - when processing batches, this filter can randomize them (seeded or unseeded) record-window - only lets a certain window of records pass through (e.g., the first 1000) rename - allows renaming of images, e.g., prefixing them with a batch number/ID sample - for selecting a random sub-sample from the stream","title":"Record management"},{"location":"filters/#sub-pipelines","text":"With the tee meta-filter, it is possible to filter the images coming through with a separate sub-pipeline. E.g., converting the incoming data into multiple output formats. The following command loads the VOC XML annotations and saves them in ADAMS and YOLO format in one command: idc-convert \\ -l INFO \\ from-voc-od \\ -l INFO \\ -i \"./voc/*.xml\" \\ tee \\ -f \"to-adams-od -o ./adams-tee/\" \\ tee \\ -f \"to-yolo-od -o ./yolo-tee/ --labels ./yolo-tee/labels.txt\"","title":"Sub-pipelines"},{"location":"image_classification/","text":"Readers and writers for image classification have the -ic suffix. Download the 17 flowers image classification dataset and extract it. Plugins # sub-dir to ADAMS # The following converts an image classification dataset from the sub-dir format (sub-directory names represent the image classification labels) into the ADAMS format , which stores the label in an associated .report file (Java properties file): idc-convert -l INFO \\ from-subdir-ic \\ -l INFO \\ -i ./subdir/ \\ to-adams-ic \\ -l INFO \\ -o ./adams \\ -c classification sub-dir (randomized train/val/test splits) # By enforcing batch-processing --force_batch and using the randomize-records filter, randomized train/val/test splits (writers typically support generating splits) can be generated like this: idc-convert -l INFO --force_batch \\ from-subdir-ic \\ -l INFO \\ -i ./subdir/ \\ randomize-records \\ -s 42 \\ to-subdir-ic \\ -l INFO \\ -o ./subdir-split \\ --split_names train val test \\ --split_ratios 70 15 15","title":"Image classification"},{"location":"image_classification/#plugins","text":"","title":"Plugins"},{"location":"image_classification/#sub-dir-to-adams","text":"The following converts an image classification dataset from the sub-dir format (sub-directory names represent the image classification labels) into the ADAMS format , which stores the label in an associated .report file (Java properties file): idc-convert -l INFO \\ from-subdir-ic \\ -l INFO \\ -i ./subdir/ \\ to-adams-ic \\ -l INFO \\ -o ./adams \\ -c classification","title":"sub-dir to ADAMS"},{"location":"image_classification/#sub-dir-randomized-trainvaltest-splits","text":"By enforcing batch-processing --force_batch and using the randomize-records filter, randomized train/val/test splits (writers typically support generating splits) can be generated like this: idc-convert -l INFO --force_batch \\ from-subdir-ic \\ -l INFO \\ -i ./subdir/ \\ randomize-records \\ -s 42 \\ to-subdir-ic \\ -l INFO \\ -o ./subdir-split \\ --split_names train val test \\ --split_ratios 70 15 15","title":"sub-dir (randomized train/val/test splits)"},{"location":"image_segmentation/","text":"Readers and writers for image segmentation have the -is suffix. Download the blue channel archive of the camvid dataset and extract it. Plugins # Blue channel to Indexed PNG # The following command-line will convert it into a dataset using indexed PNG files: idc-convert \\ -l INFO \\ from-blue-channel-is \\ -l INFO \\ -i \"./bluechannel/*.png\" \\ --labels Animal Archway Bicyclist Bridge Building Car CartLuggagePram Child Column_Pole \\ Fence LaneMkgsDriv LaneMkgsNonDriv Misc_Text MotorcycleScooter OtherMoving ParkingBlock \\ Pedestrian Road RoadShoulder Sidewalk SignSymbol Sky SUVPickupTruck TrafficCone \\ TrafficLight Train Tree Truck_Bus Tunnel VegetationMisc Void Wall \\ to-indexed-png-is \\ -l INFO \\ -p x11 \\ -o ./indexedpng NB: Uses the X11 color palette for the palette in the PNGs. Here is an example (0001TP_007050.png): Blue channel to Indexed PNG (cyclists only) # By applying filters, you can also generate subsets, e.g., for building more specialized models. The following will extract only images that have cyclists and discard all other annotations ( filter-labels ). Images with no annotations left will get discarded ( discard-negatives ): idc-convert \\ -l INFO \\ from-blue-channel-is \\ -l INFO \\ -i \"./bluechannel/*.png\" \\ --labels Animal Archway Bicyclist Bridge Building Car CartLuggagePram Child Column_Pole \\ Fence LaneMkgsDriv LaneMkgsNonDriv Misc_Text MotorcycleScooter OtherMoving ParkingBlock \\ Pedestrian Road RoadShoulder Sidewalk SignSymbol Sky SUVPickupTruck TrafficCone \\ TrafficLight Train Tree Truck_Bus Tunnel VegetationMisc Void Wall \\ filter-labels \\ -l INFO \\ --labels Bicyclist \\ discard-negatives \\ -l INFO \\ to-indexed-png-is \\ -l INFO \\ -p x11 \\ -o ./indexedpng-cyclists Here is an example (0001TP_007380.png):","title":"Image segmentation"},{"location":"image_segmentation/#plugins","text":"","title":"Plugins"},{"location":"image_segmentation/#blue-channel-to-indexed-png","text":"The following command-line will convert it into a dataset using indexed PNG files: idc-convert \\ -l INFO \\ from-blue-channel-is \\ -l INFO \\ -i \"./bluechannel/*.png\" \\ --labels Animal Archway Bicyclist Bridge Building Car CartLuggagePram Child Column_Pole \\ Fence LaneMkgsDriv LaneMkgsNonDriv Misc_Text MotorcycleScooter OtherMoving ParkingBlock \\ Pedestrian Road RoadShoulder Sidewalk SignSymbol Sky SUVPickupTruck TrafficCone \\ TrafficLight Train Tree Truck_Bus Tunnel VegetationMisc Void Wall \\ to-indexed-png-is \\ -l INFO \\ -p x11 \\ -o ./indexedpng NB: Uses the X11 color palette for the palette in the PNGs. Here is an example (0001TP_007050.png):","title":"Blue channel to Indexed PNG"},{"location":"image_segmentation/#blue-channel-to-indexed-png-cyclists-only","text":"By applying filters, you can also generate subsets, e.g., for building more specialized models. The following will extract only images that have cyclists and discard all other annotations ( filter-labels ). Images with no annotations left will get discarded ( discard-negatives ): idc-convert \\ -l INFO \\ from-blue-channel-is \\ -l INFO \\ -i \"./bluechannel/*.png\" \\ --labels Animal Archway Bicyclist Bridge Building Car CartLuggagePram Child Column_Pole \\ Fence LaneMkgsDriv LaneMkgsNonDriv Misc_Text MotorcycleScooter OtherMoving ParkingBlock \\ Pedestrian Road RoadShoulder Sidewalk SignSymbol Sky SUVPickupTruck TrafficCone \\ TrafficLight Train Tree Truck_Bus Tunnel VegetationMisc Void Wall \\ filter-labels \\ -l INFO \\ --labels Bicyclist \\ discard-negatives \\ -l INFO \\ to-indexed-png-is \\ -l INFO \\ -p x11 \\ -o ./indexedpng-cyclists Here is an example (0001TP_007380.png):","title":"Blue channel to Indexed PNG (cyclists only)"},{"location":"imgaug/","text":"Requirements # Requires the image-dataset-converter-imgaug library. Download the 17 flowers object detection VOC XML dataset and extract it. Plugins # Convert VOC XML to YOLO (crop/rotate) # The following converts VOC XML annotations into YOLO ones. It also augments the dataset with randomly adding cropped/rotated images to the stream ( -m add ), with the annotations getting processed accordingly: idc-convert \\ -l INFO \\ from-voc-od \\ -l INFO \\ -i \"./voc/*.xml\" \\ crop \\ -m add \\ -f 0.1 \\ -t 0.2 \\ -u \\ rotate \\ -m add \\ -f \"-45\" \\ -t 45 \\ to-yolo-od \\ -l INFO \\ -o ./yolo-split-augmented \\ --labels ./yolo-split-augmented/labels.txt \\ --split_names train val test \\ --split_ratios 70 15 15 Here is an example of a processed image (image_0001.jpg): Convert VOC XML to MS COCO (resize) # The following converts VOC XML annotations into smaller MS COCO ones, using a maximum width of 300 while keeping the aspect ratio intact. The data also gets split into train/validation/test: idc-convert \\ -l INFO \\ from-voc-od \\ -l INFO \\ -i \"./voc/*.xml\" \\ resize \\ -W 300 \\ -H keep-aspect-ratio \\ to-coco-od \\ -l INFO \\ -o ./coco-split-small \\ --split_names train val test \\ --split_ratios 70 15 15 Split images into smaller ones # With the sub-images filter it is possible to split images into smaller ones or extract just specific regions of interest from images. The idc-generate-regions tool can be used to generate regions for a set number of rows/columns or fixed row heights/column widths. For images that are at maximum 800x800, we can generate regions for a 2x2 grid as follows: idc-generate-regions \\ -l INFO \\ -W 800 \\ -H 800 \\ -r 2 \\ -c 2 Will output this, with each quadruplet consisting of x, y, width and height: 0,0,400,400 400,0,400,400 0,400,400,400 400,400,400,400 These regions we can now use with the sub-images filter to generate smaller images: idc-convert \\ -l INFO \\ from-voc-od \\ -l INFO \\ -i \"./voc/*.xml\" \\ sub-images \\ -l INFO \\ -r 0,0,400,400 400,0,400,400 0,400,400,400 400,400,400,400 \\ -p \\ -e \\ to-yolo-od \\ -l INFO \\ -o ./yolo-sub \\ --labels ./yolo-sub/labels.txt Using -p we will keep partial annotations, ones that got cut off a bit, and with -e we will suppress images that have no annotations in them. Using the meta-sub-images filter, you can feed the small images that were generated through the specified base filter. The output of this filter gets reassembled and forwarded. This base filter can be a simple filter or a more complex pipeline, which passes the images through a Redis model , for instance.","title":"Image augmentation"},{"location":"imgaug/#requirements","text":"Requires the image-dataset-converter-imgaug library. Download the 17 flowers object detection VOC XML dataset and extract it.","title":"Requirements"},{"location":"imgaug/#plugins","text":"","title":"Plugins"},{"location":"imgaug/#convert-voc-xml-to-yolo-croprotate","text":"The following converts VOC XML annotations into YOLO ones. It also augments the dataset with randomly adding cropped/rotated images to the stream ( -m add ), with the annotations getting processed accordingly: idc-convert \\ -l INFO \\ from-voc-od \\ -l INFO \\ -i \"./voc/*.xml\" \\ crop \\ -m add \\ -f 0.1 \\ -t 0.2 \\ -u \\ rotate \\ -m add \\ -f \"-45\" \\ -t 45 \\ to-yolo-od \\ -l INFO \\ -o ./yolo-split-augmented \\ --labels ./yolo-split-augmented/labels.txt \\ --split_names train val test \\ --split_ratios 70 15 15 Here is an example of a processed image (image_0001.jpg):","title":"Convert VOC XML to YOLO (crop/rotate)"},{"location":"imgaug/#convert-voc-xml-to-ms-coco-resize","text":"The following converts VOC XML annotations into smaller MS COCO ones, using a maximum width of 300 while keeping the aspect ratio intact. The data also gets split into train/validation/test: idc-convert \\ -l INFO \\ from-voc-od \\ -l INFO \\ -i \"./voc/*.xml\" \\ resize \\ -W 300 \\ -H keep-aspect-ratio \\ to-coco-od \\ -l INFO \\ -o ./coco-split-small \\ --split_names train val test \\ --split_ratios 70 15 15","title":"Convert VOC XML to MS COCO (resize)"},{"location":"imgaug/#split-images-into-smaller-ones","text":"With the sub-images filter it is possible to split images into smaller ones or extract just specific regions of interest from images. The idc-generate-regions tool can be used to generate regions for a set number of rows/columns or fixed row heights/column widths. For images that are at maximum 800x800, we can generate regions for a 2x2 grid as follows: idc-generate-regions \\ -l INFO \\ -W 800 \\ -H 800 \\ -r 2 \\ -c 2 Will output this, with each quadruplet consisting of x, y, width and height: 0,0,400,400 400,0,400,400 0,400,400,400 400,400,400,400 These regions we can now use with the sub-images filter to generate smaller images: idc-convert \\ -l INFO \\ from-voc-od \\ -l INFO \\ -i \"./voc/*.xml\" \\ sub-images \\ -l INFO \\ -r 0,0,400,400 400,0,400,400 0,400,400,400 400,400,400,400 \\ -p \\ -e \\ to-yolo-od \\ -l INFO \\ -o ./yolo-sub \\ --labels ./yolo-sub/labels.txt Using -p we will keep partial annotations, ones that got cut off a bit, and with -e we will suppress images that have no annotations in them. Using the meta-sub-images filter, you can feed the small images that were generated through the specified base filter. The output of this filter gets reassembled and forwarded. This base filter can be a simple filter or a more complex pipeline, which passes the images through a Redis model , for instance.","title":"Split images into smaller ones"},{"location":"imgstats/","text":"Requirements # Requires the image-dataset-converter-imgstats library. Download the 17 flowers object detection VOC XML dataset and extract it. Plugins # Label distribution # Using the label-dist writer, you can output a distribution of the labels within a dataset as follows: idc-convert \\ -l INFO \\ from-voc-od \\ -l INFO \\ -i \"./voc/*.xml\" \\ label-dist \\ -l INFO This will output the following: Bluebell: 28 Buttercup: 54 ColtsFoot: 55 Crocus: 50 Daffodil: 71 Daisy: 57 Dandelion: 43 Fritillary: 65 Iris: 77 LilyValley: 17 Pansy: 56 Snowdrop: 50 Sunflower: 71 Tigerlily: 50 Tulip: 41 Windflower: 63 NB: You can also generate CSV or JSON output. Area histogram # Knowing the distribution of the extends of your annotations can be quite useful, e.g., for determining whether down-sampling can be an option. The area-histogram writer generates histograms for all objects combined and one for each category: idc-convert \\ -l INFO \\ from-voc-od \\ -l INFO \\ -i \"./voc/*.xml\" \\ area-histogram \\ -l INFO The output for the 17 flowers dataset looks like this: ALL: +1.26e+04 - +3.29e+04 [ 14] \u2588\u2588\u2588\u2588\u2588\u258e +3.29e+04 - +5.33e+04 [ 30] \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258d +5.33e+04 - +7.36e+04 [ 36] \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258b +7.36e+04 - +9.40e+04 [ 54] \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258d +9.40e+04 - +1.14e+05 [ 71] \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258a +1.14e+05 - +1.35e+05 [ 71] \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258a +1.35e+05 - +1.55e+05 [ 95] \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2589 +1.55e+05 - +1.75e+05 [104] \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258e +1.75e+05 - +1.96e+05 [ 94] \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258c +1.96e+05 - +2.16e+05 [106] \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588 +2.16e+05 - +2.37e+05 [ 77] \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588 +2.37e+05 - +2.57e+05 [ 48] \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258f +2.57e+05 - +2.77e+05 [ 14] \u2588\u2588\u2588\u2588\u2588\u258e +2.77e+05 - +2.98e+05 [ 10] \u2588\u2588\u2588\u258a +2.98e+05 - +3.18e+05 [ 9] \u2588\u2588\u2588\u258d +3.18e+05 - +3.38e+05 [ 6] \u2588\u2588\u258e +3.38e+05 - +3.59e+05 [ 2] \u258a +3.59e+05 - +3.79e+05 [ 5] \u2588\u2589 +3.79e+05 - +3.99e+05 [ 1] \u258d +3.99e+05 - +4.20e+05 [ 1] \u258d Bluebell: +7.68e+04 - +9.18e+04 [1] \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588 +9.18e+04 - +1.07e+05 [1] \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588 +1.07e+05 - +1.22e+05 [2] \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588 +1.22e+05 - +1.37e+05 [0] +1.37e+05 - +1.51e+05 [0] +1.51e+05 - +1.66e+05 [2] \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588 +1.66e+05 - +1.81e+05 [2] \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588 +1.81e+05 - +1.96e+05 [0] +1.96e+05 - +2.11e+05 [5] \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588 +2.11e+05 - +2.26e+05 [1] \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588 +2.26e+05 - +2.41e+05 [4] \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588 +2.41e+05 - +2.56e+05 [3] \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588 +2.56e+05 - +2.71e+05 [1] \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588 +2.71e+05 - +2.86e+05 [0] +2.86e+05 - +3.01e+05 [2] \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588 +3.01e+05 - +3.16e+05 [0] +3.16e+05 - +3.31e+05 [1] \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588 +3.31e+05 - +3.46e+05 [0] +3.46e+05 - +3.61e+05 [0] +3.61e+05 - +3.76e+05 [3] \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588 Buttercup: ... NB: You can also generate CSV or JSON output.","title":"Image statistics"},{"location":"imgstats/#requirements","text":"Requires the image-dataset-converter-imgstats library. Download the 17 flowers object detection VOC XML dataset and extract it.","title":"Requirements"},{"location":"imgstats/#plugins","text":"","title":"Plugins"},{"location":"imgstats/#label-distribution","text":"Using the label-dist writer, you can output a distribution of the labels within a dataset as follows: idc-convert \\ -l INFO \\ from-voc-od \\ -l INFO \\ -i \"./voc/*.xml\" \\ label-dist \\ -l INFO This will output the following: Bluebell: 28 Buttercup: 54 ColtsFoot: 55 Crocus: 50 Daffodil: 71 Daisy: 57 Dandelion: 43 Fritillary: 65 Iris: 77 LilyValley: 17 Pansy: 56 Snowdrop: 50 Sunflower: 71 Tigerlily: 50 Tulip: 41 Windflower: 63 NB: You can also generate CSV or JSON output.","title":"Label distribution"},{"location":"imgstats/#area-histogram","text":"Knowing the distribution of the extends of your annotations can be quite useful, e.g., for determining whether down-sampling can be an option. The area-histogram writer generates histograms for all objects combined and one for each category: idc-convert \\ -l INFO \\ from-voc-od \\ -l INFO \\ -i \"./voc/*.xml\" \\ area-histogram \\ -l INFO The output for the 17 flowers dataset looks like this: ALL: +1.26e+04 - +3.29e+04 [ 14] \u2588\u2588\u2588\u2588\u2588\u258e +3.29e+04 - +5.33e+04 [ 30] \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258d +5.33e+04 - +7.36e+04 [ 36] \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258b +7.36e+04 - +9.40e+04 [ 54] \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258d +9.40e+04 - +1.14e+05 [ 71] \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258a +1.14e+05 - +1.35e+05 [ 71] \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258a +1.35e+05 - +1.55e+05 [ 95] \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2589 +1.55e+05 - +1.75e+05 [104] \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258e +1.75e+05 - +1.96e+05 [ 94] \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258c +1.96e+05 - +2.16e+05 [106] \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588 +2.16e+05 - +2.37e+05 [ 77] \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588 +2.37e+05 - +2.57e+05 [ 48] \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258f +2.57e+05 - +2.77e+05 [ 14] \u2588\u2588\u2588\u2588\u2588\u258e +2.77e+05 - +2.98e+05 [ 10] \u2588\u2588\u2588\u258a +2.98e+05 - +3.18e+05 [ 9] \u2588\u2588\u2588\u258d +3.18e+05 - +3.38e+05 [ 6] \u2588\u2588\u258e +3.38e+05 - +3.59e+05 [ 2] \u258a +3.59e+05 - +3.79e+05 [ 5] \u2588\u2589 +3.79e+05 - +3.99e+05 [ 1] \u258d +3.99e+05 - +4.20e+05 [ 1] \u258d Bluebell: +7.68e+04 - +9.18e+04 [1] \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588 +9.18e+04 - +1.07e+05 [1] \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588 +1.07e+05 - +1.22e+05 [2] \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588 +1.22e+05 - +1.37e+05 [0] +1.37e+05 - +1.51e+05 [0] +1.51e+05 - +1.66e+05 [2] \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588 +1.66e+05 - +1.81e+05 [2] \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588 +1.81e+05 - +1.96e+05 [0] +1.96e+05 - +2.11e+05 [5] \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588 +2.11e+05 - +2.26e+05 [1] \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588 +2.26e+05 - +2.41e+05 [4] \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588 +2.41e+05 - +2.56e+05 [3] \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588 +2.56e+05 - +2.71e+05 [1] \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588 +2.71e+05 - +2.86e+05 [0] +2.86e+05 - +3.01e+05 [2] \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588 +3.01e+05 - +3.16e+05 [0] +3.16e+05 - +3.31e+05 [1] \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588 +3.31e+05 - +3.46e+05 [0] +3.46e+05 - +3.61e+05 [0] +3.61e+05 - +3.76e+05 [3] \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588 Buttercup: ... NB: You can also generate CSV or JSON output.","title":"Area histogram"},{"location":"imgvis/","text":"Requirements # Requires the image-dataset-converter-imgvis module. Plugins # Annotation overlays (image classification) # Download the 17 flowers image classification dataset and extract it. Having annotations separate from the images is a necessity when training models, but it can be a hindrance when trying to inspect the data. Adding overlays with the annotations is therefore a useful step sometimes. The following command using the add-annotation-overlay-ic filter adds the labels to the images and just outputs these modified images: idc-convert -l INFO \\ from-subdir-ic \\ -l INFO \\ -i ./subdir/ \\ add-annotation-overlay-ic \\ -l INFO \\ to-data \\ -l INFO \\ -o ./17flowers-ic-overlay Here is an example (image_0008.jpg): Annotation overlays (object detection) # Download the 17 flowers object detection VOC XML dataset and extract it. With the add-annotation-overlay-od filter you can overlay the objects on the images: idc-convert -l INFO \\ from-voc-od \\ -l INFO \\ -i \"./voc/*.xml\" \\ add-annotation-overlay-od \\ -l INFO \\ to-data \\ -l INFO \\ -o ./17flowers-od-overlay Here is an example (image_0014.jpg): Annotation overlays (image segmentation) # Download the blue channel archive of the camvid dataset and extract it. With the add-annotation-overlay-is filter you can overlay the segmentation layers: idc-convert -l INFO \\ from-blue-channel-is \\ -l INFO \\ -i \"./bluechannel/*.png\" \\ --labels Animal Archway Bicyclist Bridge Building Car CartLuggagePram Child Column_Pole \\ Fence LaneMkgsDriv LaneMkgsNonDriv Misc_Text MotorcycleScooter OtherMoving ParkingBlock \\ Pedestrian Road RoadShoulder Sidewalk SignSymbol Sky SUVPickupTruck TrafficCone \\ TrafficLight Train Tree Truck_Bus Tunnel VegetationMisc Void Wall \\ add-annotation-overlay-is \\ -l INFO \\ to-data \\ -l INFO \\ -o ./camvid-is-overlay Here is an example (0001TP_006900.jpg): Combining all annotations in one image # Using the to-annotation-overlay-od writer, you can generate a single PNG file that contains the outlines (bbox or polygon) of your object detection annotations. That way, you can see whether certain areas are under-represented with annotations and where hotspots are: idc-convert -l INFO \\ from-voc-od \\ -l INFO \\ -i \"./voc/*.xml\" \\ to-annotation-overlay-od \\ -l INFO \\ -o ./17flowers-annotations-overlay.png The 17 flowers dataset looks like this:","title":"Image visualizations"},{"location":"imgvis/#requirements","text":"Requires the image-dataset-converter-imgvis module.","title":"Requirements"},{"location":"imgvis/#plugins","text":"","title":"Plugins"},{"location":"imgvis/#annotation-overlays-image-classification","text":"Download the 17 flowers image classification dataset and extract it. Having annotations separate from the images is a necessity when training models, but it can be a hindrance when trying to inspect the data. Adding overlays with the annotations is therefore a useful step sometimes. The following command using the add-annotation-overlay-ic filter adds the labels to the images and just outputs these modified images: idc-convert -l INFO \\ from-subdir-ic \\ -l INFO \\ -i ./subdir/ \\ add-annotation-overlay-ic \\ -l INFO \\ to-data \\ -l INFO \\ -o ./17flowers-ic-overlay Here is an example (image_0008.jpg):","title":"Annotation overlays (image classification)"},{"location":"imgvis/#annotation-overlays-object-detection","text":"Download the 17 flowers object detection VOC XML dataset and extract it. With the add-annotation-overlay-od filter you can overlay the objects on the images: idc-convert -l INFO \\ from-voc-od \\ -l INFO \\ -i \"./voc/*.xml\" \\ add-annotation-overlay-od \\ -l INFO \\ to-data \\ -l INFO \\ -o ./17flowers-od-overlay Here is an example (image_0014.jpg):","title":"Annotation overlays (object detection)"},{"location":"imgvis/#annotation-overlays-image-segmentation","text":"Download the blue channel archive of the camvid dataset and extract it. With the add-annotation-overlay-is filter you can overlay the segmentation layers: idc-convert -l INFO \\ from-blue-channel-is \\ -l INFO \\ -i \"./bluechannel/*.png\" \\ --labels Animal Archway Bicyclist Bridge Building Car CartLuggagePram Child Column_Pole \\ Fence LaneMkgsDriv LaneMkgsNonDriv Misc_Text MotorcycleScooter OtherMoving ParkingBlock \\ Pedestrian Road RoadShoulder Sidewalk SignSymbol Sky SUVPickupTruck TrafficCone \\ TrafficLight Train Tree Truck_Bus Tunnel VegetationMisc Void Wall \\ add-annotation-overlay-is \\ -l INFO \\ to-data \\ -l INFO \\ -o ./camvid-is-overlay Here is an example (0001TP_006900.jpg):","title":"Annotation overlays (image segmentation)"},{"location":"imgvis/#combining-all-annotations-in-one-image","text":"Using the to-annotation-overlay-od writer, you can generate a single PNG file that contains the outlines (bbox or polygon) of your object detection annotations. That way, you can see whether certain areas are under-represented with annotations and where hotspots are: idc-convert -l INFO \\ from-voc-od \\ -l INFO \\ -i \"./voc/*.xml\" \\ to-annotation-overlay-od \\ -l INFO \\ -o ./17flowers-annotations-overlay.png The 17 flowers dataset looks like this:","title":"Combining all annotations in one image"},{"location":"object_detection/","text":"Readers and writers for object detection have the -od suffix. Download the 17 flowers object detection VOC XML dataset and extract it. Plugins # VOC XML to MS COCO # The following converts the VOC XML dataset into MS COCO format, a format used by frameworks like Detectron2 or MMDetection : idc-convert \\ -l INFO \\ from-voc-od \\ -l INFO \\ -i \"./voc/*.xml\" \\ to-coco-od \\ -l INFO \\ -o ./coco VOC XML to YOLO (train/val/test splits) # You can also split the data, e.g., into train, validation and test subsets. The following converts the VOC XML now into YOLO format: idc-convert \\ -l INFO \\ from-voc-od \\ -l INFO \\ -i \"./voc/*.xml\" \\ to-yolo-od \\ -l INFO \\ -o ./yolo-split \\ --labels ./yolo-split/labels.txt \\ --split_names train val test \\ --split_ratios 70 15 15 NB: The subsets will be placed into sub-directories according to the split name.","title":"Object detection"},{"location":"object_detection/#plugins","text":"","title":"Plugins"},{"location":"object_detection/#voc-xml-to-ms-coco","text":"The following converts the VOC XML dataset into MS COCO format, a format used by frameworks like Detectron2 or MMDetection : idc-convert \\ -l INFO \\ from-voc-od \\ -l INFO \\ -i \"./voc/*.xml\" \\ to-coco-od \\ -l INFO \\ -o ./coco","title":"VOC XML to MS COCO"},{"location":"object_detection/#voc-xml-to-yolo-trainvaltest-splits","text":"You can also split the data, e.g., into train, validation and test subsets. The following converts the VOC XML now into YOLO format: idc-convert \\ -l INFO \\ from-voc-od \\ -l INFO \\ -i \"./voc/*.xml\" \\ to-yolo-od \\ -l INFO \\ -o ./yolo-split \\ --labels ./yolo-split/labels.txt \\ --split_names train val test \\ --split_ratios 70 15 15 NB: The subsets will be placed into sub-directories according to the split name.","title":"VOC XML to YOLO (train/val/test splits)"},{"location":"pdf/","text":"Requirements # General PDF support requires the image-dataset-converter-pdf library. Extract images from PDF files # The pipeline below extracts any images from the PDF files and places them in the output directory: idc-convert \\ -l INFO \\ from-pdf \\ -l INFO \\ -i ./input/*.pdf \\ -t od \\ to-data \\ -o ./output Object detection overlay example # Requirements # Additional image-dataset-converter library for generating the overlays: image-dataset-converter-imgvis PDF generation # The following pipeline loads predictions in OPEX format, filters out any annotations that are not of type car or truck , generates overlays of bounding boxes with label/score and creates a PDF from these: idc-convert \\ -l INFO \\ --force_batch \\ from-opex-od \\ -l INFO \\ -i ./predictions \\ filter-labels \\ -l INFO \\ --labels car truck \\ add-annotation-overlay-od \\ --outline_alpha 255 \\ --outline_thickness 1 \\ --fill \\ --fill_alpha 128 \\ --vary_colors \\ --font_size 10 \\ --text_format \"{label}: {score}\" \\ --text_placement T,L \\ --force_bbox \\ to-pdf \\ -l INFO \\ -o ./output/vehicles.pdf \\ -t --image_scale=-1","title":"PDF"},{"location":"pdf/#requirements","text":"General PDF support requires the image-dataset-converter-pdf library.","title":"Requirements"},{"location":"pdf/#extract-images-from-pdf-files","text":"The pipeline below extracts any images from the PDF files and places them in the output directory: idc-convert \\ -l INFO \\ from-pdf \\ -l INFO \\ -i ./input/*.pdf \\ -t od \\ to-data \\ -o ./output","title":"Extract images from PDF files"},{"location":"pdf/#object-detection-overlay-example","text":"","title":"Object detection overlay example"},{"location":"pdf/#requirements_1","text":"Additional image-dataset-converter library for generating the overlays: image-dataset-converter-imgvis","title":"Requirements"},{"location":"pdf/#pdf-generation","text":"The following pipeline loads predictions in OPEX format, filters out any annotations that are not of type car or truck , generates overlays of bounding boxes with label/score and creates a PDF from these: idc-convert \\ -l INFO \\ --force_batch \\ from-opex-od \\ -l INFO \\ -i ./predictions \\ filter-labels \\ -l INFO \\ --labels car truck \\ add-annotation-overlay-od \\ --outline_alpha 255 \\ --outline_thickness 1 \\ --fill \\ --fill_alpha 128 \\ --vary_colors \\ --font_size 10 \\ --text_format \"{label}: {score}\" \\ --text_placement T,L \\ --force_bbox \\ to-pdf \\ -l INFO \\ -o ./output/vehicles.pdf \\ -t --image_scale=-1","title":"PDF generation"},{"location":"pyfunc/","text":"No library can dream of offering all the required functionality. Especially for one-off tasks, it makes no sense to develop a whole new plugin library. Hence, there are the following generic plugins that allow the user to utilize custom Python functions: reader: from-pyfunc - takes a single string as input and outputs an iterable of image containers (as per specified data type) filter: pyfunc-filter - takes a single image container or an iterable of them as input and outputs a single container or an iterable of them (as per specified input and output data types) writer: to-pyfunc - processes a single image container or an iterable of them as per specified data type and an optional split name In order to use such a custom function, they must be specified in the following format (option: -f/--function ): module_name:function_name If the code below were available through module my.code , then the function specifications would be as follows: reader: my.code:pyfunc_reader filter: my.code:pyfunc_filter writer: my.code:pyfunc_writer from typing import Iterable from idc.api import ImageClassificationData, make_list, flatten_list # reader: generates image classification containers from the path def pyfunc_reader(path: str) -> Iterable[ImageClassificationData]: return [ImageClassificationData(source=path)] # filter: simply adds a note to the meta-data def pyfunc_filter(data): result = [] for item in make_list(data): if not item.has_metadata(): meta = dict() else: meta = item.get_metadata() meta[\"note\"] = \"filtered by a python function!\" item.set_metadata(meta) result.append(item) return flatten_list(result) # writer: simply outputs name and meta-data and, if present, also the split def pyfunc_writer(data: ImageClassificationData, split: str = None): if split is None: print(\"name: \", data.image_name, \", meta:\", data.get_metadata()) else: print(\"split:\", split, \", name:\", data.image_name, \", meta:\", data.get_metadata())","title":"External functions"},{"location":"redis/","text":"Requirements # Requires the image-dataset-converter-redis library. Object detection example # In this example we are using a prebuilt yolov5 model (using MS-COCO) to make predictions on the frames that come from a dashcam video, overlay the predictions on the images and display them. For the model we will be using an existing docker container. Requirements # NB: No GPU required. Additional image-dataset-converter libraries: image-dataset-converter-imgvis image-dataset-converter-video Data # Input # Output # Preparation # NB: Place all the downloads in the current directory Download the dashcam01.mp4 video from the BoofCV project Download the yolo5n.pt model Download the coco.yaml data description for the yolo5n model The host machine must have a Redis server instance running. Two options: Install it natively via sudo apt-get install redis (and then restart it with sudo systemctl restart redis ) Spin up a docker container with: docker run --net=host --name redis-server -d redis Yolov5 model # The following command launches a Yolov5 model via the container's yolov5_predict_redis command, running on the CPU: docker run \\ --net=host -u $(id -u):$(id -g) \\ -v `pwd`:/workspace \\ -it waikatodatamining/pytorch-yolov5:2022-01-21_cpu \\ yolov5_predict_redis \\ --redis_in images \\ --redis_out predictions \\ --model /workspace/yolov5n.pt \\ --data /workspace/coco.yaml image-dataset-converter (direct prediction) # The following pipeline loads every 2nd frame from the dashcam01.mp4 video, obtains predictions from the Yolov5 model (using the Redis backend), overlays the predictions and then displays them: idc-convert \\ -l INFO \\ from-video-file \\ -i ./dashcam01.mp4 \\ -n 2 \\ -t od \\ redis-predict-od \\ --channel_out images \\ --channel_in predictions \\ --timeout 1.0 \\ add-annotation-overlay-od \\ --outline_alpha 255 \\ --outline_thickness 1 \\ --fill \\ --fill_alpha 128 \\ --vary_colors \\ --font_size 10 \\ --text_format \"{label}: {score}\" \\ --text_placement T,L \\ --force_bbox \\ image-viewer \\ --size 800,224 \\ --delay 1 image-dataset-converter (via meta-sub-images) # Like the above example, but uses the meta-sub-images filter to split the incoming images into two and sends them to the model one-by-one, then assembles the predictions and forwards them: idc-convert \\ -l INFO \\ from-video-file \\ -i ./dashcam01.mp4 \\ -n 2 \\ -t od \\ meta-sub-images \\ -l INFO \\ -r 0,0,400,224 400,0,400,224 \\ --merge_adjacent_polygons \\ --base_filter \"redis-predict-od --channel_out images --channel_in predictions --timeout 1.0\" \\ add-annotation-overlay-od \\ --outline_alpha 255 \\ --outline_thickness 1 \\ --fill \\ --fill_alpha 128 \\ --vary_colors \\ --font_size 10 \\ --text_format \"{label}: {score}\" \\ --text_placement T,L \\ --force_bbox \\ image-viewer \\ --size 800,224 \\ --delay 1 NB: Requires the image-dataset-converter-imgaug library. Meta-data of merged polygons gets lost apart from the label ( type ), which has to be the same, and any score values, which get averaged.","title":"Redis"},{"location":"redis/#requirements","text":"Requires the image-dataset-converter-redis library.","title":"Requirements"},{"location":"redis/#object-detection-example","text":"In this example we are using a prebuilt yolov5 model (using MS-COCO) to make predictions on the frames that come from a dashcam video, overlay the predictions on the images and display them. For the model we will be using an existing docker container.","title":"Object detection example"},{"location":"redis/#requirements_1","text":"NB: No GPU required. Additional image-dataset-converter libraries: image-dataset-converter-imgvis image-dataset-converter-video","title":"Requirements"},{"location":"redis/#data","text":"","title":"Data"},{"location":"redis/#input","text":"","title":"Input"},{"location":"redis/#output","text":"","title":"Output"},{"location":"redis/#preparation","text":"NB: Place all the downloads in the current directory Download the dashcam01.mp4 video from the BoofCV project Download the yolo5n.pt model Download the coco.yaml data description for the yolo5n model The host machine must have a Redis server instance running. Two options: Install it natively via sudo apt-get install redis (and then restart it with sudo systemctl restart redis ) Spin up a docker container with: docker run --net=host --name redis-server -d redis","title":"Preparation"},{"location":"redis/#yolov5-model","text":"The following command launches a Yolov5 model via the container's yolov5_predict_redis command, running on the CPU: docker run \\ --net=host -u $(id -u):$(id -g) \\ -v `pwd`:/workspace \\ -it waikatodatamining/pytorch-yolov5:2022-01-21_cpu \\ yolov5_predict_redis \\ --redis_in images \\ --redis_out predictions \\ --model /workspace/yolov5n.pt \\ --data /workspace/coco.yaml","title":"Yolov5 model"},{"location":"redis/#image-dataset-converter-direct-prediction","text":"The following pipeline loads every 2nd frame from the dashcam01.mp4 video, obtains predictions from the Yolov5 model (using the Redis backend), overlays the predictions and then displays them: idc-convert \\ -l INFO \\ from-video-file \\ -i ./dashcam01.mp4 \\ -n 2 \\ -t od \\ redis-predict-od \\ --channel_out images \\ --channel_in predictions \\ --timeout 1.0 \\ add-annotation-overlay-od \\ --outline_alpha 255 \\ --outline_thickness 1 \\ --fill \\ --fill_alpha 128 \\ --vary_colors \\ --font_size 10 \\ --text_format \"{label}: {score}\" \\ --text_placement T,L \\ --force_bbox \\ image-viewer \\ --size 800,224 \\ --delay 1","title":"image-dataset-converter (direct prediction)"},{"location":"redis/#image-dataset-converter-via-meta-sub-images","text":"Like the above example, but uses the meta-sub-images filter to split the incoming images into two and sends them to the model one-by-one, then assembles the predictions and forwards them: idc-convert \\ -l INFO \\ from-video-file \\ -i ./dashcam01.mp4 \\ -n 2 \\ -t od \\ meta-sub-images \\ -l INFO \\ -r 0,0,400,224 400,0,400,224 \\ --merge_adjacent_polygons \\ --base_filter \"redis-predict-od --channel_out images --channel_in predictions --timeout 1.0\" \\ add-annotation-overlay-od \\ --outline_alpha 255 \\ --outline_thickness 1 \\ --fill \\ --fill_alpha 128 \\ --vary_colors \\ --font_size 10 \\ --text_format \"{label}: {score}\" \\ --text_placement T,L \\ --force_bbox \\ image-viewer \\ --size 800,224 \\ --delay 1 NB: Requires the image-dataset-converter-imgaug library. Meta-data of merged polygons gets lost apart from the label ( type ), which has to be the same, and any score values, which get averaged.","title":"image-dataset-converter (via meta-sub-images)"},{"location":"video/","text":"Requirements # Requires the image-dataset-converter-video library. Plugins # Extract frames from video file # The following extracts a total of 100 frames from ./input/video.mp4 , using every 20th frame and stores the images in the ./output directory: idc-convert \\ -l INFO \\ from-video-file \\ -l INFO \\ -i ./input/video.mp4 \\ -t od \\ -n 20 \\ -m 100 \\ to-data \\ -l INFO \\ -o ./output/ Though the data type is object detection ( -t od ), this is not really relevant in this example. If there were other filters, e.g., for applying object detection Redis models , then it would be necessary to specify the correct type. Capture frames from a webcam (individual JPG files) # The following captures a total of 10 images from a webcam (using every 20th frame) and stores them as individual JPG files in the ./output directory: idc-convert \\ -l INFO \\ from-webcam \\ -l INFO \\ -i 0 \\ -t od \\ -n 20 \\ -m 10 \\ to-data \\ -l INFO \\ -o ./output Capture frames from a webcam (store as MJPEG file) # The following captures a total of 100 images from a webcam (using every 10th frame) and generates a MJPEG file, which can be viewed with, e.g., ffplay : idc-convert \\ -l INFO \\ from-webcam \\ -l INFO \\ -i 0 \\ -t od \\ -n 10 \\ -m 100 \\ to-video-file \\ -l INFO \\ -o ./output/webcam.mjpeg Calculating differences between frames # Depending on the scene, frames may not differ much between each other, making it hard to extract frames by using specific intervals. The calc-frame-changes writer computes differences between frames and output a histogram of these differences. Using the min/max of the bins of the histogram, it is possible to find a suitable change threshold. This threshold can be utilized by the skip-similar-frames filter to discard too similar frames, whittling down the number of extracted frames automatically. The histogram can be output in textual form, CSV or JSON; either on stdout or stored in a file. idc-convert \\ -l INFO \\ from-video-file \\ -l INFO \\ -i ./input/video.mp4 \\ -t od \\ -n 10 \\ -m 2000 \\ calc-frame-changes \\ -f text Skipping similar frames # The following command keeps frames that differ enough from each other and discards all others, using the skip-similar-frames filter. The threshold applied by the filter was previously determined by the calc-frame-changes filter. idc-convert \\ -l INFO \\ from-video-file \\ -l INFO \\ -i ./input/video.mp4 \\ -t od \\ skip-similar-frames \\ -t 0.005 \\ to-data \\ -l INFO \\ -o ./output Youtube video # The from-youtube reader allows you to grab frames from a Youtube video: idc-convert \\ -l INFO \\ from-youtube \\ -l INFO \\ -i \"https://www.youtube.com/watch?v=CFdZWgiAj8I\" \\ -p the_cloud-kitty_flanagan- \\ -t od \\ -n 20 \\ -m 10 \\ to-data \\ -l INFO \\ -o ./output NB: Please make sure not to infringe on any copyright. Youtube live stream # The from-youtube-live reader allows you to grab frames from a Youtube live stream, e.g., wildlife cameras: idc-convert \\ -l INFO \\ from-youtube-live \\ -l INFO \\ -i \"https://www.youtube.com/watch?v=2swy9gysvOY\" \\ -p african_river_wildlife_camera- \\ -t od \\ -n 20 \\ -m 10 \\ to-data \\ -l INFO \\ -o ./output NB: Please make sure not to infringe on any copyright.","title":"Video"},{"location":"video/#requirements","text":"Requires the image-dataset-converter-video library.","title":"Requirements"},{"location":"video/#plugins","text":"","title":"Plugins"},{"location":"video/#extract-frames-from-video-file","text":"The following extracts a total of 100 frames from ./input/video.mp4 , using every 20th frame and stores the images in the ./output directory: idc-convert \\ -l INFO \\ from-video-file \\ -l INFO \\ -i ./input/video.mp4 \\ -t od \\ -n 20 \\ -m 100 \\ to-data \\ -l INFO \\ -o ./output/ Though the data type is object detection ( -t od ), this is not really relevant in this example. If there were other filters, e.g., for applying object detection Redis models , then it would be necessary to specify the correct type.","title":"Extract frames from video file"},{"location":"video/#capture-frames-from-a-webcam-individual-jpg-files","text":"The following captures a total of 10 images from a webcam (using every 20th frame) and stores them as individual JPG files in the ./output directory: idc-convert \\ -l INFO \\ from-webcam \\ -l INFO \\ -i 0 \\ -t od \\ -n 20 \\ -m 10 \\ to-data \\ -l INFO \\ -o ./output","title":"Capture frames from a webcam (individual JPG files)"},{"location":"video/#capture-frames-from-a-webcam-store-as-mjpeg-file","text":"The following captures a total of 100 images from a webcam (using every 10th frame) and generates a MJPEG file, which can be viewed with, e.g., ffplay : idc-convert \\ -l INFO \\ from-webcam \\ -l INFO \\ -i 0 \\ -t od \\ -n 10 \\ -m 100 \\ to-video-file \\ -l INFO \\ -o ./output/webcam.mjpeg","title":"Capture frames from a webcam (store as MJPEG file)"},{"location":"video/#calculating-differences-between-frames","text":"Depending on the scene, frames may not differ much between each other, making it hard to extract frames by using specific intervals. The calc-frame-changes writer computes differences between frames and output a histogram of these differences. Using the min/max of the bins of the histogram, it is possible to find a suitable change threshold. This threshold can be utilized by the skip-similar-frames filter to discard too similar frames, whittling down the number of extracted frames automatically. The histogram can be output in textual form, CSV or JSON; either on stdout or stored in a file. idc-convert \\ -l INFO \\ from-video-file \\ -l INFO \\ -i ./input/video.mp4 \\ -t od \\ -n 10 \\ -m 2000 \\ calc-frame-changes \\ -f text","title":"Calculating differences between frames"},{"location":"video/#skipping-similar-frames","text":"The following command keeps frames that differ enough from each other and discards all others, using the skip-similar-frames filter. The threshold applied by the filter was previously determined by the calc-frame-changes filter. idc-convert \\ -l INFO \\ from-video-file \\ -l INFO \\ -i ./input/video.mp4 \\ -t od \\ skip-similar-frames \\ -t 0.005 \\ to-data \\ -l INFO \\ -o ./output","title":"Skipping similar frames"},{"location":"video/#youtube-video","text":"The from-youtube reader allows you to grab frames from a Youtube video: idc-convert \\ -l INFO \\ from-youtube \\ -l INFO \\ -i \"https://www.youtube.com/watch?v=CFdZWgiAj8I\" \\ -p the_cloud-kitty_flanagan- \\ -t od \\ -n 20 \\ -m 10 \\ to-data \\ -l INFO \\ -o ./output NB: Please make sure not to infringe on any copyright.","title":"Youtube video"},{"location":"video/#youtube-live-stream","text":"The from-youtube-live reader allows you to grab frames from a Youtube live stream, e.g., wildlife cameras: idc-convert \\ -l INFO \\ from-youtube-live \\ -l INFO \\ -i \"https://www.youtube.com/watch?v=2swy9gysvOY\" \\ -p african_river_wildlife_camera- \\ -t od \\ -n 20 \\ -m 10 \\ to-data \\ -l INFO \\ -o ./output NB: Please make sure not to infringe on any copyright.","title":"Youtube live stream"}]}